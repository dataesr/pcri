{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd, pickle, numpy as np\n",
    "pd.options.mode.copy_on_write = True\n",
    "from config_path import PATH_CLEAN, PATH_API\n",
    "from functions_shared import chunkify, work_csv\n",
    "from step7_persons.prep_persons import persons_preparation\n",
    "from step7_persons.affiliations import persons_affiliation\n",
    "\n",
    "CSV_DATE='20250121'\n",
    "# persons_preparation(CSV_DATE)\n",
    "\n",
    "PATH_PERSONS=f\"{PATH_API}persons/\"\n",
    "perso_part = pd.read_pickle(f\"{PATH_CLEAN}persons_participants.pkl\")\n",
    "perso_app = pd.read_pickle(f\"{PATH_CLEAN}persons_applicants.pkl\")\n",
    "\n",
    "#provisoire\n",
    "project=pd.read_pickle(f\"{PATH_CLEAN}projects_current.pkl\") \n",
    "perso_part=perso_part.merge(project[['project_id', 'stage', 'destination_code', 'thema_code']], how ='left', on=['project_id', 'stage'])\n",
    "perso_app=perso_app.merge(project[['project_id', 'stage', 'destination_code', 'thema_code']], how ='left', on=['project_id', 'stage'])\n",
    "\n",
    "\n",
    "#PREPRATION data for request openalex\n",
    "# lvar=['contact','orcid_id','country_code','destination_code','thema_code','nationality_country_code']\n",
    "# pp = pd.concat([perso_part[lvar].drop_duplicates(), perso_app[lvar].drop_duplicates()], ignore_index=True)\n",
    "\n",
    "# mask=((pp.country_code=='FRA')|(pp.nationality_country_code=='FRA')|(pp.destination_code.isin(['COG', 'PF', 'STG', 'ADG', 'POC','SyG', 'PERA', 'SJI'])))&(~(pp.contact.isnull()&pp.orcid_id.isnull()))\n",
    "# pp=pp.loc[mask].sort_values(['country_code','orcid_id'], ascending=False).drop_duplicates()\n",
    "# print(f\"size pp: {len(pp)}, info sur pp with orcid: {len(pp.loc[pp.orcid_id.isnull()])}\")\n",
    "\n",
    "\n",
    "# pp=pp[['contact', 'orcid_id']].fillna('').drop_duplicates().sort_values('orcid_id', ascending=False)\n",
    "# data_chunks=list(chunkify(pp, 2000))\n",
    "# for i in range(0, len(data_chunks)):\n",
    "#     print(f\"Loop {i}, size data_chunks: {len(data_chunks)}\")\n",
    "#     # print(type(data_chunks))\n",
    "#     df_temp = data_chunks[i]\n",
    "#     persons_affiliation(df_temp, i, PATH_PERSONS)\n",
    "\n",
    "# #Return openalex results\n",
    "# pers_api=[]\n",
    "# # for i in range(0, len(data_chunks)):\n",
    "# for i in range(0, 7):\n",
    "#     with open(f\"{PATH_PERSONS}persons_author_{i}.pkl\", 'rb') as f:\n",
    "#         globals()[f\"pers_api{i}\"] = pickle.load(f)\n",
    "#         if globals()[f\"pers_api{i}\"]==[]:\n",
    "#             print(f\"- empty list: {globals()[f\"pers_api{i}\"]}\")\n",
    "#         else:\n",
    "#             pers_api.extend(globals()[f\"pers_api{i}\"])\n",
    "# with open(f'{PATH_PERSONS}persons_authors_{CSV_DATE}.pkl', 'wb') as f:\n",
    "#     pickle.dump(pers_api, f)\n",
    "\n",
    "with open(f'{PATH_PERSONS}persons_authors_{CSV_DATE}.pkl', 'rb') as f:\n",
    "    pers_api=pickle.load(f)\n",
    "\n",
    "pers_api=pd.json_normalize(pers_api)\n",
    "pers_api=pers_api[~pers_api.astype(str).duplicated()]\n",
    "\n",
    "# remove name null and author found by orcid but without institutions\n",
    "pers_api=pers_api[(~pers_api.name.isnull())]\n",
    "# pers_api=pers_api[~((pers_api.match=='orcid')&(pers_api.affiliations.str.len()==0))]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pers_api.loc[(pers_api.match=='name')&(~pers_api['ids.orcid'].isnull()), ['orcid','ids.orcid']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#traitement au niveau ORCID\n",
    "# pers_orcid=pers_api.loc[(pers_api.match=='orcid')&(pers_api.affiliations.str.len()!=0)]\n",
    "# pers_orcid\n",
    "\n",
    "def persons_results_clean(df, match):\n",
    "    df=df.loc[(df.match==match)&(df.affiliations.str.len()>0)]\n",
    "    print(f\"-1 size {match} with affiliations: {len(df)}\")\n",
    "    df=df.explode('affiliations')\n",
    "    df=df.join(pd.json_normalize(df['affiliations'], max_level=1))\n",
    "\n",
    "    df.columns = df.columns.str.replace(r\"[.*_]+\", '_', regex=True)\n",
    "    df = (df\n",
    "                .rename(columns={'institution_display_name':'institution_name',\n",
    "                                'institution_country_code':'country_code'})\n",
    "                .drop(columns=['institution_type','institution_lineage','affiliations',\n",
    "                                'topics','topics',\n",
    "                                'ids_openalex','ids_scopus','ids_twitter']))\n",
    "    print(f\"-2 size {match} after explode affiliations: {len(df)}\")\n",
    "\n",
    "    df['display_name_alternatives']=df['display_name_alternatives'].map(lambda x: ';'.join(filter(None, x)))\n",
    "    df=df[~df.astype(str).duplicated()]\n",
    "\n",
    "    for i in ['ids_orcid', 'institution_ror']:\n",
    "        df.loc[~df[i].isnull(), i] = df.loc[~df[i].isnull()][i].str.split(\"/\").str[-1]\n",
    "    print(f\"-3 size {match} cleaned: {len(df)}\")\n",
    "    return df\n",
    "\n",
    "pers_orcid=persons_results_clean(pers_api, 'orcid')\n",
    "pers_name=persons_results_clean(pers_api, 'name')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from step8_referentiels.paysage import paysage_prep\n",
    "from config_path import PATH\n",
    "DUMP_PATH=f'{PATH}referentiel/'\n",
    "paysage = paysage_prep(DUMP_PATH)\n",
    "paysage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pers_api[pers_api.name=='zytkiewicz malgorzata']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def openalex_name(author):\n",
    "    import time, requests\n",
    "    try:\n",
    "        url = f\"https://api.openalex.org/authors?filter=display_name.search:{author.get('name')}\"\n",
    "        nb_openalex=requests.get(url).json().get(\"meta\").get('count')\n",
    "        dl=[]\n",
    "        if nb_openalex>0:\n",
    "            print(nb_openalex)\n",
    "            for n in range(0, nb_openalex): \n",
    "                author_openalex = requests.get(url).json().get(\"results\")[n]\n",
    "                if author_openalex.get('affiliations')!=[]:\n",
    "                    result=author | {'display_name':author_openalex.get('display_name'), \n",
    "                                    'openalex_id':author_openalex.get('id'), \n",
    "                                    'affiliations':author_openalex.get('affiliations'),\n",
    "                                    'topics':author_openalex.get('topics'),\n",
    "                                    'x_concepts':author_openalex.get('x_concepts'), \n",
    "                                    'ids':author_openalex.get('ids'), \n",
    "                                    'display_name_alternatives':author_openalex.get('display_name_alternatives'),\n",
    "                                    'match':'name'}\n",
    "                    dl.append(result)\n",
    "        return dl\n",
    "    \n",
    "    # except requests.exceptions.HTTPError as http_err:\n",
    "    #     print(f\"\\n{time.strftime(\"%H:%M:%S\")}, {author}-> HTTP error occurred: {http_err}\")\n",
    "    except requests.exceptions.RequestException as err:\n",
    "        print(f\"\\n{time.strftime(\"%H:%M:%S\")}, {author}-> Error occurred: {err}\")    \n",
    "    except Exception as e:\n",
    "        print(f\"\\n{time.strftime(\"%H:%M:%S\")}, {author}-> An unexpected error occurred: {e}\")\n",
    "\n",
    "def openalex_orcid(author):\n",
    "    from config_api import openalex_usermail\n",
    "    import requests, time\n",
    "    try:\n",
    "        url = f\"https://api.openalex.org/authors/orcid:{author.get('orcid')}?mailto={openalex_usermail}\"\n",
    "        author_openalex = requests.get(url).json()\n",
    "        result = author | {'display_name':author_openalex.get('display_name'), \n",
    "                           'openalex_id':author_openalex.get('id'), \n",
    "                           'affiliations':author_openalex.get('affiliations'), \n",
    "                           'topics':author_openalex.get('topics'),  \n",
    "                           'x_concepts':author_openalex.get('x_concepts'), \n",
    "                           'ids':author_openalex.get('ids'), \n",
    "                           'display_name_alternatives':author_openalex.get('display_name_alternatives'),\n",
    "                           'match':'orcid'}\n",
    "        return result\n",
    "    \n",
    "    # except requests.exceptions.HTTPError as http_err:\n",
    "    #     print(f\"\\n{time.strftime(\"%H:%M:%S\")}, {author}-> HTTP error occurred: {http_err}\")\n",
    "    #     return author\n",
    "    except requests.exceptions.RequestException as err:\n",
    "        print(f\"\\n{time.strftime(\"%H:%M:%S\")}, {author}-> Error occurred: {err}\")\n",
    "        return author         \n",
    "    except Exception as e:\n",
    "        print(f\"\\n{time.strftime(\"%H:%M:%S\")}, {author}-> An unexpected error occurred: {e}\")\n",
    "        return author\n",
    "    \n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time, pickle, requests\n",
    "from step7_persons.affiliations import openalex_name, openalex_orcid\n",
    "from config_path import PATH_CLEAN, PATH_API, PATH_WORK\n",
    "print(time.strftime(\"%H:%M:%S\"))\n",
    "rlist=[]\n",
    "\n",
    "try:\n",
    "\n",
    "\n",
    "    author = {\n",
    "    \"name\": \"caye pierre'\",\n",
    "    \"orcid\": \"\"\n",
    "    }\n",
    "\n",
    "    if author.get(\"orcid\"):\n",
    "        result = openalex_orcid(author)\n",
    "        if result.get('match'):\n",
    "            rlist.append(result)\n",
    "        else:\n",
    "            result = openalex_name(author)\n",
    "            if result:\n",
    "                rlist.extend(result)\n",
    "    if author.get(\"orcid\")=='':\n",
    "        result = openalex_name(author)\n",
    "        if result:\n",
    "            rlist.extend(result)\n",
    "\n",
    "    nf=f\"persons_author\"\n",
    "    with open(f'{PATH_WORK}test.pkl', 'wb') as f:\n",
    "        pickle.dump(rlist, f)\n",
    "    print(time.strftime(\"%H:%M:%S\"))\n",
    "\n",
    "except requests.exceptions.HTTPError as http_err:\n",
    "    print(f\"\\n{time.strftime(\"%H:%M:%S\")}, {author}-> HTTP error occurred: {http_err}\")\n",
    "    nf=f\"persons_author\"\n",
    "    with open(f'{PATH_WORK}test.pkl', 'wb') as f:\n",
    "        pickle.dump(rlist, f)\n",
    "    print(time.strftime(\"%H:%M:%S\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open(f'{PATH_WORK}test.pkl', 'rb') as f:\n",
    "    test = pickle.load(f)\n",
    "test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## return results from apenalex\n",
    "\n",
    "1 - à partir d'orcid\n",
    "2 - à partir du nom car orcid pas dans openalex -> vérifier que même personne ?\n",
    "3 - à partir du nom car orcid non renseigné\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_country_code():\n",
    "    import pycountry, pandas as pd, json, numpy as np, warnings\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    pycountry.countries.add_entry(alpha_2=\"XK\", alpha_3=\"XXK\", name=\"Kosovo\")\n",
    "    pycountry.countries.add_entry(alpha_2=\"UK\", alpha_3=\"GBR\", name=\"United Kingdom\")\n",
    "    pycountry.countries.add_entry(alpha_2=\"EL\", alpha_3=\"GRC\", name=\"Greece\")\n",
    "    pycountry.countries.add_entry(alpha_2=\"AN\", alpha_3=\"ANT\", name=\"Netherlands Antilles (former 2011)\")\n",
    "    dict1 = [c.__dict__['_fields'] for c in list(pycountry.countries)]\n",
    "    df = (pd.DataFrame(dict1)[['alpha_2', 'alpha_3', 'name']]\n",
    "                .rename(columns={'alpha_2':'iso2', 'alpha_3':'iso3', 'name':'country_name_en'})\n",
    "                .drop_duplicates()\n",
    "                .assign(parent_iso2=np.nan)\n",
    "        )\n",
    "\n",
    "    list_var=['iso2']\n",
    "    ccode=json.load(open(\"data_files/countries_parent.json\"))\n",
    "    for c in list_var:\n",
    "        for k,v in ccode.items():\n",
    "            df.loc[df[c]==k, 'parent_iso2'] = v\n",
    "\n",
    "    df.loc[df.parent_iso2.isnull(), 'parent_iso2'] = df.loc[df.parent_iso2.isnull(), 'iso2']\n",
    "    df=(df.merge(df[['iso2','iso3']].drop_duplicates().rename(columns={'iso2':'parent_iso2','iso3':'parent_iso3'}), \n",
    "                    how='left', on='parent_iso2'))\n",
    "\n",
    "    print(len(df))\n",
    "    return df\n",
    "my_countries=my_country_code()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp1=pers_api.merge(my_countries[['iso2', 'iso3', 'parent_iso3']].drop_duplicates(),how='left', left_on='country_code', right_on='iso2')\n",
    "if any(tmp1.country_code.isnull()):\n",
    "    print(tmp1.loc[tmp1.country_code.isnull(), ['name', 'orcid', 'institution_ror', 'institution_name']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pers_api.mask(pers_api=='', inplace=True)\n",
    "tmp1=pers_api.merge(my_countries, how='left', left_on='country_code', right_on='iso2')\n",
    "# pers_api=country_clean(pers_api, ['country_code'])\n",
    "tmp1=pers_api.loc[pers_api.match=='orcid']\n",
    "len(tmp1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{len(perso_part)} ; {perso_part.columns}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_part=perso_part.merge(tmp1, how='inner', left_on=['orcid_id'], right_on=['orcid'], suffixes=('','_api'))\n",
    "tmp_part.loc[tmp_part.country_code!=tmp_part.country_code_api]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [\n",
    "    {\n",
    "        \"state\": \"Florida\",\n",
    "        \"shortname\": \"FL\",\n",
    "        \"info\": {\"governor\": \"Rick Scott\"},\n",
    "        \"counties\": [\n",
    "            {\"name\": \"Dade\", \"population\": 12345},\n",
    "            {\"name\": \"Broward\", \"population\": 40000},\n",
    "            {\"name\": \"Palm Beach\", \"population\": 60000},\n",
    "        ],\n",
    "    },\n",
    "    {\n",
    "        \"state\": \"Ohio\",\n",
    "        \"shortname\": \"OH\",\n",
    "        \"info\": {\"governor\": \"John Kasich\"},\n",
    "        \"counties\": [\n",
    "            {\"name\": \"Summit\", \"population\": 1234},\n",
    "            {\"name\": \"Cuyahoga\", \"population\": 1337},\n",
    "        ],\n",
    "    },\n",
    "]\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def openalex_name(author):\n",
    "    url = f\"https://api.openalex.org/authors?filter=display_name.search:{author.get('name')}\"\n",
    "    nb_openalex=requests.get(url).json().get(\"meta\").get('count')\n",
    "    result={}\n",
    "    if nb_openalex>0:\n",
    "        for n in range(nb_openalex): \n",
    "            author_openalex = requests.get(url).json().get(\"results\")[n]\n",
    "            result.update({'display_name':author_openalex.get('display_name'), 'openalex_id':author_openalex.get('id'), 'affiliations':author_openalex.get('affiliations'), 'topics':author_openalex.get('topics'),  'x_concepts':author_openalex.get('x_concepts'), 'ids':author_openalex.get('ids'), 'display_name_alternatives':author_openalex.get('display_name_alternatives')})\n",
    "            author.update(result)\n",
    "            df=pd.concat([df, pd.json_normalize(author)])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from config_path import PATH_API\n",
    "import os, pandas as pd\n",
    " \n",
    "\n",
    "for racine, repertoires, fichiers in os.walk(PATH_API):\n",
    "    print(f\"{racine}, {repertoires}, {fichiers}\")\n",
    "    for fichier in fichiers:\n",
    "        if fichier.startswith('persons'):\n",
    "            print(os.path.join(racine, fichier))\n",
    "            globals()[f\"{fichier}\"]= pd.read_pickle(os.path.join(racine, fichier))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fl=['persons_author.pkl', 'persons_author_10000.pkl', 'persons_author_12000.pkl', 'persons_author_2000.pkl', 'persons_author_4000.pkl', 'persons_author_6000.pkl', 'persons_author_8000.pkl', 'persons_author_name.pkl', 'persons_author_orcid.pkl']\n",
    "for racine, repertoires, fichiers in os.walk(PATH_API):\n",
    "    for i in fl:\n",
    "        name=f\"{i}\".split('.')[0]\n",
    "        print(name)\n",
    "        globals()[name] = pd.read_pickle(os.path.join(racine, fichier))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pers=pd.concat([persons_author_10000, persons_author_12000, persons_author_2000, persons_author_4000, persons_author_6000, persons_author_8000], ignore_index=True)\n",
    "\n",
    "fl=['persons_author.pkl', 'persons_author_name.pkl', 'persons_author_orcid.pkl']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def openalex_orcid(author):\n",
    "    # from config_api import openalex_usermail\n",
    "    import requests, time\n",
    "    try:\n",
    "        url = f\"https://api.openalex.org/authors/orcid:{author.get('orcid')}?mailto=zmenesr@gmail.com\"\n",
    "        author_openalex = requests.get(url).json()\n",
    "        result = author | {'display_name':author_openalex.get('display_name'), \n",
    "                           'openalex_id':author_openalex.get('id'), \n",
    "                           'affiliations':author_openalex.get('affiliations'), \n",
    "                           'topics':author_openalex.get('topics'),  \n",
    "                           'x_concepts':author_openalex.get('x_concepts'), \n",
    "                           'ids':author_openalex.get('ids'), \n",
    "                           'display_name_alternatives':author_openalex.get('display_name_alternatives'),\n",
    "                           'match':'orcid'}\n",
    "        return result\n",
    "    \n",
    "    except requests.exceptions.HTTPError as http_err:\n",
    "        print(f\"\\n{time.strftime(\"%H:%M:%S\")}, {author}-> HTTP error occurred: {http_err}\")\n",
    "        return author\n",
    "    except requests.exceptions.RequestException as err:\n",
    "        print(f\"\\n{time.strftime(\"%H:%M:%S\")}, {author}-> Error occurred: {err}\")\n",
    "        return author           \n",
    "    except Exception as e:\n",
    "        print(f\"\\n{time.strftime(\"%H:%M:%S\")}, {author}-> An unexpected error occurred: {e}\")\n",
    "        return author"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def openalex_name(author):\n",
    "    import time\n",
    "    try:\n",
    "        url = f\"https://api.openalex.org/authors?filter=display_name.search:{author.get('name')}\"\n",
    "        nb_openalex=requests.get(url).json().get(\"meta\").get('count')\n",
    "        dl=[]\n",
    "        if nb_openalex>0:\n",
    "            print(nb_openalex)\n",
    "            for n in range(nb_openalex): \n",
    "                author_openalex = requests.get(url).json().get(\"results\")[n]\n",
    "                if author_openalex.get('affiliations')!=[]:\n",
    "                    result=author | {'display_name':author_openalex.get('display_name'), \n",
    "                                    'openalex_id':author_openalex.get('id'), \n",
    "                                    'affiliations':author_openalex.get('affiliations'),\n",
    "                                    'topics':author_openalex.get('topics'),\n",
    "                                    'x_concepts':author_openalex.get('x_concepts'), \n",
    "                                    'ids':author_openalex.get('ids'), \n",
    "                                    'display_name_alternatives':author_openalex.get('display_name_alternatives'),\n",
    "                                    'match':'name'}\n",
    "                    dl.append(result)\n",
    "        return dl\n",
    "    \n",
    "    except requests.exceptions.HTTPError as http_err:\n",
    "        print(f\"\\n{time.strftime(\"%H:%M:%S\")}, {author}-> HTTP error occurred: {http_err}\")\n",
    "        return author\n",
    "    except requests.exceptions.RequestException as err:\n",
    "        print(f\"\\n{time.strftime(\"%H:%M:%S\")}, {author}-> Error occurred: {err}\")\n",
    "        return author           \n",
    "    except Exception as e:\n",
    "        print(f\"\\n{time.strftime(\"%H:%M:%S\")}, {author}-> An unexpected error occurred: {e}\")\n",
    "        return author\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from config_path import PATH_CLEAN\n",
    "# from functions_shared import chunkify\n",
    "perso_part = pd.read_pickle(f\"{PATH_CLEAN}persons_participants.pkl\")\n",
    "# pp = pd.concat([perso_part[['contact', 'orcid_id', 'country_code']].drop_duplicates(), perso_app[['contact', 'orcid_id', 'country_code']].drop_duplicates()], ignore_index=True)\n",
    "pp = perso_part[['contact', 'orcid_id', 'country_code']].fillna('')\n",
    "pp=pp.loc[(pp.country_code=='FRA')].sort_values('orcid_id', ascending=False)\n",
    "pp\n",
    "data_chunks=list(chunkify(pp, 10000))\n",
    "for i in range(0, len(data_chunks)):\n",
    "    print(f\"Loop {i}, size data_chunks: {len(data_chunks)}\")\n",
    "    # print(type(data_chunks))\n",
    "    df_temp = data_chunks[i]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle, pandas as pd\n",
    "from config_path import PATH_API\n",
    "pers_api=[]\n",
    "for i in range(1,3):\n",
    "    print(i)\n",
    "    with open(f\"{PATH_API}fr_persons_author_{i}.pkl\", 'rb') as f:\n",
    "        globals()[f\"pers_api{i}\"] = pickle.load(f)\n",
    "    pers_api.extend(globals()[f\"pers_api{i}\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pers_api=pd.json_normalize(pers_api1, record_path=['affiliations'], meta=['name', 'orcid', 'display_name', 'openalex_id',  'match',  [\"ids\", \"orcid\"]],\n",
    "        errors='ignore')\n",
    "\n",
    "pers_api.columns = pers_api.columns.str.replace(r\"[.*_]+\", '_', regex=True)\n",
    "\n",
    "pers_api = (pers_api\n",
    "            .rename(columns={\n",
    "                    'institution_country_code':'country_code'})\n",
    "            .drop(columns=['institution_type','institution_lineage']))\n",
    "\n",
    "for i in ['ids_orcid', 'institution_ror']:\n",
    "    pers_api.loc[~pers_api[i].isnull(), i] = pers_api.loc[~pers_api[i].isnull()][i].str.split(\"/\").str[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from config_path import PATH_CLEAN\n",
    "perso_part = pd.read_pickle(f\"{PATH_CLEAN}persons_participants.pkl\")\n",
    "perso_app = pd.read_pickle(f\"{PATH_CLEAN}persons_applicants.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"{PATH_API}persons_author.pkl\", 'rb') as f:\n",
    "    author_orcid = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "author_orcid=pd.json_normalize(author_orcid, record_path=['affiliations'], meta=['name','orcid', 'display_name', 'ids', 'match'])\n",
    "author_orcid"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
