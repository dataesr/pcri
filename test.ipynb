{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def openalex_name(author):\n",
    "    import requests, time\n",
    "    try:\n",
    "        url = f\"https://api.openalex.org/authors?filter=display_name.search:{author.get('name')}\"\n",
    "        nb_openalex=requests.get(url).json().get(\"meta\").get('count')\n",
    "        if nb_openalex>0:\n",
    "            d=[]\n",
    "            for n in range(nb_openalex):\n",
    "                author_openalex = requests.get(url).json().get(\"results\")[n]\n",
    "                print(author_openalex)\n",
    "                result = author | {'display_name':author_openalex.get('display_name'),\n",
    "                                'openalex_id':author_openalex.get('id'), \n",
    "                                'affiliations':author_openalex.get('affiliations'), \n",
    "                                'topics':author_openalex.get('topics'), \n",
    "                                'x_concepts':author_openalex.get('x_concepts'), \n",
    "                                'ids':author_openalex.get('ids'), \n",
    "                                'display_name_alternatives':author_openalex.get('display_name_alternatives')}\n",
    "                d.append(result)\n",
    "        return d\n",
    "\n",
    "    except requests.exceptions.HTTPError as http_err:\n",
    "        print(f\"\\n{time.strftime(\"%H:%M:%S\")}, {author}-> HTTP error occurred: {http_err}\")\n",
    "    except requests.exceptions.RequestException as err:\n",
    "        print(f\"\\n{time.strftime(\"%H:%M:%S\")}, {author}-> Error occurred: {err}\")                    \n",
    "    except Exception as e:\n",
    "        print(f\"\\n{time.strftime(\"%H:%M:%S\")}, {author}-> An unexpected error occurred: {e}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def openalex_orcid(author):\n",
    "    import requests, time\n",
    "    try:\n",
    "        url = f\"https://api.openalex.org/authors/orcid:{author.get('orcid')}?mailto=zmenesr@gmail.com\"\n",
    "        author_openalex = requests.get(url).json()\n",
    "        result = author | {'display_name':author_openalex.get('display_name'), \n",
    "                           'openalex_id':author_openalex.get('id'), \n",
    "                           'affiliations':author_openalex.get('affiliations'), \n",
    "                           'topics':author_openalex.get('topics'),  \n",
    "                           'x_concepts':author_openalex.get('x_concepts'), \n",
    "                           'ids':author_openalex.get('ids'), \n",
    "                           'display_name_alternatives':author_openalex.get('display_name_alternatives')}\n",
    "        return result\n",
    "    \n",
    "    except requests.exceptions.HTTPError as http_err:\n",
    "        print(f\"\\n{time.strftime(\"%H:%M:%S\")}, {author}-> HTTP error occurred: {http_err}\")\n",
    "    except requests.exceptions.RequestException as err:\n",
    "        print(f\"\\n{time.strftime(\"%H:%M:%S\")}, {author}-> Error occurred: {err}\")                    \n",
    "    except Exception as e:\n",
    "        print(f\"\\n{time.strftime(\"%H:%M:%S\")}, {author}-> An unexpected error occurred: {e}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df=pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd, time, re, numpy as np, requests, pprint as pp\n",
    "author = {\n",
    "\"name\": 'jeremy peglion',\n",
    "\"orcid\": ''\n",
    "}\n",
    "# try:\n",
    "if author.get(\"orcid\"):\n",
    "    result = openalex_orcid(author)\n",
    "    if result:\n",
    "        df=pd.concat([df, pd.json_normalize(result)])\n",
    "    else:\n",
    "        result = openalex_name(author)\n",
    "        if result:\n",
    "            df=pd.concat([df, pd.json_normalize(result)])\n",
    "if author.get(\"orcid\")=='':\n",
    "    result = openalex_name(author)\n",
    "    if result:\n",
    "        df=pd.concat([df, pd.json_normalize(result)])\n",
    "\n",
    "# except requests.exceptions.HTTPError as http_err:\n",
    "#     print(f\"\\n{time.strftime(\"%H:%M:%S\")}, {author}-> HTTP error occurred: {http_err}\")\n",
    "# except requests.exceptions.RequestException as err:\n",
    "#     print(f\"\\n{time.strftime(\"%H:%M:%S\")}, {author}-> Error occurred: {err}\")                    \n",
    "# except Exception as e:\n",
    "#     print(f\"\\n{time.strftime(\"%H:%M:%S\")}, {author}-> An unexpected error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep(stage, df, countries, lien):\n",
    "\n",
    "    test = df.merge(countries[['countryCode', 'country_code_mapping','country_code']], how='left', on='countryCode')\n",
    "    test = test.assign(stage=stage).drop(columns=['countryCode','orderNumber', 'departmentUniqueId','framework', 'lastUpdateDate' ]).drop_duplicates()\n",
    "#     test['nb'] = test.groupby(['project_id', 'generalPic', 'pic'])['department'].transform('count')\n",
    "\n",
    "    if stage=='evaluated':\n",
    "        tmp=(lien.loc[lien.inProposal==True, ['project_id', 'generalPic', 'proposal_orderNumber','proposal_participant_pic', 'calculated_pic', 'nuts_applicants', 'n_app']]\n",
    "            .rename(columns={'nuts_applicants':'entities_nuts', 'proposal_participant_pic':'pic', 'proposal_orderNumber':'orderNumber', 'n_app':'ent_nb'}))\n",
    "        tmp=tmp.merge(test, how='inner', on=['project_id',  'generalPic',  'pic'])\n",
    "    elif  stage=='successful':\n",
    "        tmp=(lien.loc[lien.inProject==True, ['project_id', 'generalPic', 'orderNumber', 'participant_pic', 'calculated_pic', 'nuts_participant', 'n_part']]\n",
    "        .rename(columns={'nuts_participant':'entities_nuts', 'participant_pic':'pic', 'n_part':'ent_nb'}))\n",
    "        tmp=tmp.merge(test, how='inner', on=['project_id',  'generalPic',  'pic'])\n",
    "    \n",
    "    tmp.entities_nuts=tmp.apply(lambda x: ','.join(x.strip() for x in x.entities_nuts if x.strip()), axis=1)\n",
    "    return tmp.sort_values('project_id').drop_duplicates()\n",
    "\n",
    "#######\n",
    "app=prep('evaluated', pp_app, countries, lien)\n",
    "part=prep('successful', pp_part, countries, lien)\n",
    "print(f\"app {len(app)}, part {len(part)}\")\n",
    "\n",
    "lp = part[['project_id', 'generalPic', 'pic', 'country_code_mapping']].drop_duplicates()\n",
    "app = app.merge(lp, how='left', indicator=True).query('_merge==\"left_only\"').drop(columns='_merge')\n",
    "\n",
    "#######\n",
    "struct = pd.concat([app, part], ignore_index=True)\n",
    "struct['nb_stage'] = struct.groupby(['project_id', 'generalPic', 'country_code', 'orderNumber','calculated_pic','stage'])['department'].transform('count')\n",
    "struct = (struct\n",
    "            .rename(columns={'country_code_mapping':'country_code_mapping_dept', 'country_code':'country_code_dept', 'nutsCode':'department_nuts'}))\n",
    "print(f\"size structure {len(struct)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(participation[['stage','project_id','generalPic','orderNumber', 'country_code','country_code_mapping']].drop_duplicates())!=len(participation[['stage','project_id','generalPic','orderNumber', 'country_code','country_code_mapping','role','participates_as']].drop_duplicates()):\n",
    "    print(\"Attention doublon d'une participation avec ajout de role+participates_as\")\n",
    "\n",
    "# merge struct (department) with participation\n",
    "part = participation[['project_id','generalPic','orderNumber', 'country_code','country_code_mapping','stage']].drop_duplicates()\n",
    "print(f\"size part {len(part)}\")\n",
    "part1 = (part\n",
    "        .merge(struct, \n",
    "            how='inner', \n",
    "            left_on=['stage','project_id', 'generalPic', 'orderNumber','country_code_mapping'],  \n",
    "            right_on=['stage','project_id', 'generalPic', 'orderNumber', 'country_code_mapping_dept'])\n",
    "        .drop_duplicates())\n",
    "print(f\"size part1 merge_on['stage','project_id', 'generalPic', 'orderNumber','country']:{len(part1)}\")\n",
    "\n",
    "part2 = (part.merge(part1[['project_id','generalPic','orderNumber', 'country_code','country_code_mapping','stage']], \n",
    "                    how='outer', on=['project_id','generalPic','orderNumber', 'country_code','country_code_mapping','stage'], indicator=True)\n",
    "        .query('_merge == \"left_only\"')\n",
    "        .drop('_merge', axis=1)\n",
    "        .drop_duplicates()\n",
    "        .merge(struct, \n",
    "            how='inner', \n",
    "            left_on=['stage','project_id', 'generalPic', 'orderNumber'],  \n",
    "            right_on=['stage','project_id', 'generalPic', 'orderNumber'])\n",
    "        .drop_duplicates())\n",
    "print(f\"size part2 merge_on['stage','project_id', 'generalPic', 'orderNumber']:{len(part2)}\")\n",
    "\n",
    "part2 = pd.concat([part1, part2], ignore_index=True, axis=0)\n",
    "\n",
    "part3 = (part.merge(part2[['project_id','generalPic','orderNumber', 'country_code','country_code_mapping','stage']], \n",
    "                    how='outer', on=['project_id','generalPic','orderNumber', 'country_code','country_code_mapping','stage'], indicator=True)\n",
    "        .query('_merge == \"left_only\"')\n",
    "        .drop('_merge', axis=1)\n",
    "        .merge(struct.drop(columns='generalPic'), \n",
    "            how='inner', \n",
    "            left_on=['stage','project_id', 'orderNumber'],  \n",
    "            right_on=['stage','project_id', 'orderNumber'])\n",
    "        .drop_duplicates())\n",
    "print(f\"size part3 merge_on['stage','project_id','orderNumber']:{len(part3)}\")\n",
    "\n",
    "part3 = pd.concat([part2, part3], ignore_index=True, axis=0)\n",
    "print(f\"size part3:{len(part3)}\")\n",
    "\n",
    "part4 = (part.merge(part3[['project_id','generalPic','orderNumber', 'country_code','country_code_mapping','stage']], \n",
    "                    how='outer', on=['project_id','generalPic','orderNumber', 'country_code','country_code_mapping','stage'], indicator=True)\n",
    "        .query('_merge == \"left_only\"')\n",
    "        .drop('_merge', axis=1)\n",
    "        .merge(part3.drop(columns='stage'), \n",
    "               how='inner', on=['project_id','generalPic','orderNumber', 'country_code','country_code_mapping']))\n",
    "\n",
    "print(f\"size part4 merge_on with part3 ['project_id','generalPic','orderNumber', 'country_code','country_code_mapping']:{len(part4)}\")\n",
    "\n",
    "part4 = pd.concat([part4, part3], ignore_index=True, axis=0)\n",
    "print(f\"size part4:{len(part4)}\")\n",
    "\n",
    "part5 = (part.merge(part4[['project_id','generalPic','orderNumber', 'country_code','country_code_mapping','stage']], \n",
    "                    how='outer', on=['project_id','generalPic','orderNumber', 'country_code','country_code_mapping','stage'], indicator=True)\n",
    "        .query('_merge == \"left_only\"')\n",
    "        .drop('_merge', axis=1))\n",
    "\n",
    "print(f\"size part5 reste participation:{len(part5)}\")\n",
    "\n",
    "part5 = pd.concat([part4, part5], ignore_index=True, axis=0)\n",
    "print(f\"size part5:{len(part5)}\")\n",
    "\n",
    "part5['nb'] = part5.groupby(['stage', 'project_id', 'generalPic', 'orderNumber'])['stage'].transform('count')\n",
    "part5['nb2'] = part5.groupby(['stage', 'project_id', 'generalPic', 'orderNumber','country_code_mapping'])['stage'].transform('count')\n",
    "part5[['country_code','country_code_mapping']] = part5[['country_code','country_code_mapping']].fillna(part5.groupby(['stage', 'project_id', 'generalPic', 'orderNumber'])[['country_code','country_code_mapping']].ffill())\n",
    "print(f\"size part5 {len(part5)}\")\n",
    "\n",
    "# #remove participation duplicates on ['stage', 'project_id', 'generalPic', 'orderNumber'] and department empty\n",
    "part5 = part5.loc[~((part5.nb>1)&(part5.department.isnull()))]\n",
    "print(f\"size part5 {len(part5)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "structure = (part5\n",
    "            .merge(entities_info[['generalPic', 'legalName', 'businessName',\n",
    "            'category_woven', 'city', 'country_code_mapping', 'country_code',  'country_name_fr', \n",
    "            'id_secondaire', 'entities_id', 'entities_name',  'entities_acronym', 'operateur_num', 'postalCode', \n",
    "            'street', 'webPage']], \n",
    "            how='left', on=['generalPic', 'country_code_mapping', 'country_code'])\n",
    "            .merge(proj[['project_id', 'call_year']].drop_duplicates(), how='left', on=['project_id'])\n",
    "            .drop(columns=['nb_stage', 'nb', 'nb2'])\n",
    "            .drop_duplicates()\n",
    "            )\n",
    "print(len(structure))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "structure = structure.loc[~structure.entities_name.isnull()].drop_duplicates()\n",
    "print(len(structure))\n",
    "\n",
    "cols = ['department', 'entities_acronym', 'entities_name', 'legalName', 'businessName']\n",
    "for i in cols:\n",
    "    structure[f\"{i}_dup\"] = structure.loc[:,i]\n",
    "\n",
    "if any(structure.call_year.isnull()):\n",
    "    print(f\"vérification de l'année (corriger les nuls si existants):\\n{structure.call_year.value_counts(dropna=False)}\")\n",
    "\n",
    "##########\n",
    "cols = ['department_dup', 'legalName_dup', 'businessName_dup', 'entities_acronym_dup','entities_name_dup','street','city']\n",
    "structure = prep_str_col(structure, cols)\n",
    "\n",
    "cedex=\"cedax|cedrex|cdexe|cdex|credex|cedex|cedx|cede|ceddex|cdx|cex|cexex|edex\"\n",
    "structure.loc[structure.postalCode.isnull(), 'postalCode'] = structure.city.str.extract(r\"(\\d+)\")\n",
    "structure['city'] = structure.city.str.replace(r\"\\d+\", ' ', regex=True).str.strip()\n",
    "structure.loc[structure.country_code=='FRA', 'city'] = structure.city.str.replace(cedex, ' ', regex=True).str.strip()\n",
    "structure.loc[structure.country_code=='FRA', 'city'] = structure.city.str.replace(r\"^france$\", '', regex=True).str.strip()\n",
    "\n",
    "##########\n",
    "# creation entities_full = entities_name + entities_acronym et department_tag\n",
    "tmp = structure.loc[(structure.legalName_dup!=structure.businessName_dup)&(~structure.businessName_dup.isnull()), ['generalPic',  'country_code', 'legalName_dup', 'businessName_dup']]\n",
    "tmp['entities_full'] = [x1 if x2 in x1 else x1+' '+x2 for x1, x2 in zip(tmp['legalName_dup'], tmp['businessName_dup'])]\n",
    "\n",
    "if len(structure.drop_duplicates())!=len(structure.merge(tmp[['generalPic', 'country_code', 'legalName_dup', 'businessName_dup', 'entities_full']].drop_duplicates(), how='left', on=['generalPic','businessName_dup', 'legalName_dup','country_code']).drop_duplicates()):\n",
    "    print(\"Attention risque de doublon si merge de tmp et structure\")\n",
    "else:\n",
    "    structure = structure.merge(tmp[['generalPic', 'country_code','legalName_dup', 'businessName_dup', 'entities_full']].drop_duplicates(), how='left', on=['generalPic','legalName_dup', 'businessName_dup', 'country_code']).drop_duplicates()\n",
    "    structure.loc[structure.entities_full.isnull(), 'entities_full'] = structure.entities_name_dup.str.lower()\n",
    "\n",
    "#############\n",
    "societe = pd.read_table('data_files/societe.txt', header=None)\n",
    "structure.loc[structure.entities_full.apply(lambda x: True if re.search(r\"(?=\\b(\"+'|'.join(list(set(societe[0])))+r\")\\b)\", x) else False), 'org1'] = 'societe'\n",
    "societe = societe.loc[societe[0]!='group']\n",
    "structure.loc[(~structure.department_dup.isnull())&(structure.department_dup.apply(lambda x: True if re.search(r\"(?=\\b(\"+'|'.join(list(set(societe[0])))+r\")\\b)\", str(x)) else False)), 'org1'] = 'societe'\n",
    "structure.loc[structure.category_woven=='Entreprise', 'org1'] = 'societe'\n",
    "\n",
    "las = r\"(\\bas(s?)ocia[ctz][aionj]+)|\\b(ev|udruga|sdruzhenie|asbl|aisbl|vzw|biedriba|kyokai|mittetulundusuhing|ry|somateio|egyesulet(e?)|stowarzyszenie|udruzenje|zdruzenie|sdruzeni(e?))\\b|([a-z]*)(verband|vereniging|asotsiatsiya|zdruzenje)\\b|([a-z]*)(verein|forening|yhdistys)([a-z]*)\"\n",
    "structure.loc[structure.entities_full.apply(lambda x: True if re.search(las , x) else False), 'org2'] = 'association'\n",
    "structure.loc[structure.category_woven=='Institutions sans but lucratif (ISBL)', 'org2'] = 'association'\n",
    "\n",
    "structure['typ_from_lib'] = structure[['org1','org2']].stack().groupby(level=0).agg(' '.join)\n",
    "structure.drop(columns=['org1','org2'], inplace=True)\n",
    "\n",
    "# mots vide à suppr\n",
    "stop_word(structure, 'country_code', ['entities_full', 'department_dup'])\n",
    "\n",
    "structure['entities_full'] = structure['entities_full_2'].apply(lambda x: ' '.join([s for s in x if s.strip()]))\n",
    "structure.loc[(~structure.department_dup.isnull()), 'department_dup'] = structure.loc[(~structure.department_dup.isnull()), 'department_dup_2'].apply(lambda x: ' '.join([s for s in x if s.strip()]))\n",
    "\n",
    "structure.drop(columns=['department_dup_2', 'entities_full_2'], inplace=True)\n",
    "structure.mask(structure=='', inplace=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################\n",
    "#########################\n",
    "#################\n",
    "### FRANCE\n",
    "\n",
    "\n",
    "structure_fr = structure.loc[structure.country_code=='FRA']\n",
    "print(len(structure_fr))\n",
    "\n",
    "\n",
    "#############\n",
    "\n",
    "\n",
    "def qualif_organisation(x):\n",
    "    lpattern = [\"cnrs\", \"inria\", \"inrae\", \"ifremer\", \"inserm\", \"cea\", \"ens\", \"fnsp\", \"cirad\", \"ird\", \"chu\", \"universite\", \n",
    "            \"pasteur\", \"curie\", \"irsn\", \"onera\", \"agrocampus\", \"ed\",\"ecole\"]\n",
    "    pattern_ifremer = \"(ifremer)|(in.* fran.* re.* ex.* mer)\"\n",
    "    pattern_cnrs =   \"(ce.* na.* (de )?(la )?re.* sc.[a-z]*)|(fr.* na.* sc.* re.* ce.[a-z]*)|(cnrs)\"\n",
    "    pattern_inria =  \"(in.* na.* (de )?re.* (en )?in.* (et )?(en )?au.[a-z]*)|(inria)\"\n",
    "    pattern_inrae =   \"(in.* na.* (de )?re.* ag.[a-z]*)|(inra)|(inrae)|(irstea)\"\n",
    "    pattern_inserm = \"(in.* na.* (de )?(la )?sa.* (et )?(de )?(la )?re.* me.[a-z]*)|(inserm)\"\n",
    "    pattern_cea =    \"(co.* (a )?l?\\\\'?en.* at.[a-z]*)|(\\\\bcea\\\\b)\"\n",
    "    pattern_ens =    \"(ec.* no.* sup[a-z]*)|(\\\\bens\\\\b)\"\n",
    "    pattern_fnsp =   \"(fo.* na.* (des )?sc.* po.[a-z]*)|(fnsp)|(sciences po)\"\n",
    "    pattern_cirad =  \"(ce.* (de )?co.* in.* (en )?re.* ag.* (pour )?(le )?dev.[a-z]*)|(cirad)\"\n",
    "    pattern_ird =    \"(in.* (de )?re.[a-z]* (pour )?(le )?dev.[a-z]*)|\\\\b(ird)\\\\b|(i r d)\"\n",
    "    pattern_chu = \"((ce.*|ctre|group.*) hos.* (univ.[a-z]*)?)|(univ.* hosp.[a-z]*)|\\\\b(chu|chr|chru)\\\\b|(hospice)\"\n",
    "    pattern_universite =   \"(univ(ersite|ersity|ersitaire))\"\n",
    "    pattern_pasteur =   \"(ins([a-z]*|\\\\.*) pasteur( de)?( lille)?)|(pasteur inst([a-z]*))\"\n",
    "    pattern_curie =    \"(inst([a-z]*|\\\\.*) curie)|(curie inst([a-z]*))\"\n",
    "    pattern_irsn =   \"(in.* (de )?radio.[a-z]* (et )?(de )?sur.[a-z]* nuc.[a-z]*)|(irsn)\"\n",
    "    pattern_onera =  \"(onera)|(off.* na.* (d )?etu.* (et )?(de )?rech.* aero.*)\"\n",
    "    pattern_agrocampus = \"(agrocampus)\"\n",
    "    pattern_ed = \"(doct.* sch.*)|(ec.* doct.*)|\\\\b(ed)\\\\b\"\n",
    "    pattern_ecole = \"(ecole)\"\n",
    "    org = []\n",
    "    for j in lpattern:\n",
    "        pattern = globals()[f\"pattern_{j}\"]\n",
    "        y = re.search(pattern, x)\n",
    "        if y:\n",
    "            org.append(j)\n",
    "    return org\n",
    "\n",
    "structure_fr['org1'] = structure_fr.apply(lambda x: qualif_organisation(x['department_dup']) if isinstance(x['department_dup'], str) else [], axis=1)\n",
    "structure_fr['org2'] = structure_fr.apply(lambda x: qualif_organisation(x['entities_full']) if isinstance(x['entities_full'], str) else [], axis=1)\n",
    "structure_fr['org3'] = structure_fr.apply(lambda x: qualif_organisation(x['entities_name_dup']) if isinstance(x['entities_name_dup'], str) else [], axis=1)\n",
    "\n",
    "\n",
    "structure_fr['org_from_lib'] = structure_fr.apply(lambda x: sorted(set(x['org1'] + x['org2'] + x['org3'])), axis=1)\n",
    "# structure_fr['org_from_lib'] = structure_fr['org_from_lib'].apply(lambda x: ' '.join(x))\n",
    "\n",
    "structure_fr.drop(columns=['org1', 'org2', 'org3'], inplace=True)\n",
    "structure_fr.mask(structure_fr=='', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "    structure_fr=structure_fr.assign(dep_tag=structure_fr.department_dup, lab_tag=structure_fr.entities_full)\n",
    "    cols = ['dep_tag', 'lab_tag']\n",
    "    structure_fr[cols] = structure_fr[cols].apply(lambda x: x.str.replace('international research lab', \"irl\", regex=False))\n",
    "    structure_fr[cols] = structure_fr[cols].apply(lambda x: x.str.replace('joint research unit', \"jru\", regex=False))\n",
    "    structure_fr[cols] = structure_fr[cols].apply(lambda x: x.str.replace('joint research unit', \"jru\", regex=False))\n",
    "    structure_fr[cols] = structure_fr[cols].apply(lambda x: x.str.replace('equipe accueil', \"ea\", regex=False))\n",
    "    structure_fr[cols] = structure_fr[cols].apply(lambda x: x.str.replace(r\"\\bumr(\\s?s\\s?)(u(\\s?)|inserm(\\s?))?(?=(\\d+)?)|\\bu\\\\s?inserm(\\s?)|\\bunit(e?)(?=(\\s?u?\\s?\\d+))|\\binserm\\s?(umr\\s?(s?)|jru)\\s?(u?)|\\binserm(u?)\\s?(?=\\d+)|\\binserm\\s?un\\s?umr\\s?u?\", \"u\", regex=True))\n",
    "    for s in ['umr','upr','uar','irl','emr','umi','usr','fre','gdr','fr']:\n",
    "        structure_fr[cols] = structure_fr[cols].apply(lambda x: x.str.replace(r\"(?<=\\b\"+s+r\")\\s?[a-z]+\\s?(?=\\d+)\", \" \", regex=True))\n",
    "    structure_fr[cols] = structure_fr[cols].apply(lambda x: x.str.replace(r\"\\bu\\s?cnrs|\\bum\\s+r|\\bcnrs\\s?(?=\\d+)|\\bjru\\s?(cnrs|umr)\", \"umr\", regex=True))\n",
    "    structure_fr[cols] = structure_fr[cols].apply(lambda x: x.str.replace(r\"\\bjru\\s?(umi)\", \"umi\", regex=True))\n",
    "    structure_fr[cols] = structure_fr[cols].apply(lambda x: x.str.replace(r\"(\\bce[a-z]* inv[a-z]* cl[a-z]*)|(\\bcl[a-z]* inv[a-z]* ce[a-z]*)|(\\bce[]* cl[a-z]* inv[a-z]*)\", \"cic\", regex=True))\n",
    "    structure_fr.loc[structure_fr.org_from_lib.map(lambda x: \"inserm\" in x), cols] = structure_fr.loc[structure_fr.org_from_lib.map(lambda x: \"inserm\" in x), cols].apply(lambda x: x.str.replace(r\"\\bjru\\b\", 'u', regex=True))\n",
    "    structure_fr.loc[structure_fr.org_from_lib.map(lambda x: \"cnrs\" in x), cols] = structure_fr.loc[structure_fr.org_from_lib.map(lambda x: \"cnrs\" in x), cols].apply(lambda x: x.str.replace(r\"\\bjru\\b\", 'umr', regex=True))\n",
    "\n",
    "    llab = [\"umr\", \"ua\", \"umrs\", \"umr s\",\"ea\", \"u\", \"gdr\", \"fre\", \"fr\", \"frc\", \"fed\", \"je\", \"us\", \"ums\",\n",
    "            \"upr\",\"upesa\",\"ifr\",\"umr a\",\"umemi\",\"epi\",\"eac\", \"ertint\", \"ur\", \"ups\", \"umr m\", \"umr t\",\n",
    "            \"uar\",\"ert\",\"usr\",\"ura\",\"umr d\",\"rtra\",\"ue\",\"ers\",\"cic\",\"ep\",\"umi\", \"unit\", 'emr', 'irl', 'jru']\n",
    "\n",
    "    def labo_sigle(x):\n",
    "        sig = []\n",
    "        for i in llab:\n",
    "            pattern = r\"\\b(\"+i+r\")(?=\\b|\\d+)\\s?[a-z]*\\s?(\\d+)\"\n",
    "            y = re.search(pattern, x)\n",
    "            if y:\n",
    "                sig.append(''.join(y.groups()))\n",
    "        return sig\n",
    "\n",
    "    structure_fr['org1'] = structure_fr.apply(lambda x: labo_sigle(x['dep_tag']) if isinstance(x['dep_tag'], str) else [], axis=1)\n",
    "    structure_fr['org2'] = structure_fr.apply(lambda x: labo_sigle(x['lab_tag']) if isinstance(x['lab_tag'], str) else [], axis=1)\n",
    "    structure_fr['lab_from_lib'] = structure_fr.apply(lambda x: list(set(x['org1'] + x['org2'])), axis=1)\n",
    "    # structure_fr['lab_from_lib'] = structure_fr['lab_from_lib'].apply(lambda x: ';'.join(x))\n",
    "    structure_fr.drop(columns=['org1', 'org2', 'dep_tag', 'lab_tag'], inplace=True)\n",
    "    structure_fr.mask(structure_fr=='', inplace=True)\n",
    "\n",
    "    print(f\"size structure_fr: {len(structure_fr)}\")\n",
    "\n",
    "    #######################################################\n",
    "    #RETOUR ORGANISMES\n",
    "\n",
    "    organisme_back = pd.read_pickle(f\"{PATH_ORG}organisme_back.pkl\").drop_duplicates()\n",
    "    organisme_back = organisme_back.drop(columns=['lib_back', 'location_back']).drop_duplicates()\n",
    "    print(len(organisme_back))\n",
    "\n",
    "    stage_proj = structure_fr[['stage', 'project_id']].drop_duplicates()\n",
    "    organisme1 = organisme_back.merge(stage_proj, how='inner', on='project_id').drop(columns=['proposal_orderNumber']).query(\"stage=='successful'\").drop_duplicates()\n",
    "    print(len(organisme1))\n",
    "    organisme_back.loc[organisme_back.proposal_orderNumber.isnull(), 'proposal_orderNumber'] = organisme_back.orderNumber\n",
    "    organisme2 = organisme_back.merge(stage_proj, how='inner', on='project_id').drop(columns=['orderNumber']).rename(columns={'proposal_orderNumber':'orderNumber'}).query(\"stage=='evaluated'\").drop_duplicates()\n",
    "    print(len(organisme2))\n",
    "    oback = pd.concat([organisme1, organisme2], ignore_index=True)\n",
    "    oback = (oback.groupby(['stage','project_id', 'generalPic', 'pic', 'orderNumber'], dropna=False)\n",
    "            .agg(lambda x: ';'.join(map(str, filter(None, x.dropna().unique())))).reset_index())\n",
    "\n",
    "    oback[['labo_back', 'org_back']] = oback[['labo_back', 'org_back']].apply(lambda x: x.str.lower())\n",
    "    # oback['labo_back'] = oback.labo_back.str.split(';').tolist()\n",
    "        \n",
    "    oback.mask(oback=='', inplace=True)\n",
    "\n",
    "    ###########################################################################################################\n",
    "\n",
    "    # MERGE ORGANISMES ET STRUCTURE\n",
    "\n",
    "    tmp = structure_fr.merge(oback, how='outer', on=['stage','project_id','generalPic', 'orderNumber'], indicator=True)\n",
    "    keep = tmp.loc[tmp._merge!='right_only'] #suppr les lignes oback en +\n",
    "\n",
    "    # traitement cea sans orderNumber\n",
    "    tmp = tmp.loc[(tmp._merge=='right_only')&(tmp.org_back=='CEA')]\n",
    "    cea = keep[(keep.generalPic=='999992401')].drop(columns='_merge').sort_values(['project_id','generalPic'])\n",
    "    print(len(cea))\n",
    "    cea = cea.drop(columns=list(oback.columns.difference(structure_fr.columns)))\n",
    "    cea = cea.merge(oback[oback.generalPic=='999992401'].dropna(axis=1, how='all'), how='left', on=['stage','project_id','generalPic'], indicator=True)\n",
    "    keep = keep[keep.generalPic!='999992401']\n",
    "    keep = pd.concat([keep, cea], ignore_index=True).drop(columns=['pic', 'role', 'participates_as']).sort_values(['project_id', 'stage', 'generalPic'])\n",
    "\n",
    "    for i in ['rnsr_back','labo_back','org_back', 'city_back']:\n",
    "        keep[i] = keep[i].apply(lambda x: x.split(';') if isinstance(x, str) else [])  \n",
    "\n",
    "    print(len(keep))\n",
    "\n",
    "    keep['org_merged'] = keep.apply(lambda x: list(set(x['org_back'] + x['org_from_lib'])), axis=1)\n",
    "    keep.mask(keep=='', inplace=True)\n",
    "\n",
    "    keep['lab_merged'] = keep.apply(lambda x: list(set(x['labo_back'] + x['lab_from_lib'])), axis=1)\n",
    "\n",
    "    pattern=r'^[0-9]{9}[A-Z]{1}($|;)'\n",
    "    keep.loc[keep.id_secondaire.str.contains('0', na=True), 'id_secondaire'] = np.nan\n",
    "    keep['id_secondaire'] = keep['id_secondaire'].apply(lambda x: x.split(';') if isinstance(x, str) else [])\n",
    "    keep['rnsr_merged'] = keep.id_secondaire.apply(lambda x: [i for i in x if re.search(pattern, i)])\n",
    "    keep['rnsr_merged'] = keep.apply(lambda x: list(set(x['rnsr_merged'] + x['rnsr_back'])), axis=1)\n",
    "    keep.mask(keep=='', inplace=True)\n",
    "\n",
    "    ################\n",
    "    labo = keep.loc[((keep.org_merged.str.len() > 0)|(~keep.operateur_num.isnull())), \n",
    "            ['call_year','stage','project_id', 'generalPic', 'entities_full', 'department_dup',\n",
    "        'typ_from_lib', 'org_merged', 'rnsr_merged', 'lab_merged',\n",
    "        'cp_back', 'city_back', 'operateur_num','category_woven']]\n",
    "    print(len(labo))\n",
    "\n",
    "    lab_a_ident = (labo.loc[(keep.rnsr_merged.str.len() == 0), \n",
    "                            ['project_id', 'generalPic', 'call_year','department_dup','entities_full','lab_merged', 'city_back']]\n",
    "                )\n",
    "\n",
    "    # lab_a_ident['org_merged'] = lab_a_ident['org_merged'].astype(str)          \n",
    "    lab_a_ident = (lab_a_ident\n",
    "                    .set_index(['call_year','department_dup','entities_full'])\n",
    "                    .explode('lab_merged').explode('city_back')\n",
    "                    .reset_index()\n",
    "                    .drop_duplicates()\n",
    "                )\n",
    "    print(len(lab_a_ident))\n",
    "\n",
    "    #################\n",
    "    # 1er step matching by id\n",
    "    ident_by_id = lab_a_ident.loc[~lab_a_ident.lab_merged.isnull(), ['call_year', 'entities_full', 'lab_merged', 'city_back']].drop_duplicates()\n",
    "    # ident_by_id\n",
    "\n",
    "    #######\n",
    "    org = ident_by_id.rename(columns={\"city_back\": \"city\", \"lab_merged\": \"labo\", 'entities_full':'supervisor'})\n",
    "    for f in ['supervisor', 'labo', 'city']:\n",
    "        org[f] = org[f].fillna('')\n",
    "    org.head()\n",
    "\n",
    "    df=org.assign(match = None)\n",
    "    lab_id=pd.DataFrame()\n",
    "\n",
    "    ######\n",
    "\n",
    "    typ=\"rnsr\"\n",
    "    now = time.strftime(\"%H:%M:%S\")\n",
    "\n",
    "    for i, row in df.iterrows():\n",
    "        query=\"{} {} {}\".format(row['city'], row['labo'], row['supervisor'])\n",
    "        strategies = [[['rnsr_code_number', 'rnsr_supervisor_name', 'rnsr_city']],\n",
    "                    [['rnsr_code_number', 'rnsr_supervisor_name']],\n",
    "                    [['rnsr_code_number', 'rnsr_city']]]\n",
    "        matcher(df, i, typ, query, strategies, year=row['call_year'])\n",
    "\n",
    "    ###\n",
    "\n",
    "    x=df.loc[df.match.isnull()]\n",
    "    lab_id=pd.concat([df.loc[~df.match.isnull()], lab_id], ignore_index=True)\n",
    "    print(len(lab_id))\n",
    "\n",
    "    lab_id['match2'] = lab_id['match'].astype(str)\n",
    "    lab_id['match2'] = lab_id.groupby('labo', as_index=False).pipe(lambda x: x['match2'].transform('nunique'))\n",
    "    if len(lab_id.loc[lab_id.match2>1, ['labo','match']].sort_values('labo'))>1:\n",
    "        print('un même identifiant de labo a ++ de rnsr: a vérifier')\n",
    "    else: print('ok')\n",
    "\n",
    "    #save first step into matcher\n",
    "    lab_id = lab_id.drop(columns=['q', 'match2'])\n",
    "    lab_id.mask(lab_id=='', inplace=True)\n",
    "    work_csv(lab_id, 'ident_lab1')\n",
    "\n",
    "    #####\n",
    "    # 2d step matching by name\n",
    "    lab_ident1 = lab_a_ident.merge(lab_id, how='left', left_on=['entities_full', 'call_year', 'lab_merged', 'city_back'], right_on=['supervisor', 'call_year', 'labo', 'city'], indicator=True)\n",
    "    lab_a_ident = lab_ident1.loc[lab_ident1._merge=='left_only'].drop(columns=['_merge', 'match', 'labo', 'city', 'supervisor'])\n",
    "    ident_by_lib = lab_a_ident[['call_year', 'department_dup', 'lab_merged', 'entities_full', 'city_back']]\n",
    "    ident_by_lib['labo'] = ident_by_lib[[ 'department_dup', 'lab_merged']].stack().groupby(level=0).apply(lambda x: ' '.join(x))\n",
    "    org = ident_by_lib.rename(columns={\"city_back\": \"city\", 'entities_full':'supervisor'}).drop_duplicates()\n",
    "    print(len(org))\n",
    "    for f in ['supervisor', 'labo', 'city']:\n",
    "        org[f] = org[f].fillna('')\n",
    "\n",
    "    df=org.assign(match = None)\n",
    "\n",
    "    #######################\n",
    "    typ=\"rnsr\"\n",
    "    now = time.strftime(\"%H:%M:%S\")\n",
    "\n",
    "    for i, row in df.iterrows():\n",
    "        query=\"{} {} {}\".format(row['city'], row['labo'], row['supervisor'])\n",
    "\n",
    "        strategies = [\n",
    "            [['rnsr_acronym', 'rnsr_name', 'rnsr_supervisor_name']],\n",
    "    #                   [['rnsr_acronym', 'rnsr_name' 'rnsr_supervisor_name']],\n",
    "    #                   [['rnsr_name', 'rnsr_supervisor_name', 'rnsr_city']],\n",
    "                    [[ 'rnsr_name', 'rnsr_supervisor_name']],\n",
    "                    [['rnsr_acronym', 'rnsr_supervisor_name']]\n",
    "        ]\n",
    "        matcher(df, i, typ, query, strategies, year=row['call_year'])\n",
    "\n",
    "    x=df.loc[df.match.isnull()]\n",
    "    lab_id=pd.concat([df.loc[~df.match.isnull()], lab_id], ignore_index=True)\n",
    "\n",
    "    lab_id = lab_id.drop(columns=['q'])\n",
    "    lab_id.mask(lab_id=='', inplace=True)\n",
    "    lab_id.to_pickle(f\"{PATH_WORK}match_lab2.pkl\", compression='gzip')\n",
    "\n",
    "    # lab_id=pd.read_csv(f\"{PATH_WORK}ident_lab2.csv\", sep=';')\n",
    "    # lab_id['call_year']=lab_id['call_year'].astype(str)\n",
    "    # lab_id['match'] = lab_id.match.apply(lambda x: ast.literal_eval(x))\n",
    "    ####################################################\n",
    "\n",
    "    lab_ident2 = lab_a_ident.merge(lab_id, how='left', left_on=['entities_full', 'call_year', 'lab_merged', 'city_back', 'department_dup'], right_on=['supervisor', 'call_year',  'lab_merged', 'city', 'department_dup'], indicator=True)\n",
    "    lab_a_ident = lab_ident2.loc[lab_ident2._merge=='left_only'].drop(columns=['_merge', 'match', 'labo', 'city', 'supervisor'])\n",
    "    lab_a_ident = lab_a_ident.loc[~lab_a_ident.department_dup.isnull()]\n",
    "    print(len(lab_a_ident))\n",
    "\n",
    "    df=structure.loc[structure.country_code!='FRA', ['entities_full', 'country_code']].drop_duplicates()\n",
    "    df=df.fillna('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "entities_tmp=entities_all.loc[((entities_all.country_code=='FRA')&(entities_all.rnsr_merged.str.len()==0))|((entities_all.country_code!='FRA')&(entities_all.entities_id.str.contains('pic'))), ['project_id','generalPic','country_code', 'entities_id', 'entities_name']]\n",
    "perso = perso[['call_year', 'thema_name_en', 'destination_name_en' ,'project_id', 'country_code', 'generalPic', 'title', 'last_name', 'first_name', 'tel_clean', 'domaine_email', 'contact', 'orcid_id']]\n",
    "\n",
    "pp = perso[['project_id','generalPic' ,'contact', 'orcid_id']].drop_duplicates().merge(entities_tmp, how='inner', on=['project_id','generalPic'])\n",
    "# pp.mask(pp=='', inplace=True)\n",
    "pp = pp.fillna('')\n",
    "# pp = pp.loc[pp.orcid_id=='', ['contact', 'orcid_id']].drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def openalex_name(author):\n",
    "    url = f\"https://api.openalex.org/authors?filter=display_name.search:{author.get('name')}\"\n",
    "    nb_openalex=requests.get(url).json().get(\"meta\").get('count')\n",
    "    result={}\n",
    "    if nb_openalex>0:\n",
    "        for n in range(nb_openalex): \n",
    "            author_openalex = requests.get(url).json().get(\"results\")[n]\n",
    "            result.update({'display_name':author_openalex.get('display_name'), 'openalex_id':author_openalex.get('id'), 'affiliations':author_openalex.get('affiliations'), 'topics':author_openalex.get('topics'),  'x_concepts':author_openalex.get('x_concepts'), 'ids':author_openalex.get('ids'), 'display_name_alternatives':author_openalex.get('display_name_alternatives')})\n",
    "            author.update(result)\n",
    "            df=pd.concat([df, pd.json_normalize(author)])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests, pandas as pd, time\n",
    "df=pd.DataFrame()\n",
    "author = {\n",
    "\"name\": 'elyaakoubi mustapha',\n",
    "# \"name\": 'jeremy peglion',\n",
    "\"orcid\": ''\n",
    "}\n",
    "try:\n",
    "    if author.get(\"orcid\"):\n",
    "    # Get author by Orcid\n",
    "        url = f\"https://api.openalex.org/authors/orcid:{author.get('orcid')}?mailto=bso@recherche.gouv.fr\"\n",
    "        author_openalex = requests.get(url).json().get(\"results\")\n",
    "        author.update({'display_name':author_openalex.get('display_name'), 'openalex_id':author_openalex.get('id'), 'affiliations':author_openalex.get('affiliations'), 'topics':author_openalex.get('topics'),  'x_concepts':author_openalex.get('x_concepts'), 'ids':author_openalex.get('ids'), 'display_name_alternatives':author_openalex.get('display_name_alternatives')})\n",
    "        df=pd.concat([df, pd.json_normalize(author)])\n",
    "    else:\n",
    "        url = f\"https://api.openalex.org/authors?filter=display_name.search:{author.get('name')}\"\n",
    "        nb_openalex=requests.get(url).json().get(\"meta\").get('count')\n",
    "        result={}\n",
    "        if nb_openalex>0:\n",
    "            for n in range(nb_openalex): \n",
    "                author_openalex = requests.get(url).json().get(\"results\")[n]\n",
    "                result.update({'display_name':author_openalex.get('display_name'), 'openalex_id':author_openalex.get('id'), 'affiliations':author_openalex.get('affiliations'), 'topics':author_openalex.get('topics'),  'x_concepts':author_openalex.get('x_concepts'), 'ids':author_openalex.get('ids'), 'display_name_alternatives':author_openalex.get('display_name_alternatives')})\n",
    "                author.update(result)\n",
    "                df=pd.concat([df, pd.json_normalize(author)])\n",
    "except requests.exceptions.HTTPError as http_err:\n",
    "    print(f\"\\n{time.strftime(\"%H:%M:%S\")}-> HTTP error occurred: {http_err}\")\n",
    "except requests.exceptions.RequestException as err:\n",
    "    print(f\"\\n{time.strftime(\"%H:%M:%S\")}-> Error occurred: {err}\")                    \n",
    "except Exception as e:\n",
    "    print(f\"\\n{time.strftime(\"%H:%M:%S\")}-> An unexpected error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if author_openalex:\n",
    "        # author.update({'openalex_id':author_openalex.get('id'),'affiliations':author_openalex.get('affiliations'), 'topics':author_openalex.get('topics'),  'x_concepts':author_openalex.get('x_concepts'), 'orcid_tmp':author_openalex.get('orcid'), 'display_name_alternatives':author_openalex.get('display_name_alternatives')})\n",
    "        result.append(author_openalex)\n",
    "# pd.json_normalize(result)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perso.orcid_id.value_counts()\n",
    "x=perso[['project_id','generalPic' ,'contact', 'orcid_id']].drop_duplicates().merge(entities_tmp, how='inner', on=['project_id','generalPic'])\n",
    "# x=perso[['project_id','generalPic' ,'contact', 'orcid_id']].drop_duplicates()\n",
    "x.orcid_id.value_counts()\n",
    "len(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "print(\"\\n### FP7\")\n",
    "def call_api():\n",
    "    call=pd.read_json(open(f\"data_files/FP7_calls.json\", 'r+', encoding='utf-8'))\n",
    "    call = pd.DataFrame(call)\n",
    "    call['call_budget'] = call['call_budget'].str.replace(',', '').astype('float')\n",
    "    return call\n",
    "call=call_api()\n",
    "\n",
    "def ref_select(FP):\n",
    "    ref_source = ref_source_load('ref')\n",
    "    # traitement ref select le FP, id non null ou/et ZONAGE non null\n",
    "    ref = ref_source_2d_select(ref_source, FP)\n",
    "    return ref\n",
    "ref, genPic_to_new=ref_select('FP7')\n",
    "\n",
    "def FP7_load():\n",
    "    FP7_PATH=f'{PATH}FP7/2022/'\n",
    "    _FP7 = pd.read_pickle(f\"{FP7_PATH}FP7_data.pkl\")\n",
    "    _FP7.rename(columns={'name_source':'legalName', 'acronym_source':'businessName'}, inplace=True)\n",
    "    print(f\"size _FP7 load: {len(_FP7)}\\n{_FP7.columns}\")\n",
    "    return _FP7\n",
    "_FP7=FP7_load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "country = pd.read_csv(f\"{PATH_SOURCE}H2020/country_current.csv\", sep=';', encoding='utf-8')\n",
    "def FP7_cleaning(_FP7, country):\n",
    "    _FP7 = _FP7.loc[~_FP7.status_code.isin(['INELIGIBLE','WITHDRAWN'])]\n",
    "    _FP7.loc[_FP7.status_code=='Project Closed', 'status_code'] = 'CLOSED'\n",
    "    _FP7.loc[_FP7.status_code=='Project Terminated', 'status_code'] = 'TERMINATED'\n",
    "\n",
    "    _FP7.loc[_FP7.participant_type_code=='N/A', 'participant_type_code'] = 'NA'\n",
    "    _FP7['role'] = _FP7['role'].str.lower()\n",
    "    _FP7.loc[_FP7.role=='participant', 'role'] = 'partner'\n",
    "    _FP7['coordination_number']=np.where(_FP7['role']=='coordinator', 1, 0)\n",
    "    _FP7.loc[(_FP7.generalPic=='998133396')&(_FP7.countryCode=='ZZ'), 'country_code_mapping'] = 'USA' # bristol meyer\n",
    "    print(f\"- size _FP7 after clean status: {len(_FP7)}, size with id: {len(_FP7.loc[~_FP7.id.isnull()])}\")\n",
    "    \n",
    "    zz = _FP7.loc[(_FP7.country_code_mapping=='ZZZ')]\n",
    "    print(f\"- size _FP7 sans country_code: {len(zz)}\")\n",
    "    zz = ref.loc[ref.generalPic.isin(zz.generalPic.unique())]\n",
    "    _FP7 = _FP7.merge(zz, how='left', on='generalPic', suffixes=['','_ref'])\n",
    "    for i in ['id', 'country_code_mapping', 'ZONAGE']:\n",
    "        _FP7.loc[~_FP7[f\"{i}_ref\"].isnull(), i] = _FP7[f\"{i}_ref\"]\n",
    "    _FP7 = _FP7.drop(_FP7.filter(regex='_ref$').columns, axis=1)\n",
    "    print(f\"- size _FP7 with country: {len(_FP7)}, {_FP7.loc[_FP7.stage=='successful', 'funding'].sum()}\")\n",
    "    \n",
    "    p = _FP7[['generalPic', 'country_code_mapping','country_code']].drop_duplicates()\n",
    "    print(f\"- size de p: {len(p)}\")\n",
    "    #lien part et ref\n",
    "    p = p.merge(ref, how='outer', on=['generalPic', 'country_code_mapping'], indicator=True).drop_duplicates()\n",
    "    p = p.loc[p._merge.isin(['both', 'left_only'])]\n",
    "    # print(f\"cols de p: {p.columns}\")\n",
    "\n",
    "    # p1 pic+ccm commun\n",
    "    p1 = p.loc[p['_merge']=='both'].drop(columns=['_merge', 'country_code'])\n",
    "    print(f\"- size p1 pic+cc: {len(p1)}\")\n",
    "\n",
    "    # p2 pic cc\n",
    "    p2 = (p.loc[p['_merge']=='left_only'].drop(columns=['_merge', 'id', 'ZONAGE'])\n",
    "        .merge(ref.rename(columns={'country_code_mapping':'country_code'}), \n",
    "                how='inner', on=['generalPic', 'country_code']).drop_duplicates()\n",
    "        .drop(columns='country_code'))\n",
    "    print(f\"- size p2 pic cc_parent: {len(p2)}\")\n",
    "\n",
    "    # acteurs sans identifiant dont le pic à plusieurs pays ou le pic certaines participations ont un identifiant et pas d'autres \n",
    "    p3 = (p.loc[p['_merge']=='left_only'].drop(columns=['_merge', 'country_code_mapping', 'id', 'ZONAGE'])\n",
    "        .merge(ref, how='inner', on=['generalPic']).drop_duplicates())\n",
    "    if not p3.empty:\n",
    "        print(f\"1 - A faire si possible, vérifier pourquoi des participations avec pic identiques ont un id ou pas nb pic: {len(p3.generalPic.unique())}\")\n",
    "\n",
    "    if 'p2' in globals() or 'p2' in locals():\n",
    "        p1 = pd.concat([p1,p2], ignore_index=True).drop_duplicates()\n",
    "        print(f\"2 - size de new p: {len(p)}, cols: {p.columns}\") \n",
    "\n",
    "    FP7 = (_FP7.drop(columns=['id', 'ZONAGE', 'country_code'])\n",
    "            .merge(p1[['generalPic', 'country_code_mapping', 'id', 'ZONAGE']], \n",
    "                how='left', on=['generalPic', 'country_code_mapping']))\n",
    "    print(f\"- size _FP7 with ref: {len(_FP7)}, size FP7: {len(FP7)},  size with id: {len(FP7.loc[~FP7.id.isnull()])}\")\n",
    "    \n",
    "    FP7 = FP7.merge(country[['country_code_mapping', 'country_name_mapping', 'country_code']].drop_duplicates(), how='left', on='country_code_mapping')\n",
    "    # FP7.loc[~FP7.ZONAGE.isnull(), 'country_code'] = FP7.ZONAGE\n",
    "    if any(FP7.country_code.isnull()):\n",
    "        print(f\"country_code null {FP7.loc[FP7.country_code.isnull(), ['country_code_mapping', 'country_name_mapping']].drop_duplicates()}\")\n",
    "        FP7.loc[FP7.country_code_mapping=='GUF', 'country_code'] = 'FRA'\n",
    "        FP7.loc[FP7.country_code_mapping=='GUF', 'country_name_mapping'] = 'French Guiana'\n",
    "        FP7.loc[FP7.country_code_mapping.isin(['SGS', 'IOT']), 'country_code'] = 'GBR'\n",
    "        FP7.loc[FP7.country_code_mapping=='IOT', 'country_name_mapping'] = 'British Indian Ocean Territory'\n",
    "        FP7.loc[FP7.country_code_mapping=='SGS', 'country_name_mapping'] = 'South Georgia and the South Sandwich Islands'\n",
    "\n",
    "    cc = country.drop(columns=['country_code_mapping', 'country_name_mapping', 'countryCode', 'countryCode_parent']).drop_duplicates()\n",
    "    FP7 = FP7.merge(cc, how='left', on='country_code')\n",
    "    FP7.loc[FP7.country_code_mapping=='ZOE', 'country_name_mapping'] = 'European organisations area'\n",
    "\n",
    "    FP7.loc[FP7.country_code_mapping=='ZOE', 'country_code'] = 'ZOE'\n",
    "    FP7.loc[FP7.country_code=='ZOE', 'country_name_fr'] = 'Union Européenne'\n",
    "    FP7.loc[FP7.country_code=='ZOE', 'country_name_en'] = 'European organisations area'\n",
    "\n",
    "    print(f\"size FP7 with country assoc: {len(FP7)},\\ncols: {FP7.columns}\")    \n",
    "    return FP7\n",
    "FP7=FP7_cleaning(_FP7, country)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FP7.loc[(FP7.country_code=='DEU')&(FP7.stage=='successful')&(FP7.pilier!='EURATOM')].funding.sum()\n",
    "# FP7.pilier.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def FP7_entities(FP7, country):\n",
    "    print(\"\\n## FP7 entities\")\n",
    "    # part.country_code.unique()\n",
    "    entities = FP7.loc[~FP7.id.isnull(), ['generalPic','id', 'country_code_mapping']].drop_duplicates()\n",
    "    print(f\"- size entities {len(entities)}\")\n",
    "    if any(entities.id.str.contains(';')):\n",
    "        entities = entities.assign(id_extend=entities.id.str.split(';')).explode('id_extend')\n",
    "        entities.loc[(entities.id.str.contains(';', na=False))&(entities.id_extend.str.len()==14), 'id_extend'] = entities.loc[(entities.id.str.contains(';', na=False))&(entities.id_extend.str.len()==14)].id_extend.str[:9]\n",
    "        entities = entities.drop_duplicates()\n",
    "        entities_size_to_keep = len(entities)\n",
    "        print(f\"2 - size entities si multi id -> entities_size_to_keep = {entities_size_to_keep}\")\n",
    "\n",
    "    ror = pd.read_pickle(f\"{PATH_REF}ror_df.pkl\")\n",
    "    entities_tmp = merge_ror(entities, ror, country)\n",
    "    print(f\"size entities_tmp after add ror_info: {len(entities_tmp)}, entities_size_to_keep: {entities_size_to_keep}\")\n",
    "\n",
    "\n",
    "    # PAYSAGE\n",
    "    ### si besoin de charger paysage pickle\n",
    "    paysage = pd.read_pickle(f\"{PATH_REF}paysage_df.pkl\")\n",
    "    if any(paysage.groupby('id')['id_clean'].transform('count')>1):\n",
    "        print(f\"1 - paysage doublon oublié: {paysage[paysage.groupby('id')['id_clean'].transform('count')>1][['id', 'id_clean']].sort_values('id')}\")\n",
    "        paysage = paysage.loc[~((paysage.id_clean=='vey7g')&(paysage.id.str.contains('265100057', na=False)))]    \n",
    "    \n",
    "    paysage_category = pd.read_pickle(f\"{PATH_SOURCE}paysage_category.pkl\")\n",
    "    cat_filter = category_paysage(paysage_category)\n",
    "    entities_tmp = merge_paysage(entities_tmp, paysage, cat_filter)\n",
    "\n",
    "    sirene = pd.read_pickle(f\"{PATH_REF}sirene_df.pkl\")\n",
    "    entities_tmp = merge_sirene(entities_tmp, sirene)\n",
    "\n",
    "    # traitement des id identifiés mais sans referentiels liés\n",
    "    entities_tmp.loc[(entities_tmp.entities_id.isnull())&(~entities_tmp.id_extend.str.contains('-', na=False)), 'entities_id'] = entities_tmp['id_extend']\n",
    "\n",
    "    entities_tmp['siren']=entities_tmp.loc[entities_tmp.entities_id.str.contains('^[0-9]{9}$|^[0-9]{14}$', na=False)].entities_id.str[:9]\n",
    "    entities_tmp.loc[entities_tmp.siren.isnull(), 'siren']=entities_tmp.paysage_siren\n",
    "\n",
    "    #groupe\n",
    "\n",
    "    # recuperation tous les siren pour lien avec groupe -> creation var SIREN \n",
    "    entities_tmp.loc[~entities_tmp.siren.isnull(), \"siren\"] = entities_tmp.loc[~entities_tmp.siren.isnull(), \"siren\"].str.split().apply(set).str.join(\";\")\n",
    "\n",
    "    if any(entities_tmp.siren.str.contains(';', na=False)):\n",
    "        print(\"1 - ATTENTION faire code pour traiter deux siren différents -> ce qui serait bizarre qu'il y ait 2 siren\")\n",
    "    else:\n",
    "        ### si besoin de charger groupe\n",
    "        file_name = f\"{PATH_REF}H20_groupe.pkl\"\n",
    "        groupe = pd.read_pickle(file_name)\n",
    "        print(f\"2 - taille de entities_tmp avant groupe:{len(entities_tmp)}\")\n",
    "\n",
    "        entities_tmp=entities_tmp.merge(groupe, how='left', on='siren')\n",
    "\n",
    "        # entities_tmp.loc[~entities_tmp.groupe_id.isnull(), 'entities_id']= entities_tmp.groupe_id\n",
    "        # entities_tmp.loc[~entities_tmp.groupe_id.isnull(), 'entities_acronym'] = entities_tmp.groupe_acronym\n",
    "        # entities_tmp.loc[~entities_tmp.groupe_id.isnull(), 'entities_name'] = entities_tmp.groupe_name\n",
    "\n",
    "        # entities_tmp.loc[entities_tmp.entities_id.str.contains('gent', na=False), 'siren_cj'] = 'GE_ENT'\n",
    "        \n",
    "        # entities_tmp = entities_tmp.drop(['groupe_id','groupe_name','groupe_acronym'], axis=1).drop_duplicates()\n",
    "        print(f\"- size entities_tmp after groupe {len(entities_tmp)}\")\n",
    "\n",
    "    entities_tmp = entities_tmp.merge(get_source_ID(entities_tmp, 'entities_id'), how='left', on='entities_id')\n",
    "        # traitement catégorie\n",
    "    # entities_tmp = category_cleaning(entities_tmp, sirene)\n",
    "    entities_tmp = category_woven(entities_tmp, sirene)\n",
    "    entities_tmp = category_agreg(entities_tmp)\n",
    "    return  entities_tmp\n",
    "entities_tmp=FP7_entities(FP7, country)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n## FP7 calculation\")\n",
    "print(f\"- size part before: {len(FP7)}\")\n",
    "part1 = (FP7[['project_id', 'participant_order', 'role', 'generalPic', 'global_costs',\n",
    "    'participant_type_code', 'legalName', 'businessName', 'countryCode', 'nutsCode',\n",
    "    'funding', 'status.x', 'ADRESS', 'city', 'post_code', 'pme', 'stage', 'nom', 'countryCode_parent', 'vat_id',\n",
    "    'country_code_mapping', 'participant_id', 'number_involved', 'coordination_number', 'id', 'ZONAGE',\n",
    "    'country_name_mapping', 'country_code', 'country_name_en','country_association_code', 'country_association_name_en',\n",
    "    'country_group_association_code', 'country_group_association_name_en','country_group_association_name_fr', \n",
    "    'country_name_fr', 'article1', 'article2']]\n",
    "        .merge(entities_tmp, how='left', on=['generalPic', 'country_code_mapping', 'id']))\n",
    "print(f\"- size part before: {len(part1)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "part1 = (FP7[['project_id', 'participant_order', 'role', 'generalPic', 'global_costs',\n",
    "    'participant_type_code', 'legalName', 'businessName', 'countryCode', 'nutsCode',\n",
    "    'funding', 'status.x', 'ADRESS', 'city', 'post_code', 'pme', 'stage', 'nom', 'countryCode_parent', 'vat_id',\n",
    "    'country_code_mapping', 'participant_id', 'number_involved', 'coordination_number', 'id', 'ZONAGE',\n",
    "    'country_name_mapping', 'country_code', 'country_name_en','country_association_code', 'country_association_name_en',\n",
    "    'country_group_association_code', 'country_group_association_name_en','country_group_association_name_fr', \n",
    "    'country_name_fr', 'article1', 'article2']]\n",
    "        .merge(entities_tmp, how='left', on=['generalPic', 'country_code_mapping', 'id']))\n",
    "\n",
    "part2=(part1.loc[part1.entities_name.isnull()].drop_duplicates())\n",
    "part3=(part2.sort_values(['legalName', 'businessName'], ascending=False)\n",
    "    .groupby(['generalPic', 'country_code_mapping'])\n",
    "    .first().reset_index()[['generalPic', 'country_code_mapping', 'legalName', 'businessName']]\n",
    "    .rename(columns={'legalName':'entities_name', 'businessName':'entities_acronym'}))\n",
    "\n",
    "part2 = (part2.drop(columns=['entities_name', 'entities_acronym', 'nom'])\n",
    "        .merge(part3, how='left', on=['generalPic', 'country_code_mapping']))\n",
    "part2['entities_name'] = part2.entities_name.str.capitalize().str.strip()\n",
    "part2['entities_id'] = \"pic\"+part2.generalPic.map(str)\n",
    "\n",
    "part1=part1.loc[~part1.entities_name.isnull()].drop_duplicates()\n",
    "\n",
    "part1=pd.concat([part1, part2], ignore_index=True).assign(number_involved=1)\n",
    "\n",
    "part1['nb'] = part1.id.str.split(';').str.len()\n",
    "for i in ['funding', 'coordination_number', 'number_involved']:\n",
    "    part1[i] = np.where(part1['nb']>1, part1[i]/part1['nb'], part1[i])\n",
    "\n",
    "# 'requestedGrant'\n",
    "print(f\"- size part after: {len(part1)}\")\n",
    "\n",
    "if any(part1.entities_id=='nan')|any(part1.entities_id.isnull()):\n",
    "    print(f\"1 - attention il reste des entities sans entities_id valides\")\n",
    "\n",
    "type_entity = pd.read_json(open('data_files/legalEntityType.json', 'r', encoding='UTF-8'))\n",
    "# part1.loc[part1.participant_type_code=='N/A', 'participant_type_code'] = 'NA'\n",
    "part1 = (part1.merge(type_entity, how='left', left_on='participant_type_code', right_on='cordis_type_entity_code')\n",
    ".drop(columns='participant_type_code'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "part1.loc[(part1.country_code=='DEU')&(part1.stage=='successful')&(part1.project_id.isin(pp))].funding.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(part1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# # # gestion code nuts\n",
    "# nuts = pd.read_pickle(\"data_files/nuts_complet.pkl\")\n",
    "# nuts = (nuts[['nuts_code_2013','nutsCode', 'lvl1Description', 'lvl2Description', 'lvl3Description']]\n",
    "#         .drop_duplicates()\n",
    "#         .rename(columns={'nuts_code_2013':'nuts_code_tmp', 'nutsCode':'nuts_code','lvl1Description':'region_1_name', 'lvl2Description': 'region_2_name', 'lvl3Description':'regional_unit_name'}))\n",
    "# # nuts['region_1_name'] = nuts['region_1_name'].str.title()\n",
    "# print(len(nuts))\n",
    "\n",
    "part1['nuts_code_tmp'] = np.where(part1.nutsCode.str.len()<3, np.nan, part1.nutsCode)\n",
    "\n",
    "print(f\"- size part1 with code after cleanup nuts: {len(part1[~part1.nuts_code_tmp.isnull()])}\")\n",
    "\n",
    "nuts = nuts.loc[(nuts.nuts_code_tmp.isin(part1.nuts_code_tmp.unique()))&(~nuts.nuts_code_tmp.isnull())]\n",
    "part1 = part1.merge(nuts, how='left', on='nuts_code_tmp').drop_duplicates()\n",
    "print(f\"- nuts code without name: {len(part1[(~part1.nuts_code.isnull())&(part1.region_1_name.isnull())])}\")\n",
    "\n",
    "# print(part1.groupby(['stage'], dropna=True )['nuts_code'].size())\n",
    "print(part1.loc[part1.stage=='successful', 'funding'].sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "instr = pd.read_csv('data_files/instru_nomenclature.csv', sep=';')\n",
    "act=pd.read_json(open(\"data_files/actions_name.json\", 'r', encoding='utf-8'))\n",
    "msca_correspondence = pd.read_table('data_files/msca_correspondence.csv', sep=\";\").drop(columns='framework')\n",
    "erc_correspondence = pd.read_json(open(\"data_files/ERC_correspondance.json\", 'r', encoding='utf-8'))\n",
    "thema = pd.read_json(open(\"data_files/thema.json\", 'r', encoding='utf-8'))\n",
    "destination = pd.read_json(open(\"data_files/destination.json\", 'r', encoding='utf-8'))\n",
    "\n",
    "def themes_cleaning(FP7):\n",
    "    print(\"## FP7 themes\")\n",
    "    print(f\"- size proj before cleaning: {len(FP7[['project_id', 'stage']].drop_duplicates())}\")\n",
    "    proj = (FP7.assign(stage_name=np.where(FP7.stage=='successful', 'projets lauréats', 'projets évalués'))\n",
    "            [['project_id', 'stage', 'acronym', 'abstract', 'title', 'call_id', 'stage_name',\n",
    "            'call_deadline', 'instrument',  'panel_code', 'panel_name', 'call_year', 'duration', 'status_code', \n",
    "        'cost_total', 'eu_reqrec_grant', 'free_keywords', 'number_involved', 'submission_date',\n",
    "        'start_date', 'signature_date', 'end_date',  'pilier', 'prog_abbr', 'prog_lib', 'area_abbr', 'area_lib']]\n",
    "            .drop_duplicates())\n",
    "\n",
    "    proj.loc[(proj.prog_abbr=='ERC')&(proj.instrument=='POC'), 'instrument'] = 'ERC-POC'\n",
    "    proj.loc[proj.prog_abbr=='PEOPLE', 'thema_code'] = 'MSCA'\n",
    "    proj.loc[proj.prog_abbr=='ERC', 'thema_code'] = 'ERC'\n",
    "\n",
    "    # print(f\"- size proj: {len(proj)}\")\n",
    "\n",
    "    proj = proj.merge(instr, how='left', on='instrument').drop(columns=['name'])\n",
    "    proj.loc[proj.instrument.str.contains('MC-'), 'action_code'] = 'MSCA'        \n",
    "\n",
    "    if any(proj.action_code.isnull()):\n",
    "        print(proj[proj.action_code.isnull()].instrument.unique())   \n",
    "        \n",
    "    print(f\"- size proj after instru: {len(proj)}\")\n",
    "\n",
    "    # ERC\n",
    "    proj = proj.merge(erc_correspondence, how='left', left_on=['instrument'], right_on=['old'])\n",
    "\n",
    "    proj.loc[(proj.thema_code=='ERC')&(proj.destination_code.isnull()), 'destination_code'] = 'ERC-OTHER'\n",
    "\n",
    "    proj.loc[proj.thema_code=='ERC', 'programme_code'] = 'ERC'\n",
    "    proj.loc[proj.thema_code=='ERC', 'programme_name_en'] = 'European Research Council (ERC)'\n",
    "    print(f\"- size proj after ERC: {len(proj)}\")\n",
    "\n",
    "    # MSCA\n",
    "    proj = proj.merge(msca_correspondence, how='left', left_on=['instrument'], right_on=['old'])\n",
    "    proj.loc[proj.call_id.str.contains('NIGHT'), 'destination_detail_code'] = 'CITIZENS'\n",
    "    proj.loc[~proj.destination_detail_code.isnull(), 'destination_code'] = proj.destination_detail_code.str.split('-').str[0]\n",
    "    proj.loc[(proj.destination_code.isnull())&(proj.thema_code=='MSCA'), 'destination_code'] = 'MSCA-OTHER'\n",
    "    proj.loc[proj.thema_code=='MSCA', 'programme_code'] = 'MSCA'\n",
    "    proj.loc[proj.thema_code=='MSCA', 'programme_name_en'] = 'Marie Skłodowska-Curie Actions (MSCA)'\n",
    "\n",
    "    proj.rename(columns={'instrument':'fp_specific_instrument'}, inplace=True)\n",
    "\n",
    "    print(f\"- size proj success after msca: {proj.loc[proj.stage=='successful'].project_id.nunique()}, nb project_id: {len(proj.loc[proj.stage=='successful'])}\")\n",
    "    print(f\"- size proj after msca: {len(proj)}\")\n",
    "    #euratom\n",
    "    proj.loc[(proj.pilier.isin(['EURATOM']))&(proj.prog_abbr=='Fission'), 'programme_code'] = 'NFRP'\n",
    "    proj.loc[(proj.pilier.isin(['EURATOM']))&(proj.programme_code=='NFRP'), 'programme_name_en'] = 'Nuclear fission and radiation protection'\n",
    "    proj.loc[proj.prog_abbr=='Fusion', 'programme_code'] = 'Fusion'\n",
    "    proj.loc[proj.prog_abbr=='Fusion', 'programme_name_en'] = 'Fusion Energy'\n",
    "\n",
    "    euratom = pd.read_csv('data_files/euratom_thema_all_FP.csv', sep=';', na_values='')\n",
    "    proj = proj.merge(euratom[['topic_area', 'thema_code', 'thema_name_en']], how='left', left_on='area_abbr', right_on='topic_area', suffixes=['', '_t'])\n",
    "    proj.loc[(~proj.thema_code_t.isnull()), 'thema_code'] = proj.loc[(~proj.thema_code_t.isnull()), 'thema_code_t']\n",
    "    proj = proj.filter(regex=r'.*(?<!_t)$')\n",
    "    print(f\"- size proj after euratom: {len(proj)}\")\n",
    "\n",
    "    #ju_jti\n",
    "    proj.loc[proj.prog_abbr=='SP1-JTI', 'thema_code'] = 'JU-JTI'\n",
    "    proj.loc[proj.prog_abbr=='SP1-JTI', 'destination_code'] = proj.area_abbr.str.split('-').str[-1]\n",
    "    proj.loc[proj.area_abbr=='JTI-CS', 'destination_code'] = 'CLEAN-AVIATION'\n",
    "\n",
    "    proj.loc[(proj.destination_code=='CLEAN-SKY'), 'destination_code'] = 'CLEAN-AVIATION'\n",
    "    proj.loc[(proj.destination_code=='FCH'), 'destination_code'] = 'CLEANH2'\n",
    "    proj.loc[(proj.destination_code=='IMI'), 'destination_code'] = 'IHI'\n",
    "    proj.loc[(proj.destination_code.isin(['ENIAC','ARTEMIS'])), 'destination_code'] = 'Chips'\n",
    "    proj.loc[proj.thema_code=='JU-JTI', 'action_code'] = proj.fp_specific_instrument.str.split('-').str[1]\n",
    "    print(f\"- size proj after ju-jti: {len(proj)}\")\n",
    "\n",
    "    # WIDENING COST\n",
    "    proj.loc[proj.area_abbr.str.contains('COST', na=False), 'thema_code'] = 'COST'\n",
    "    proj.loc[proj.area_abbr.str.contains('COST', na=False), 'programme_code'] = 'Widening'\n",
    "    proj.loc[proj.area_abbr.str.contains('COST', na=False), 'programme_name_en'] = 'Widening participation and spreading excellence'\n",
    "    print(f\"- size proj after cost: {len(proj)}\")\n",
    "\n",
    "    proj.loc[proj.pilier=='EURATOM', 'pilier_name_en'] = 'Euratom'\n",
    "    proj.loc[(proj.prog_abbr.isin(['PEOPLE','ERC']))|(proj.prog_abbr=='INFRA'), 'pilier_name_en'] = 'Excellent Science'\n",
    "    proj.loc[proj.pilier_name_en.isnull(), 'pilier_name_en'] = proj.pilier.str.capitalize()\n",
    "    print(f\"- size proj after cost: {len(proj)}\")\n",
    "\n",
    "    proj.loc[proj.programme_code.isnull(), 'programme_code'] = proj.prog_abbr\n",
    "    proj.loc[proj.programme_name_en.isnull(), 'programme_name_en'] = proj.prog_lib\n",
    "    print(f\"- size proj after cost: {len(proj)}\")\n",
    "\n",
    "    proj.loc[(~proj.thema_code.isin(['MSCA','ERC']))&(proj.destination_code.isnull()), 'destination_code'] = proj.area_abbr\n",
    "    proj.loc[proj.destination_code.isnull(), 'destination_code'] = proj.thema_code+'-OTHER'\n",
    "    proj = proj.merge(destination[['destination_code', 'destination_name_en']], how='left', on='destination_code')\n",
    "    proj = (proj\n",
    "            .merge(destination.rename(columns={'destination_code':'destination_detail_code', 'destination_name_en':'destination_detail_name_en'})\n",
    "            [['destination_detail_code', 'destination_detail_name_en']], how='left', on='destination_detail_code')\n",
    "            .drop_duplicates())\n",
    "    print(f\"- size proj after cost: {len(proj)}\")\n",
    "\n",
    "    proj.loc[(~proj.thema_code.isin(['MSCA','ERC']))&(proj.destination_name_en.isnull()), 'destination_name_en'] = proj.area_lib\n",
    "    proj.loc[proj.thema_code.isnull(), 'thema_code'] = proj.prog_abbr\n",
    "    proj = proj.merge(thema[['thema_code', 'thema_name_en']], how='left', on='thema_code', suffixes=['', '_t'])\n",
    "    proj.loc[proj.thema_name_en.isnull(), 'thema_name_en'] = proj.thema_name_en_t\n",
    "    proj.loc[proj.thema_name_en.isnull(),'thema_name_en'] = proj.prog_lib\n",
    "    proj = proj.filter(regex=r'.*(?<!_t)$')\n",
    "    print(f\"- size proj after thema_code: {len(proj)}\")\n",
    "\n",
    "    proj = (proj.drop(columns=['area_abbr', 'area_lib'])\n",
    "            .rename(columns={'prog_lib':'fp_specific_programme', 'pilier':'fp_specific_pilier'}))\n",
    "    \n",
    "    print(proj[['programme_code',\n",
    "    'programme_name_en', 'thema_name_en', 'destination_code', 'destination_name_en',\n",
    "    'destination_detail_code','destination_detail_name_en']].drop_duplicates())\n",
    "    print(len(proj))\n",
    "    return proj\n",
    "proj=themes_cleaning(FP7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def proj_cleaning(proj):\n",
    "    print(f\"- size proj before cleaning: {len(proj)}\")\n",
    "    proj = proj.merge(act, how='left', on='action_code')\n",
    "    proj = proj.merge(call, how='left', on='call_id').assign(ecorda_date=pd.to_datetime('2021-04-30'), framework='FP7')\n",
    "    proj = proj.assign(ecorda_date=pd.to_datetime('2021-04-30'), framework='FP7')\n",
    "    for i in ['title', 'abstract', 'free_keywords']:\n",
    "        proj[i]=proj[i].str.replace('\\\\n|\\\\t|\\\\r|\\\\s+', ' ', regex=True).str.strip()\n",
    "\n",
    "    kw = proj[['project_id', 'free_keywords']]\n",
    "    kw = kw.assign(free_keywords = kw.free_keywords.str.split(';|,')).explode('free_keywords')\n",
    "    kw = kw.loc[kw.free_keywords.str.len()>3].drop_duplicates()\n",
    "    kw.free_keywords = kw.free_keywords.groupby(level=0).apply(lambda x: '|'.join(x.str.strip().unique()))\n",
    "\n",
    "    proj = proj.drop(columns='free_keywords').merge(kw.drop_duplicates(), how='left', on='project_id')\n",
    "    proj.mask(proj=='', inplace=True)  \n",
    "\n",
    "    for d in ['call_deadline', 'signature_date',  'start_date',  'end_date', 'submission_date']:\n",
    "        proj[d] = pd.to_datetime(proj[d],format='%d/%m/%Y %H:%M:%S')\n",
    "    print(f\"- size proj cleaned: {len(proj)}\")\n",
    "    return proj\n",
    "proj=proj_cleaning(proj)\n",
    "# def proj_ods(proj, part1):\n",
    "#     country=(part1.loc[part1.stage=='successful',\n",
    "#                 ['project_id','country_code','country_name_fr','country_code_mapping', 'ZONAGE',\n",
    "#                     'country_name_mapping', 'nuts_code', 'region_1_name', 'region_2_name','regional_unit_name']]\n",
    "#         .drop_duplicates()\n",
    "#         .groupby(['project_id'], as_index = False).agg(lambda x: ';'.join(map(str,filter(None, x))))\n",
    "#         .drop_duplicates())\n",
    "\n",
    "#     prop = (proj.loc[proj.stage=='evaluated', ['project_id', 'cost_total', 'eu_reqrec_grant', 'number_involved']]\n",
    "#         .rename(columns={'number_involved':'proposal_numberofapplicants', 'eu_reqrec_grant':'proposal_requestedgrant', 'cost_total':'proposal_budget'})\n",
    "#         .drop_duplicates())\n",
    "\n",
    "#     p = (proj.loc[proj.stage=='successful', ['project_id', 'eu_reqrec_grant', 'number_involved', 'cost_total']]\n",
    "#         .rename(columns={'eu_reqrec_grant':'project_eucontribution', 'number_involved':'project_numberofparticipants','cost_total':'project_totalcost'})\n",
    "#         .drop_duplicates())\n",
    "\n",
    "#     # # PROVISOIRE quand def call refonctionnera\n",
    "#     # proj=proj.assign(call_budget=np.nan)\n",
    "\n",
    "#     project = (proj.loc[proj.stage=='successful', \n",
    "#             ['abstract', 'acronym', 'action_code', 'action_name', 'call_budget','call_deadline', 'call_id', 'call_year',\n",
    "#             'destination_code','destination_detail_code', 'destination_detail_name_en', 'destination_name_en', \n",
    "#             'duration', 'ecorda_date', 'end_date', 'fp_specific_instrument', 'framework', 'free_keywords', \n",
    "#             'panel_code', 'panel_name', 'fp_specific_programme', 'fp_specific_pilier',\n",
    "#             'pilier_name_en', 'programme_code', 'programme_name_en', 'project_id', 'signature_date', 'stage', 'stage_name', \n",
    "#             'start_date', 'status_code', 'submission_date', 'thema_code', 'thema_name_en', 'title']]\n",
    "            \n",
    "#         .drop_duplicates())\n",
    "\n",
    "#     project = project.merge(p, how='left', on='project_id').merge(country, how='inner', on='project_id').merge(prop, how='left' , on='project_id')\n",
    "\n",
    "#     print(f\"1 - size project lauréats: {len(project)}, {len(p)}, fund: {'{:,.1f}'.format(p['project_eucontribution'].sum())}\")\n",
    "\n",
    "#     with open(f\"{PATH_CLEAN}FP7_successful_projects.pkl\", 'wb') as file:\n",
    "#         pd.to_pickle(project, file)\n",
    "#     return project\n",
    "# proj_ods(proj, part1)\n",
    "\n",
    "def FP7_all(proj, part1):\n",
    "    t = (proj.drop(columns=['cost_total', 'duration', 'end_date', 'eu_reqrec_grant', 'fp_specific_instrument', \n",
    "                        'fp_specific_programme', 'fp_specific_pilier',\n",
    "                        'number_involved', 'signature_date', 'start_date', 'submission_date'])\n",
    "        .merge(part1, how='inner', on=['project_id', 'stage'])\n",
    "        .rename(columns={'funding':'calculated_fund', 'ZONAGE':'extra_joint_organization'}))\n",
    "    \n",
    "    t = (t.assign(is_ejo=np.where(t.extra_joint_organization.isnull(), 'Sans', 'Avec')))\n",
    "\n",
    "    t.loc[(t.destination_code.isin(['PF', 'ERARESORG', 'GA']))|((t.thema_code.isin(['ERC', 'COST']))&(t.destination_code!='SyG')), 'coordination_number'] = 0\n",
    "    t=t.assign(with_coord=True)\n",
    "    t.loc[(t.destination_code.isin(['PF', 'ERARESORG', 'GA']))|((t.thema_code.isin(['ERC', 'COST']))&(t.destination_code!='SyG')), 'with_coord'] = False\n",
    "\n",
    "    t.loc[t.thema_code=='ERC', 'erc_role'] = 'partner'\n",
    "\n",
    "    t.loc[(t.destination_code=='SyG'), 'erc_role'] = 'PI'\n",
    "    t.loc[(t.action_code=='ERC')&(t.destination_code!='SyG')&(t.role=='coordinator'), 'erc_role'] = 'PI'\n",
    "    t.loc[(t.destination_code=='ERC-OTHER'), 'erc_role'] = np.nan\n",
    "\n",
    "\n",
    "    file_name = f\"{PATH_CLEAN}FP7_data.pkl\"\n",
    "    with open(file_name, 'wb') as file:\n",
    "        pd.to_pickle(t, file)\n",
    "\n",
    "    print(f\"size proj: {t.loc[t.stage=='successful'].project_id.nunique()}, nb project_id: {len(t.loc[t.stage=='successful'])}, {t.loc[t.stage=='successful', 'calculated_fund'].sum()}\")\n",
    "    return t\n",
    "t=FP7_all(proj, part1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t.loc[(t.country_code=='DEU')&(t.stage=='successful')&(t.call_year=='2007')&(t.pilier_name_en!='Euratom')].calculated_fund.sum()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functions_shared import *\n",
    "t=t.drop_duplicates().loc[(t.stage=='successful')&(t.pilier_name_en!='Euratom')]\n",
    "print(len(t))\n",
    "x=pd.crosstab(t['country_code'], t['call_year'], values=t['calculated_fund'], aggfunc='sum',margins=True, margins_name= 'All').reset_index()\n",
    "work_csv(x, 'fp7_count')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
