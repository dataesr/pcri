{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd, pickle, numpy as np, warnings, time, os\n",
    "warnings.filterwarnings(\"ignore\", \"FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas\")\n",
    "pd.options.mode.copy_on_write = True\n",
    "from config_path import PATH_CLEAN, PATH_API\n",
    "from functions_shared import chunkify, work_csv\n",
    "from step7_persons.prep_persons import persons_preparation\n",
    "from step7_persons.affiliations import affiliations, persons_files_import, persons_api_simplify, persons_results_clean\n",
    "\n",
    "PATH_PERSONS=f\"{PATH_API}persons/\"\n",
    "CSV_DATE='20250121'\n",
    "def persons_preparation(csv_date):\n",
    "\n",
    "    import pandas as pd, numpy as np, warnings\n",
    "    warnings.filterwarnings(\"ignore\", \"This pattern is interpreted as a regular expression, and has match groups\")\n",
    "    pd.options.mode.copy_on_write = True\n",
    "    from constant_vars import FRAMEWORK\n",
    "    from config_path import PATH_SOURCE, PATH_CLEAN\n",
    "    from functions_shared import unzip_zip, my_country_code, country_iso_shift, prop_string\n",
    "\n",
    "    ###############################\n",
    "    participation = pd.read_pickle(f\"{PATH_CLEAN}participation_current.pkl\")\n",
    "    entities = pd.read_pickle(f\"{PATH_CLEAN}entities_info_current2.pkl\")\n",
    "    project = pd.read_pickle(f\"{PATH_CLEAN}projects_current.pkl\")\n",
    "    my_countries=my_country_code()\n",
    "\n",
    "    print(f\"size participation: {len(participation)}\")\n",
    "    ######################\n",
    "    print(f\"\\n### IMPORT datasets\")\n",
    "    perso_part = unzip_zip(f'he_grants_ecorda_pd_{csv_date}.zip', f\"{PATH_SOURCE}{FRAMEWORK}/\", \"participant_persons.csv\", 'utf-8')\n",
    "    perso_part = (perso_part.loc[perso_part.FRAMEWORK=='HORIZON',\n",
    "            ['PROJECT_NBR', 'GENERAL_PIC', 'PARTICIPANT_PIC', 'ROLE', 'FIRST_NAME',\n",
    "            'LAST_NAME', 'TITLE', 'GENDER', 'PHONE', 'EMAIL',\n",
    "            'BIRTH_COUNTRY_CODE', 'NATIONALITY_COUNTRY_CODE', 'HOST_COUNTRY_CODE', 'SENDING_COUNTRY_CODE']]\n",
    "                .rename(columns=str.lower)\n",
    "                .rename(columns={'project_nbr':'project_id', 'general_pic':'generalPic', 'participant_pic':'pic'})\n",
    "                .assign(stage='successful'))\n",
    "    print(f\"size perso_part import: {len(perso_part)}\")\n",
    "\n",
    "    ######################################\n",
    "    perso_app = unzip_zip(f'he_proposals_ecorda_pd_{csv_date}.zip', f\"{PATH_SOURCE}{FRAMEWORK}/\", \"applicant_persons.csv\", 'utf-8')\n",
    "\n",
    "    perso_app = (perso_app.loc[perso_app.FRAMEWORK=='HORIZON',\n",
    "        ['PROPOSAL_NBR', 'GENERAL_PIC', 'APPLICANT_PIC', 'ROLE', 'FIRST_NAME',\n",
    "        'FAMILY_NAME', 'TITLE', 'GENDER', 'PHONE', 'EMAIL',\n",
    "        'RESEARCHER_ID', 'ORCID_ID', 'GOOGLE_SCHOLAR_ID','SCOPUS_AUTHOR_ID']]\n",
    "                .rename(columns=str.lower)\n",
    "                .rename(columns={'proposal_nbr':'project_id', 'general_pic':'generalPic', 'applicant_pic':'pic', 'family_name':'last_name'})\n",
    "                .assign(stage='evaluated'))\n",
    "    print(f\"size perso_app import: {len(perso_app)}\")\n",
    "\n",
    "    ######################################\n",
    "    print(f\"\\n### COUNTRY shift iso2 to iso3\")\n",
    "    for el in ['birth_country_code','nationality_country_code','host_country_code','sending_country_code']:\n",
    "        perso_part = country_iso_shift(perso_part, el, iso2_to3=True)\n",
    "\n",
    "    ####################################\n",
    "    print(f\"\\n### TITLE cleaning\")\n",
    "    def title_clean(df):\n",
    "        df.loc[~df['title'].isnull(), 'title_clean'] = df.loc[~df['title'].isnull(), 'title'].str.replace(r\"[^\\w\\s]+\", \" \", regex=True)\n",
    "        df.loc[~df['title_clean'].isnull(), 'title_clean'] = df.loc[~df['title_clean'].isnull(), 'title_clean'].str.replace(r\"\\s+\", \" \", regex=True).str.strip()\n",
    "        df.mask(df == '', inplace=True)\n",
    "        return df\n",
    "\n",
    "    perso_part = title_clean(perso_part)\n",
    "    perso_app = title_clean(perso_app)\n",
    "\n",
    "    ###############################\n",
    "    print(f\"\\n### STRING cleaning\")\n",
    "    # def prop_string(tab):\n",
    "    #     from unidecode import unidecode\n",
    "    #     cols = ['role', 'first_name', 'last_name','title_clean', 'gender']\n",
    "    #     tab[cols] = tab[cols].map(lambda s:s.casefold() if type(s) == str else s)\n",
    "                \n",
    "    #     for i in cols:\n",
    "    #         tab.loc[~tab[i].isnull(), i] = tab.loc[~tab[i].isnull(), i].str.replace(r\"[^\\w\\s]+\", \" \", regex=True)\n",
    "    #         tab.loc[~tab[i].isnull(), i] = tab.loc[~tab[i].isnull(), i].apply(unidecode)\n",
    "    #     return tab\n",
    "\n",
    "    cols = ['role', 'first_name', 'last_name','title_clean', 'gender']\n",
    "    perso_part = prop_string(perso_part, cols)\n",
    "    perso_app = prop_string(perso_app, cols)\n",
    "\n",
    "    ##########\n",
    "    print(f\"\\n### CONTACT create\")\n",
    "    def contact_name(df):\n",
    "        for f in ['first_name', 'last_name']:\n",
    "            df[f] = df[f].fillna('')\n",
    "            df[f] = df[f].str.strip().str.replace(r\"\\s+\", '-', regex=True)\n",
    "            df[f] = df[f].str.strip().str.replace(r\"-{2,}\", '-', regex=True)\n",
    "\n",
    "        df['contact'] = df.first_name.astype(str).str.lower() + ' ' + df.last_name.astype(str).str.lower()\n",
    "        \n",
    "        str_remove=['not applicable']\n",
    "        df['contact'] = df['contact'].str.strip().str.replace(r\"\\^s+$\", '-', regex=True)\n",
    "        df = df.loc[~df.contact.isin(str_remove)]\n",
    "        return df\n",
    "\n",
    "    perso_app = contact_name(perso_app)\n",
    "    perso_part = contact_name(perso_part)\n",
    "\n",
    "\n",
    "    # ###########\n",
    "    print(f\"\\n### PIC empty fix\")\n",
    "    # generalPic empty ; replace by pic or fill by generalPic participation\n",
    "    def empty_pic(df, participation, stage):\n",
    "        if any(df.generalPic.isnull()):\n",
    "            print(f\"1 - size rows with generelPic null for {stage}: {len(df[df.generalPic.isnull()])}\")\n",
    "            df.loc[df.generalPic.isnull(), 'generalPic'] = df.loc[df.generalPic.isnull(), 'pic']\n",
    "\n",
    "            # gestion empty generalPic for principal investigator\n",
    "            x=df.loc[(df.generalPic.isnull())&(df.role=='principal investigator')].project_id.unique()\n",
    "            if x.size>0:\n",
    "                y=participation.loc[(participation.project_id.isin(x))&(participation.stage==stage), ['project_id', 'generalPic']]\n",
    "                df=df.merge(y, how='left', on=['project_id'], suffixes=('', '_y'))\n",
    "                df.loc[(df.generalPic.isnull())&(~df.generalPic_y.isnull()), 'generalPic'] = df.loc[(df.generalPic.isnull())&(~df.generalPic_y.isnull()), 'generalPic_y'] \n",
    "                df.drop(columns='generalPic_y', inplace=True)\n",
    "                print(f\"2 - size rows with generelPic null for {stage}: {len(df[df.generalPic.isnull()])}\")\n",
    "        print(f\"size df_{stage} after empty_pic: {len(df)}\")\n",
    "        return df\n",
    "\n",
    "    perso_part = empty_pic(perso_part, participation, 'successful')\n",
    "    perso_app = empty_pic(perso_app, participation, 'evaluated')\n",
    "\n",
    "    ################\n",
    "    print(f\"\\n### CALCULATION measures\")\n",
    "    def perso_measure(df):\n",
    "        df['nb_pic_unique']=df.groupby(['project_id'])['generalPic'].transform('nunique') #combien de pics / projet\n",
    "        df['nb_name_unique']=df.groupby(['project_id'])['last_name'].transform('nunique') #combien de pics / projet\n",
    "        df['nb_row_by_pic']=df.groupby(['project_id', 'generalPic'])['last_name'].transform('count') #combien de lignes par pic\n",
    "        df['nb_name_unique_by_pic']=df.groupby(['project_id', 'generalPic'])['last_name'].transform('nunique')\n",
    "        df['nb_row_by_pic_name'] = df.groupby(['project_id', 'generalPic','last_name'])['last_name'].transform('count')\n",
    "        df['nb_row_by_pic_name_unique'] = df.groupby(['project_id', 'generalPic','last_name'])['last_name'].transform('nunique')\n",
    "        df['nb_pic_by_contact_unique'] = df.groupby(['project_id','contact'])['generalPic'].transform('count')\n",
    "        \n",
    "        # print(f\"size df: {len(df)}\\ncolumns:{df.columns}\")\n",
    "        print(f\"size df: {len(df)}\")\n",
    "        return df\n",
    "\n",
    "    perso_part = perso_measure(perso_part)\n",
    "    perso_app = perso_measure(perso_app)\n",
    "\n",
    "    ################\n",
    "    print(f\"\\n### without PIC remove\")\n",
    "    def generaPic_remove(df):\n",
    "        return df.loc[~((df.nb_pic_unique>0)&(df.generalPic.isnull()))]\n",
    "\n",
    "    perso_part = generaPic_remove(perso_part)\n",
    "    perso_app = generaPic_remove(perso_app)\n",
    "\n",
    "    ##############################\n",
    "    print(f\"\\n### NAME duplicated remove\")\n",
    "    def name_duplicated_remove(df):\n",
    "        #### cleaning name duplicated by project \n",
    "        ## if by project single name but several rows\n",
    "        # x[x.project_id=='101039481']\n",
    "\n",
    "        print(df.role.unique())\n",
    "        keep_order=['principal investigator', 'fellow', 'main_contact']\n",
    "        if len(df.role.unique()) > len(keep_order):\n",
    "            print(f\"2 - Attention ! un role nouveau dans perso -> {set(df.role.unique())-set(keep_order)}\")\n",
    "\n",
    "        tmp=pd.DataFrame()\n",
    "        mask=[(df.nb_row_by_pic_name_unique==1)&(df.nb_row_by_pic_name>1)]\n",
    "        for i in mask:\n",
    "            x=df.loc[i]\n",
    "            print(f\"3 - size x before remove: {len(x)}\")\n",
    "            x=x.groupby(['project_id','generalPic', 'last_name']).apply(lambda i: i.sort_values('role', key=lambda col: pd.Categorical(col, categories=keep_order, ordered=True)), include_groups=True).reset_index(drop=True)\n",
    "            for v in ['title', 'gender','phone','email','birth_country_code','nationality_country_code','host_country_code','sending_country_code']:\n",
    "                if v in x.columns:\n",
    "                    x[v]=x.groupby(['project_id', 'generalPic', 'last_name'])[v].bfill()\n",
    "            x=x.groupby(['project_id', 'generalPic', 'last_name']).head(1)\n",
    "            print(f\"3 - size x after remove: {len(x)}\")\n",
    "\n",
    "            tmp=pd.concat([tmp, x], ignore_index=True)\n",
    "\n",
    "        df=df.merge(tmp[['project_id', 'generalPic', 'last_name']].drop_duplicates(), how='outer', on=['project_id', 'generalPic', 'last_name'], indicator=True).query('_merge==\"left_only\"')\n",
    "        df=pd.concat([df, tmp], ignore_index=True)\n",
    "\n",
    "        if len(df)==0:\n",
    "            print(f\"ATTENTION table vide après traitement name_duplicated_remove\")\n",
    "        else:\n",
    "            print(f\"size après traitement name_duplicated_remove: {len(df)}\")\n",
    "\n",
    "        return df.drop(columns=['_merge'])\n",
    "\n",
    "    perso_part = name_duplicated_remove(perso_part)\n",
    "    perso_app = name_duplicated_remove(perso_app)\n",
    "    # ####################################\n",
    "    perso_part = perso_measure(perso_part)\n",
    "    perso_app = perso_measure(perso_app)\n",
    "\n",
    "    print(f\"\\n### PI duplicated\")\n",
    "    def PI_duplicated(df):\n",
    "        if any(df.role=='principal investigator'):\n",
    "            # select if same person and one PI in a single project \n",
    "            mask=(df.nb_pic_by_contact_unique>1)&(df.role=='principal investigator')\n",
    "            pi=df.loc[mask, ['project_id', 'contact']].drop_duplicates().merge(df, how='inner')\n",
    "            pi['role'] = 'principal investigator'\n",
    "            for v in ['title', 'gender','birth_country_code','nationality_country_code','sending_country_code']:\n",
    "                if v in df.columns:\n",
    "                    pi=pi.sort_values(v)\n",
    "                    pi[v]=pi.groupby(['project_id', 'contact'])[v].ffill()\n",
    "            \n",
    "            df=df.merge(pi[['project_id', 'generalPic', 'contact']].drop_duplicates(), how='outer', on=['project_id', 'generalPic', 'contact'], indicator=True).query('_merge==\"left_only\"')\n",
    "            df=pd.concat([df, pi], ignore_index=True)\n",
    "            print(f\"-size df after cleaning pi_duplicated: {len(df)}\")\n",
    "            return df.drop(columns=['_merge'])\n",
    "        \n",
    "    perso_part=PI_duplicated(perso_part)\n",
    "\n",
    "    #######################\n",
    "    print(f\"\\n### PARTICIPATION+PERSO\")\n",
    "    def perso_participation(df, participation, project, entities, stage):\n",
    "        \n",
    "        df=df.loc[df.project_id.isin(participation[participation.stage==stage].project_id.unique())]\n",
    "        df=df.merge(participation.loc[participation.stage==stage, ['project_id', 'generalPic', 'country_code']], how='outer', on=['project_id', 'generalPic'], indicator=True).query('_merge!=\"right_only\"')\n",
    "        df.loc[df._merge=='left_only', 'shift'] = 'past'\n",
    "\n",
    "        if stage=='successful':\n",
    "            df.loc[(df._merge=='both')&(df.host_country_code.isnull()), 'host_country_code'] = df.loc[(df._merge=='both')&(df.host_country_code.isnull()), 'country_code']\n",
    "\n",
    "        df=df.merge(project.loc[project.stage==stage, ['project_id', 'call_year', 'thema_code', 'destination_code']], how='inner', on=['project_id'])\n",
    "\n",
    "        x=entities[['entities_id', 'entities_name', 'generalPic', 'id_secondaire', 'country_code', 'country_code_mapping']].drop_duplicates()\n",
    "        df=df.merge(x, how='left', on=['generalPic', 'country_code'])\n",
    "\n",
    "        if len(df)==0:\n",
    "            print(f\"ATTENTION table vide après lien avec participation\")\n",
    "        else:\n",
    "            print(f\"size app lien avec participation clean : {len(df)}\\ncolumns:{df.columns}\")\n",
    "        return df.drop(columns=['_merge'])\n",
    "\n",
    "    perso_part = perso_participation(perso_part, participation, project, entities, 'successful')\n",
    "    perso_app = perso_participation(perso_app, participation, project, entities, 'evaluated')\n",
    "\n",
    "    def iso2_add(df):\n",
    "        df = (df.merge(my_countries[['iso2', 'iso3']].drop_duplicates(), how='left', left_on='country_code', right_on='iso3')\n",
    "                .drop(columns='iso3')\n",
    "        )\n",
    "        if any(df.iso2.isnull()):\n",
    "            print(f\"country iso2 missing for iso3 -> {df[df.iso2.isnull()].country_code.unique()}\")\n",
    "        return df\n",
    "    perso_part = iso2_add(perso_part)\n",
    "    perso_app = iso2_add(perso_app)\n",
    "\n",
    "    # ##################\n",
    "    print(f\"\\n### PHONE cleaning\")\n",
    "    def phone_clean(df):\n",
    "        y = df.loc[(df.country_code=='FRA')&(~df.phone.isnull()), ['phone']]\n",
    "        y['tel_clean']=y.phone.str.replace(r\"(^\\++[0-9]{1,3}\\s+)\", '', regex=True)\n",
    "        y['tel_clean']=y.tel_clean.str.replace(r\"[^0-9]+\", '', regex=True)\n",
    "        y['tel_clean']=y.tel_clean.str.replace(r\"^(33|033|0033)\", '', regex=True).str.rjust(10, '0')\n",
    "        y.loc[(y.tel_clean.str.len()>10)&(y.tel_clean.str[0:1]=='0'), 'tel_clean'] = y.tel_clean.str[0:10]\n",
    "        y['tel_clean']=y.tel_clean.str.replace(r\"^0+$\", '', regex=True)\n",
    "        # work_csv(y, 'tel_perso')\n",
    "        return pd.concat([df, y[['tel_clean']]], axis=1)\n",
    "\n",
    "    perso_part = phone_clean(perso_part)\n",
    "    perso_app = phone_clean(perso_app)\n",
    "\n",
    "    # #######################\n",
    "    print(f\"\\n### MAIL cleaning\")\n",
    "    def mail_clean(df):\n",
    "        mail_del=[\"gmail\", \"yahoo\", \"hotmail\", \"wanadoo\", \"aol\", \"free\", \"skynet\", \"outlook\", \"icloud\", \"googlemail\"]\n",
    "\n",
    "        df['domaine'] = df.email.str.split('@').str[1].str.split('.').str[:-1].fillna('').apply(' '.join)\n",
    "        tmp = df.loc[~df.domaine.isnull(), ['domaine']]\n",
    "\n",
    "        for el in mail_del:\n",
    "            m = r\"^\"+el+r\"($|\\s)\"\n",
    "            tmp.loc[tmp['domaine'].str.contains(m, case=True, flags=0, na=None, regex=True) == True, 'domaine_email'] = ''\n",
    "            tmp.loc[tmp['domaine_email'].isnull(), 'domaine_email'] = tmp['domaine']\n",
    "\n",
    "        return pd.concat([df, tmp], axis=1).drop(columns='domaine')\n",
    "\n",
    "    perso_app = mail_clean(perso_app)\n",
    "    perso_part = mail_clean(perso_part)\n",
    "    ##############\n",
    "\n",
    "    def nationality_clean(df):\n",
    "        filter_df=df.loc[(df.nationality_country_code.isnull()), ['generalPic', 'contact']].drop_duplicates().assign(fill_nat=True)\n",
    "        df=df.merge(filter_df, how='left', on=['generalPic', 'contact'])\n",
    "        df['rows_by_picContact']=df.groupby(['generalPic', 'contact'], dropna=False)['nationality_country_code'].transform('nunique')\n",
    "        df.loc[(df.fill_nat==True)&(df.rows_by_picContact==1), 'nationality_country_code']=df.loc[(df.fill_nat==True)&(df.rows_by_picContact==1)].sort_values(['generalPic', 'contact', 'nationality_country_code']).groupby(['generalPic', 'contact'], group_keys=True)['nationality_country_code'].ffill()\n",
    "        df.drop(columns='rows_by_picContact', inplace=True)\n",
    "        return df\n",
    "    \n",
    "    perso_part = nationality_clean(perso_part)\n",
    "    #################\n",
    "\n",
    "    # def orcid_id_fill(df):\n",
    "    #     print(\"### orcid fillna\")\n",
    "    #     temp=df.groupby(['generalPic', 'contact', 'domaine_email'], dropna=False)['orcid_id'].nunique(dropna=False).reset_index()\n",
    "    #     print(temp[temp.orcid_id>2])\n",
    "    #     temp=temp[temp.orcid_id>1].drop(columns='orcid_id')\n",
    "    #     df=df.merge(temp, how='left', on=['generalPic', 'contact', 'domaine_email'], indicator=True)\n",
    "    #     df.loc[df._merge=='both', 'orcid_id'] = df.loc[df._merge=='both'].sort_values(['generalPic', 'contact', 'domaine_email', 'orcid_id']).groupby(['generalPic', 'contact', 'domaine_email'], group_keys=True)['orcid_id'].ffill()\n",
    "    #     return df.drop(columns='_merge')\n",
    "    \n",
    "    # perso_app = orcid_id_fill(perso_app)\n",
    "    # #################\n",
    "\n",
    "    # # add orcid_id (perso_app) into perso_part\n",
    "    # print(f\"\\n### INFO missing between datasets\")\n",
    "    # perso_part=perso_part.merge(perso_app[['project_id', 'contact', 'orcid_id']], how='left', on=['project_id', 'contact']) \n",
    "    # perso_app=perso_app.merge(perso_part[['project_id', 'contact', 'nationality_country_code']], how='left', on=['project_id', 'contact'])\n",
    "    # ##################\n",
    "    \n",
    "    # # fill missing value with other df part/app\n",
    "    # print(f\"\\n### GENDER/TITLE missing\")\n",
    "    # def gender_title_missing(part, app):\n",
    "    #     tab=(part[['project_id', 'contact', 'gender','title_clean']]\n",
    "    #     .merge(app[['project_id', 'contact', 'gender', 'title_clean']], \n",
    "    #             how='inner', on=['project_id', 'contact'], suffixes=('_x','_y'))\n",
    "    #             .drop_duplicates())\n",
    "    #     cl=['gender', 'title_clean']\n",
    "    #     for i in cl:\n",
    "    #         if any(tab.loc[(tab[f\"{i}_x\"].isnull())&(~tab[f\"{i}_y\"].isnull())]):\n",
    "    #             tab.loc[(tab[f\"{i}_x\"].isnull())&(~tab[f\"{i}_y\"].isnull()), f\"{i}_x\"] = tab[f\"{i}_y\"]\n",
    "    #         if any(tab.loc[(~tab[f\"{i}_x\"].isnull())&(tab[f\"{i}_y\"].isnull())]):\n",
    "    #             tab.loc[(~tab[f\"{i}_x\"].isnull())&(tab[f\"{i}_y\"].isnull()), f\"{i}_y\"] = tab[f\"{i}_x\"]\n",
    "\n",
    "    #         part = part.merge(tab[['project_id', 'contact', f\"{i}_x\"]].drop_duplicates(), how='left', on=['project_id', 'contact'])\n",
    "    #         part.loc[part[i].isnull(), i] = part.loc[part[i].isnull(), f\"{i}_x\"]\n",
    "    #         part.drop(columns=f\"{i}_x\", inplace=True)\n",
    "    #         app = app.merge(tab[['project_id', 'contact', f\"{i}_y\"]].drop_duplicates(), how='left', on=['project_id', 'contact'])\n",
    "    #         app.loc[app[i].isnull(), i] = app.loc[app[i].isnull(), f\"{i}_y\"]\n",
    "    #         app.drop(columns=f\"{i}_y\", inplace=True)\n",
    "\n",
    "    #     return part, app\n",
    "\n",
    "    # perso_part, perso_app = gender_title_missing(perso_part, perso_app)\n",
    "\n",
    "    # print(f\"\\n### EXPORT final datasets\")\n",
    "    # (perso_part[['project_id', 'generalPic', 'role', 'first_name', 'last_name',\n",
    "    #     'title_clean', 'gender', 'email', 'tel_clean', 'domaine_email', 'orcid_id', 'birth_country_code',\n",
    "    #     'nationality_country_code', 'host_country_code', 'sending_country_code', 'iso2',\n",
    "    #     'stage', 'contact', 'country_code', 'shift', 'call_year', 'thema_code', 'destination_code',\n",
    "    #     'entities_id', 'entities_name','id_secondaire', 'country_code_mapping']]\n",
    "    #     .drop_duplicates()\n",
    "    #     .to_pickle(f\"{PATH_CLEAN}persons_participants.pkl\"))\n",
    "\n",
    "    # (perso_app[['project_id', 'generalPic', 'role', 'first_name', 'last_name', 'nationality_country_code',\n",
    "    #     'title_clean', 'gender', 'tel_clean', 'email', 'domaine_email', 'researcher_id', 'orcid_id',\n",
    "    #     'google_scholar_id', 'scopus_author_id', 'stage', 'iso2',\n",
    "    #     'contact', 'country_code', 'shift', 'call_year', 'thema_code', 'destination_code',\n",
    "    #     'entities_id', 'entities_name','id_secondaire', 'country_code_mapping']]\n",
    "    #     .drop_duplicates()\n",
    "    #     .to_pickle(f\"{PATH_CLEAN}persons_applicants.pkl\"))\n",
    "    return perso_app\n",
    "# df=persons_preparation(CSV_DATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_PERSONS=f\"{PATH_API}persons/\"\n",
    "# result=pd.read_pickle(f\"{PATH_PERSONS}persons_{CSV_DATE}.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result[result.display_name=='anwesha sarkar']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp=df.groupby(['generalPic', 'contact'], dropna=False)['orcid_id'].nunique(dropna=False).reset_index()\n",
    "print(temp[temp.orcid_id>2])\n",
    "# temp=temp[temp.orcid_id>1].drop(columns='orcid_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[(df.contact.str.contains('feng'))&(df.entities_name.str.contains('Leeds'))][['project_id', 'generalPic','role', 'orcid_id', 'stage', 'contact', 'country_code',\n",
    "       'shift', 'call_year', 'thema_code', 'destination_code', 'entities_id',\n",
    "       'entities_name', 'id_secondaire', 'country_code_mapping', 'iso2',\n",
    "       'tel_clean', 'domaine_email']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lvar=['project_id', 'generalPic', 'role', \n",
    "       'title_clean', 'gender', 'email', 'tel_clean', 'domaine_email',\n",
    "       'orcid_id', 'birth_country_code', 'nationality_country_code',\n",
    "       'host_country_code', 'sending_country_code', 'iso2', 'stage', 'contact',\n",
    "       'country_code', 'shift', 'call_year', 'thema_code', 'destination_code',\n",
    "       'entities_id', 'entities_name', 'id_secondaire', 'country_code_mapping']\n",
    "pp = pd.concat([perso_part.drop_duplicates(), perso_app.drop_duplicates()], ignore_index=True)\n",
    "\n",
    "mask=((pp.country_code=='FRA')|(pp.nationality_country_code=='FRA')|(pp.destination_code.isin(['COG', 'PF', 'STG', 'ADG', 'POC','SyG', 'PERA', 'SJI'])))&(~(pp.contact.isnull()&pp.orcid_id.isnull()))\n",
    "pp=pp.loc[mask, lvar].sort_values(['country_code','orcid_id'], ascending=False).drop_duplicates()\n",
    "pp['contact2']=pp.contact.str.replace('-', ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def persons_files_import(thema, PATH_PERSONS):\n",
    "    import re, os\n",
    "    fname=''.join([filename for filename in os.listdir(PATH_PERSONS) if thema in filename])\n",
    "    print(fname)\n",
    "\n",
    "    if fname:\n",
    "        with open(f\"{PATH_PERSONS}{fname}\", 'rb') as f:\n",
    "            return pickle.load(f)\n",
    "\n",
    "    if fname == []:\n",
    "        fmax=max(int(os.path.splitext(filename)[0].split('_')[-1]) for filename in os.listdir(PATH_PERSONS) if re.search(r\"persons_authors_[0-9]+\",filename))\n",
    "        if fmax:\n",
    "            with open(f\"{PATH_PERSONS}persons_authors_{fmax}.pkl\", 'rb') as f:\n",
    "                return pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'{PATH_PERSONS}persons_authors_erc_{CSV_DATE}.pkl', 'rb') as f:\n",
    "    pers_ers = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'{PATH_PERSONS}persons_authors_{CSV_DATE}.pkl', 'rb') as f:\n",
    "    pers_api = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def persons_api_simplify(df):\n",
    "    pers = [] \n",
    "    for p in df:\n",
    "        # elem = {k: v for k, v in p.items() if (v and v != \"NaT\")}\n",
    "\n",
    "        p['institutions'] = []\n",
    "        if p.get(\"affiliations\"):\n",
    "            for aff in p[\"affiliations\"]:  \n",
    "                res={\"institution_name\":aff.get('institution').get(\"display_name\"),\n",
    "                \"institution_ror\":aff.get('institution').get(\"ror\"),\n",
    "                \"institution_country2\":aff.get('institution').get(\"country_code\"),\n",
    "                \"years\":aff.get(\"years\")}\n",
    "                p['institutions'].append(res)\n",
    "    \n",
    "        p[\"orcid_openalex\"] = p[\"ids\"].get(\"orcid\")            \n",
    "\n",
    "        delete=['display_name_alternatives', 'topics', 'affiliations', 'id', 'last_known_institutions', 'ids']\n",
    "        for field in delete:\n",
    "            if p.get(field):\n",
    "                p.pop(field)\n",
    "\n",
    "        # elem = {k: v for k, v in elem.items() if (v and v != \"NaT\")}\n",
    "        pers.append(p)\n",
    "\n",
    "    print(len(pers))\n",
    "    return pers\n",
    "\n",
    "pers = persons_api_simplify(pers_api)\n",
    "perc = persons_api_simplify(pers_ers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def persons_results_clean(df):\n",
    "    from unidecode import unidecode\n",
    "    from functions_shared import my_country_code, prop_string\n",
    "\n",
    "    # df=pd.json_normalize(df, max_level=1)\n",
    "    df=pd.json_normalize(df, record_path=['institutions'], meta=['match', 'orcid', 'display_name', 'orcid_openalex'], errors='ignore')\n",
    "    df=df[~df.astype(str).duplicated()]\n",
    "    cols = ['display_name']\n",
    "    df = prop_string(df, cols)\n",
    "\n",
    "    df['rows_by_name_orcid'] = df.groupby(['display_name', 'orcid'], dropna=False).transform('size')\n",
    "\n",
    "    # persName_withOrcid_noAff=df[(df.match=='full_name')&(~df.orcid.isnull())&(df.institution_name.isnull())]\n",
    "    # print(f\"size person detect by name with an orcid but no affiliations: {len(persName_withOrcid_noAff)}\")\n",
    "\n",
    "\n",
    "    for i in ['orcid_openalex', 'orcid', 'institution_ror']:\n",
    "        df.loc[~df[i].isnull(), i] = df.loc[~df[i].isnull()][i].str.split(\"/\").str[-1]\n",
    "    df['institution_ror'] = 'R'+ df['institution_ror'].astype(str)\n",
    "\n",
    "    df['years']=df['years'].map(lambda liste: ';'.join(str(x) for x in liste))\n",
    "    df=df[['match', 'display_name', 'orcid_openalex', 'years', 'institution_ror', 'institution_name', 'institution_country2', 'rows_by_name_orcid']]\n",
    "    my_countries=my_country_code()\n",
    "    df=(df.merge(my_countries[['iso2', 'iso3', 'parent_iso3']].drop_duplicates(), \n",
    "                 how='left', left_on='institution_country2', right_on='iso2')\n",
    "        .drop(columns=['iso2'])\n",
    "        .rename(columns={'iso3':'institution_country_map', 'parent_iso3':'institution_country'})\n",
    "        )\n",
    "\n",
    "    from step8_referentiels.paysage import paysage_prep\n",
    "    from config_path import PATH\n",
    "    DUMP_PATH=f'{PATH}referentiel/'\n",
    "    paysage = paysage_prep(DUMP_PATH)\n",
    "    df=(df.merge(paysage[['nom_long', 'numero_ror', 'numero_paysage', 'country_code_map', 'num_nat_struct']].drop_duplicates(), \n",
    "                 how='left', left_on='institution_ror', right_on='numero_ror'))\n",
    "\n",
    "\n",
    "\n",
    "#     print(f\"-3 size {match} cleaned: {len(df)}\")\n",
    "    return df\n",
    "\n",
    "perc=persons_results_clean(perc)\n",
    "pers=persons_results_clean(pers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp=pd.concat([perc, pers], ignore_index=True)\n",
    "temp=temp[~temp.astype(str).duplicated()]\n",
    "pd.to_pickle(temp,f\"{PATH_PERSONS}persons_{CSV_DATE}.pkl\")\n",
    "len(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pers.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# temp=temp[~temp.astype(str).duplicated()]\n",
    "pd.to_pickle(temp,f\"{PATH_PERSONS}persons_{CSV_DATE}.pkl\")\n",
    "len(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# oth=pp.loc[~pp.thema_code.isin(['ERC', 'MSCA'])].drop_duplicates().reset_index(drop=True).sort_values('contact2')\n",
    "# # pers_oth[pers_oth.display_name.str.contains('aubert')].display_name.unique()\n",
    "# df=oth.merge(temp, how='inner', left_on=['contact2', 'country_code'], right_on=['display_name','iso3'])\n",
    "# df=df[~df.astype(str).duplicated()]\n",
    "# # df['years']=df['years'].map(lambda liste: ';'.join(str(x) for x in liste))\n",
    "# df['filt']=df.apply(lambda x: x['call_year'] in x['years'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pers_oth[pers_oth.display_name.str.contains('aubert')].display_name.unique()\n",
    "df=oth.merge(temp, how='inner', left_on=['contact2', 'country_code'], right_on=['display_name','iso3'])\n",
    "df=df[~df.astype(str).duplicated()]\n",
    "# df['years']=df['years'].map(lambda liste: ';'.join(str(x) for x in liste))\n",
    "df['filt']=df.apply(lambda x: x['call_year'] in x['years'], axis=1)\n",
    "\n",
    "df=df[['project_id', 'generalPic', 'role', 'orcid_id', 'orcid_openalex',\n",
    "       'nationality_country_code', 'contact2',\n",
    "        'stage', 'contact', 'display_name','country_code', \n",
    "        'entities_id', 'entities_name',\n",
    "       'id_secondaire', 'country_code_mapping', \n",
    "        'institution_ror', 'institution_name',\n",
    "       'institution_country',  'iso3', 'parent_iso3',\n",
    "       'nom_long', 'numero_ror', 'numero_paysage', 'country_code_map',\n",
    "       'num_nat_struct','rows_by_name_orcid','call_year','years', 'filt','thema_code', 'destination_code']]\n",
    "# \n",
    "\n",
    "#extract entities_id begun by PIC\n",
    "work_csv(df.loc[df.entities_id.str.startswith('pic', na=False), \n",
    "                ['generalPic', 'entities_name','id_secondaire', 'institution_ror', 'institution_name',\n",
    "                'institution_country','numero_ror', 'numero_paysage', 'num_nat_struct']].drop_duplicates(), 'verif_id_struct_from openalex')\n",
    "\n",
    "# #keep only rows with th same year\n",
    "df1=df.loc[(df.filt==True)]\n",
    "\n",
    "\n",
    "# df=df.loc[(df.filt==True)&(~df.num_nat_struct.isnull())]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "work_csv(df.loc[(df.filt==True)&(~df.num_nat_struct.isnull())], 'pers_with_nns')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp = pd.concat([perso_part.drop_duplicates(), perso_app.drop_duplicates()], ignore_index=True)\n",
    "# mask=((pp.country_code=='FRA')|(pp.nationality_country_code=='FRA')|(pp.destination_code.isin(['COG', 'PF', 'STG', 'ADG', 'POC','SyG', 'PERA', 'SJI'])))&(~(pp.contact.isnull()&pp.orcid_id.isnull()))\n",
    "pp=pp.sort_values(['country_code','orcid_id'], ascending=False).drop_duplicates()\n",
    "pp['contact2']=pp.contact.str.replace('-', ' ')\n",
    "len(pp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp.loc[pp.contact=='zoubir khatir', ['stage','project_id', 'generalPic', 'role', \n",
    "       'title_clean', 'gender', 'email', 'tel_clean',\n",
    "       'orcid_id', 'nationality_country_code',\n",
    "       'host_country_code', 'contact',\n",
    "       'country_code', 'call_year', 'thema_code', 'destination_code',\n",
    "       'entities_id', 'entities_name', 'id_secondaire', 'country_code_mapping'\n",
    "       ]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp.loc[temp.display_name=='zoubir khatir']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pp.merge(temp, how='inner', left_on=['contact2', 'country_code'], right_on=['display_name','iso3'])\n",
    "df['filt_year']=df.apply(lambda x: x['call_year'] in x['years'], axis=1)\n",
    "# same year\n",
    "df=df.loc[(df.filt_year==True)]\n",
    "#same entities\n",
    "df1=df.loc[(df.entities_id==df.numero_paysage)]\n",
    "df1=df1.assign(orcid_clean=np.where(df1.orcid_openalex.isnull(), df1.orcid_id, df1.orcid_openalex)).drop(columns='orcid_openalex').drop_duplicates()\n",
    "# df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "work_csv(df1, 'verif_orcid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import time, pickle, requests\n",
    "# from step7_persons.affiliations import openalex_name, openalex_orcid\n",
    "# from config_path import PATH_CLEAN, PATH_API, PATH_WORK\n",
    "# print(time.strftime(\"%H:%M:%S\"))\n",
    "# rlist=[]\n",
    "\n",
    "# try:\n",
    "\n",
    "\n",
    "#     author = {\n",
    "#     \"name\": \"caye pierre'\",\n",
    "#     \"orcid\": \"\"\n",
    "#     }\n",
    "\n",
    "#     if author.get(\"orcid\"):\n",
    "#         result = openalex_orcid(author)\n",
    "#         if result.get('match'):\n",
    "#             rlist.append(result)\n",
    "#         else:\n",
    "#             result = openalex_name(author)\n",
    "#             if result:\n",
    "#                 rlist.extend(result)\n",
    "#     if author.get(\"orcid\")=='':\n",
    "#         result = openalex_name(author)\n",
    "#         if result:\n",
    "#             rlist.extend(result)\n",
    "\n",
    "#     nf=f\"persons_author\"\n",
    "#     with open(f'{PATH_WORK}test.pkl', 'wb') as f:\n",
    "#         pickle.dump(rlist, f)\n",
    "#     print(time.strftime(\"%H:%M:%S\"))\n",
    "\n",
    "# except requests.exceptions.HTTPError as http_err:\n",
    "#     print(f\"\\n{time.strftime(\"%H:%M:%S\")}, {author}-> HTTP error occurred: {http_err}\")\n",
    "#     nf=f\"persons_author\"\n",
    "#     with open(f'{PATH_WORK}test.pkl', 'wb') as f:\n",
    "#         pickle.dump(rlist, f)\n",
    "#     print(time.strftime(\"%H:%M:%S\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open(f'{PATH_WORK}test.pkl', 'rb') as f:\n",
    "    test = pickle.load(f)\n",
    "test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## return results from apenalex\n",
    "\n",
    "1 - à partir d'orcid\n",
    "2 - à partir du nom car orcid pas dans openalex -> vérifier que même personne ?\n",
    "3 - à partir du nom car orcid non renseigné\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pers_api.mask(pers_api=='', inplace=True)\n",
    "tmp1=pers_api.merge(my_countries, how='left', left_on='country_code', right_on='iso2')\n",
    "# pers_api=country_clean(pers_api, ['country_code'])\n",
    "tmp1=pers_api.loc[pers_api.match=='orcid']\n",
    "len(tmp1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{len(perso_part)} ; {perso_part.columns}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_part=perso_part.merge(tmp1, how='inner', left_on=['orcid_id'], right_on=['orcid'], suffixes=('','_api'))\n",
    "tmp_part.loc[tmp_part.country_code!=tmp_part.country_code_api]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [\n",
    "    {\n",
    "        \"state\": \"Florida\",\n",
    "        \"shortname\": \"FL\",\n",
    "        \"info\": {\"governor\": \"Rick Scott\"},\n",
    "        \"counties\": [\n",
    "            {\"name\": \"Dade\", \"population\": 12345},\n",
    "            {\"name\": \"Broward\", \"population\": 40000},\n",
    "            {\"name\": \"Palm Beach\", \"population\": 60000},\n",
    "        ],\n",
    "    },\n",
    "    {\n",
    "        \"state\": \"Ohio\",\n",
    "        \"shortname\": \"OH\",\n",
    "        \"info\": {\"governor\": \"John Kasich\"},\n",
    "        \"counties\": [\n",
    "            {\"name\": \"Summit\", \"population\": 1234},\n",
    "            {\"name\": \"Cuyahoga\", \"population\": 1337},\n",
    "        ],\n",
    "    },\n",
    "]\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def openalex_name(author):\n",
    "    url = f\"https://api.openalex.org/authors?filter=display_name.search:{author.get('name')}\"\n",
    "    nb_openalex=requests.get(url).json().get(\"meta\").get('count')\n",
    "    result={}\n",
    "    if nb_openalex>0:\n",
    "        for n in range(nb_openalex): \n",
    "            author_openalex = requests.get(url).json().get(\"results\")[n]\n",
    "            result.update({'display_name':author_openalex.get('display_name'), 'openalex_id':author_openalex.get('id'), 'affiliations':author_openalex.get('affiliations'), 'topics':author_openalex.get('topics'),  'x_concepts':author_openalex.get('x_concepts'), 'ids':author_openalex.get('ids'), 'display_name_alternatives':author_openalex.get('display_name_alternatives')})\n",
    "            author.update(result)\n",
    "            df=pd.concat([df, pd.json_normalize(author)])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from config_path import PATH_API\n",
    "import os, pandas as pd\n",
    " \n",
    "\n",
    "for racine, repertoires, fichiers in os.walk(PATH_API):\n",
    "    print(f\"{racine}, {repertoires}, {fichiers}\")\n",
    "    for fichier in fichiers:\n",
    "        if fichier.startswith('persons'):\n",
    "            print(os.path.join(racine, fichier))\n",
    "            globals()[f\"{fichier}\"]= pd.read_pickle(os.path.join(racine, fichier))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fl=['persons_author.pkl', 'persons_author_10000.pkl', 'persons_author_12000.pkl', 'persons_author_2000.pkl', 'persons_author_4000.pkl', 'persons_author_6000.pkl', 'persons_author_8000.pkl', 'persons_author_name.pkl', 'persons_author_orcid.pkl']\n",
    "for racine, repertoires, fichiers in os.walk(PATH_API):\n",
    "    for i in fl:\n",
    "        name=f\"{i}\".split('.')[0]\n",
    "        print(name)\n",
    "        globals()[name] = pd.read_pickle(os.path.join(racine, fichier))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pers=pd.concat([persons_author_10000, persons_author_12000, persons_author_2000, persons_author_4000, persons_author_6000, persons_author_8000], ignore_index=True)\n",
    "\n",
    "fl=['persons_author.pkl', 'persons_author_name.pkl', 'persons_author_orcid.pkl']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def openalex_orcid(author):\n",
    "    # from config_api import openalex_usermail\n",
    "    import requests, time\n",
    "    try:\n",
    "        url = f\"https://api.openalex.org/authors/orcid:{author.get('orcid')}?mailto=zmenesr@gmail.com\"\n",
    "        author_openalex = requests.get(url).json()\n",
    "        result = author | {'display_name':author_openalex.get('display_name'), \n",
    "                           'openalex_id':author_openalex.get('id'), \n",
    "                           'affiliations':author_openalex.get('affiliations'), \n",
    "                           'topics':author_openalex.get('topics'),  \n",
    "                           'x_concepts':author_openalex.get('x_concepts'), \n",
    "                           'ids':author_openalex.get('ids'), \n",
    "                           'display_name_alternatives':author_openalex.get('display_name_alternatives'),\n",
    "                           'match':'orcid'}\n",
    "        return result\n",
    "    \n",
    "    except requests.exceptions.HTTPError as http_err:\n",
    "        print(f\"\\n{time.strftime(\"%H:%M:%S\")}, {author}-> HTTP error occurred: {http_err}\")\n",
    "        return author\n",
    "    except requests.exceptions.RequestException as err:\n",
    "        print(f\"\\n{time.strftime(\"%H:%M:%S\")}, {author}-> Error occurred: {err}\")\n",
    "        return author           \n",
    "    except Exception as e:\n",
    "        print(f\"\\n{time.strftime(\"%H:%M:%S\")}, {author}-> An unexpected error occurred: {e}\")\n",
    "        return author"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def openalex_name(author):\n",
    "    import time\n",
    "    try:\n",
    "        url = f\"https://api.openalex.org/authors?filter=display_name.search:{author.get('name')}\"\n",
    "        nb_openalex=requests.get(url).json().get(\"meta\").get('count')\n",
    "        dl=[]\n",
    "        if nb_openalex>0:\n",
    "            print(nb_openalex)\n",
    "            for n in range(nb_openalex): \n",
    "                author_openalex = requests.get(url).json().get(\"results\")[n]\n",
    "                if author_openalex.get('affiliations')!=[]:\n",
    "                    result=author | {'display_name':author_openalex.get('display_name'), \n",
    "                                    'openalex_id':author_openalex.get('id'), \n",
    "                                    'affiliations':author_openalex.get('affiliations'),\n",
    "                                    'topics':author_openalex.get('topics'),\n",
    "                                    'x_concepts':author_openalex.get('x_concepts'), \n",
    "                                    'ids':author_openalex.get('ids'), \n",
    "                                    'display_name_alternatives':author_openalex.get('display_name_alternatives'),\n",
    "                                    'match':'name'}\n",
    "                    dl.append(result)\n",
    "        return dl\n",
    "    \n",
    "    except requests.exceptions.HTTPError as http_err:\n",
    "        print(f\"\\n{time.strftime(\"%H:%M:%S\")}, {author}-> HTTP error occurred: {http_err}\")\n",
    "        return author\n",
    "    except requests.exceptions.RequestException as err:\n",
    "        print(f\"\\n{time.strftime(\"%H:%M:%S\")}, {author}-> Error occurred: {err}\")\n",
    "        return author           \n",
    "    except Exception as e:\n",
    "        print(f\"\\n{time.strftime(\"%H:%M:%S\")}, {author}-> An unexpected error occurred: {e}\")\n",
    "        return author\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from config_path import PATH_CLEAN\n",
    "# from functions_shared import chunkify\n",
    "perso_part = pd.read_pickle(f\"{PATH_CLEAN}persons_participants.pkl\")\n",
    "# pp = pd.concat([perso_part[['contact', 'orcid_id', 'country_code']].drop_duplicates(), perso_app[['contact', 'orcid_id', 'country_code']].drop_duplicates()], ignore_index=True)\n",
    "pp = perso_part[['contact', 'orcid_id', 'country_code']].fillna('')\n",
    "pp=pp.loc[(pp.country_code=='FRA')].sort_values('orcid_id', ascending=False)\n",
    "pp\n",
    "data_chunks=list(chunkify(pp, 10000))\n",
    "for i in range(0, len(data_chunks)):\n",
    "    print(f\"Loop {i}, size data_chunks: {len(data_chunks)}\")\n",
    "    # print(type(data_chunks))\n",
    "    df_temp = data_chunks[i]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle, pandas as pd\n",
    "from config_path import PATH_API\n",
    "pers_api=[]\n",
    "for i in range(1,3):\n",
    "    print(i)\n",
    "    with open(f\"{PATH_API}fr_persons_author_{i}.pkl\", 'rb') as f:\n",
    "        globals()[f\"pers_api{i}\"] = pickle.load(f)\n",
    "    pers_api.extend(globals()[f\"pers_api{i}\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pers_api=pd.json_normalize(pers_api1, record_path=['affiliations'], meta=['name', 'orcid', 'display_name', 'openalex_id',  'match',  [\"ids\", \"orcid\"]],\n",
    "        errors='ignore')\n",
    "\n",
    "pers_api.columns = pers_api.columns.str.replace(r\"[.*_]+\", '_', regex=True)\n",
    "\n",
    "pers_api = (pers_api\n",
    "            .rename(columns={\n",
    "                    'institution_country_code':'country_code'})\n",
    "            .drop(columns=['institution_type','institution_lineage']))\n",
    "\n",
    "for i in ['ids_orcid', 'institution_ror']:\n",
    "    pers_api.loc[~pers_api[i].isnull(), i] = pers_api.loc[~pers_api[i].isnull()][i].str.split(\"/\").str[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from config_path import PATH_CLEAN\n",
    "perso_part = pd.read_pickle(f\"{PATH_CLEAN}persons_participants.pkl\")\n",
    "perso_app = pd.read_pickle(f\"{PATH_CLEAN}persons_applicants.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"{PATH_API}persons_author.pkl\", 'rb') as f:\n",
    "    author_orcid = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "author_orcid=pd.json_normalize(author_orcid, record_path=['affiliations'], meta=['name','orcid', 'display_name', 'ids', 'match'])\n",
    "author_orcid"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
