{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### IMPORT datasets\n",
      "- size pp_app: 380615\n",
      "- size pp_part: 97205\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zfriant\\AppData\\Local\\Temp\\ipykernel_15860\\2420430520.py:30: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  pp_part = pp_part.replace({'None': np.nan})\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd, time, re, numpy as np\n",
    "pd.options.mode.copy_on_write = True\n",
    "from IPython.display import HTML\n",
    "from functions_shared import stop_word, unzip_zip, prep_str_col, work_csv, adr_tag\n",
    "from constant_vars import ZIPNAME, FRAMEWORK\n",
    "from config_path import PATH_MATCH, PATH_SOURCE, PATH_CLEAN, PATH_ORG, PATH_WORK\n",
    "from api_process.matcher import matcher\n",
    "\n",
    "print(f\"### IMPORT datasets\")\n",
    "participation = pd.read_pickle(f\"{PATH_CLEAN}participation_current.pkl\") \n",
    "participation['participation_linked'] = participation['project_id']+\"-\"+participation['orderNumber']\n",
    "# participation = pd.read_pickle(f\"{PATH_CLEAN}participation_complete.pkl\")\n",
    "entities_info = pd.read_pickle(f\"{PATH_CLEAN}entities_info_current2.pkl\")\n",
    "# # entities = pd.read_pickle(f\"{PATH_WORK}entities_participation_current.pkl\")\n",
    "proj = pd.read_pickle(f\"{PATH_CLEAN}projects_current.pkl\")\n",
    "nuts = pd.read_pickle(\"data_files/nuts_complet.pkl\")\n",
    "\n",
    "countries = pd.read_pickle(f\"{PATH_CLEAN}country_current.pkl\")\n",
    "lien = pd.read_pickle(f\"{PATH_CLEAN}lien.pkl\")\n",
    "perso = pd.read_pickle(f\"{PATH_CLEAN}persons_current.pkl\")\n",
    "\n",
    "pp_app = unzip_zip(ZIPNAME, f\"{PATH_SOURCE}{FRAMEWORK}/\", 'proposals_applicants_departments.json', 'utf8')\n",
    "pp_app = pd.DataFrame(pp_app)\n",
    "pp_app = pp_app.rename(columns={'proposalNbr':'project_id', 'applicantPic':'pic','departmentApplicantName':'department'}).astype(str)\n",
    "pp_app = pp_app.replace({'None': np.nan})\n",
    "print(f\"- size pp_app: {len(pp_app)}\")\n",
    "\n",
    "pp_part = unzip_zip(ZIPNAME, f\"{PATH_SOURCE}{FRAMEWORK}/\", 'projects_participants_departments.json', 'utf8')\n",
    "pp_part = pd.DataFrame(pp_part)\n",
    "pp_part = pp_part.rename(columns={'projectNbr':'project_id', 'participantPic':'pic','departmentParticipantName':'department'}).astype(str)\n",
    "pp_part = pp_part.replace({'None': np.nan})\n",
    "print(f\"- size pp_part: {len(pp_part)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['project_id', 'generalPic', 'orderNumber', 'participation_linked',\n",
       "       'erc_role', 'cordis_is_sme', 'flag_entreprise',\n",
       "       'cordis_type_entity_code', 'cordis_type_entity_name_fr',\n",
       "       'cordis_type_entity_name_en', 'cordis_type_entity_acro',\n",
       "       'participation_nuts', 'region_1_name', 'region_2_name',\n",
       "       'regional_unit_name', 'country_code', 'country_code_mapping',\n",
       "       'extra_joint_organization', 'role', 'participates_as',\n",
       "       'calculated_fund', 'stage', 'beneficiary_subv', 'coordination_number',\n",
       "       'with_coord', 'is_ejo'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "participation.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size entities_all: 596738\n"
     ]
    }
   ],
   "source": [
    "entities_all = pd.concat([keep,  struct_et], ignore_index=True, axis=0)\n",
    "print(f\"size entities_all: {len(entities_all)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## add PERSO\n",
      "size entities_all before perso: 596738\n",
      "size entities_all after perso: 596738\n",
      "size entities_all after perso clean: 596738\n"
     ]
    }
   ],
   "source": [
    "########\n",
    "print(\"## add PERSO\")\n",
    "var_perso=['tel_clean', 'domaine_email', 'contact', 'num_nat_struct', 'email']\n",
    "perso=(perso\n",
    "    .mask(perso == '')\n",
    "    .groupby(['project_id', 'generalPic', 'stage'], as_index=False)[var_perso]\n",
    "    .agg(lambda x: ';'.join( x.dropna().unique()))\n",
    "    .drop_duplicates())\n",
    "\n",
    "print(f\"size entities_all before perso: {len(entities_all)}\")\n",
    "tmp=(entities_all.drop(columns='_merge')\n",
    "    .merge(perso, how='left', on=['project_id','generalPic', 'stage'], indicator=True))\n",
    "print(f\"size entities_all after perso: {len(tmp)}\")\n",
    "\n",
    "tmp1=tmp[tmp._merge=='both']\n",
    "\n",
    "var_perso.append('_merge')\n",
    "var_perso.remove('contact')\n",
    "tmp2=(tmp[tmp._merge=='left_only']\n",
    "    .drop(columns=var_perso)\n",
    "    .merge(perso.drop(columns=['stage'])\n",
    "    .drop_duplicates(), how='inner', on=['project_id','generalPic', 'contact']))\n",
    "\n",
    "if len(tmp2)>0:\n",
    "    # tmp=pd.concat([tmp[tmp._merge=='left_only'], tmp1, tmp2], ignore_index=True)\n",
    "    print(f\"A verifier code si tmp2 n'est pas null: {len(tmp)}\")\n",
    "else:\n",
    "    entities_all=pd.concat([tmp[tmp._merge=='left_only'], tmp1], ignore_index=True)\n",
    "    print(f\"size entities_all after perso clean: {len(tmp)}\")\n",
    "\n",
    "#############\n",
    "#merge des nouveaux nns\n",
    "\n",
    "entities_all=entities_all.mask(entities_all=='')\n",
    "entities_all['num_nat_struct'] = entities_all['num_nat_struct'].map(lambda x: x.split(';') if isinstance(x, str) else [])\n",
    "\n",
    "entities_all.loc[entities_all.rnsr_back.str.len()>0, 'method'] = 'orga'\n",
    "entities_all.loc[(entities_all.method.isnull())&(entities_all.rnsr_merged.str.len()>0), 'method'] = 'corda'\n",
    "entities_all.loc[(entities_all.method.isnull())&(entities_all.num_nat_struct.str.len()>0), 'method'] = 'openalex'\n",
    "entities_all.loc[(entities_all.method=='corda')&(entities_all.num_nat_struct.str.len()>0), 'method'] = entities_all.method+';openalex'\n",
    "entities_all.loc[entities_all.method.str.contains('openalex', na=False), 'resultat'] = 'a controler'\n",
    "\n",
    "\n",
    "entities_all.loc[entities_all.rnsr_merged.isnull(), 'rnsr_merged'] = entities_all.loc[entities_all.rnsr_merged.isnull(),'rnsr_merged'].apply(lambda x: [])\n",
    "entities_all.loc[(entities_all.method=='corda')|(entities_all.method=='openalex'), 'rnsr_merged'] = entities_all.apply(lambda x: list(set(x['rnsr_merged'] + x['num_nat_struct'])), axis=1)\n",
    "\n",
    "########################################\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## geoloc cleaning\n"
     ]
    }
   ],
   "source": [
    "########\n",
    "print(\"## geoloc cleaning\")\n",
    "stop_word(entities_all, 'country_code', ['street'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>postalCode</th>\n",
       "      <th>code_postal</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>31029</td>\n",
       "      <td>31029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>33600</td>\n",
       "      <td>33600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>13572</td>\n",
       "      <td>13572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>38610</td>\n",
       "      <td>38610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>75794</td>\n",
       "      <td>75794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113727</th>\n",
       "      <td>68170</td>\n",
       "      <td>68170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113747</th>\n",
       "      <td>16220</td>\n",
       "      <td>16220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113749</th>\n",
       "      <td>31750</td>\n",
       "      <td>31750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113782</th>\n",
       "      <td>92514</td>\n",
       "      <td>92514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119596</th>\n",
       "      <td>88480</td>\n",
       "      <td>88480</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1959 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       postalCode code_postal\n",
       "0           31029       31029\n",
       "1           33600       33600\n",
       "2           13572       13572\n",
       "3           38610       38610\n",
       "4           75794       75794\n",
       "...           ...         ...\n",
       "113727      68170       68170\n",
       "113747      16220       16220\n",
       "113749      31750       31750\n",
       "113782      92514       92514\n",
       "119596      88480       88480\n",
       "\n",
       "[1959 rows x 2 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp=entities_all.loc[(entities_all.country_code=='FRA')&(~entities_all.postalCode.isnull()), ['postalCode']].drop_duplicates()\n",
    "tmp['code_postal'] = tmp.postalCode.str.replace(r\"\\D*\", '', regex=True).str.strip()\n",
    "tmp['code_postal'] = tmp.code_postal.map(lambda x: np.nan if len(x)!=5. else x)\n",
    "tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "entities_all = pd.concat([entities_all, tmp.drop(columns='postalCode')], axis=1)\n",
    "HTML(entities_all.loc[(entities_all.country_code=='FRA')&(~entities_all.city.isnull()), ['city']].drop_duplicates().sort_values('city').to_html())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>postalCode</th>\n",
       "      <th>code_postal</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>31029</td>\n",
       "      <td>31029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>33600</td>\n",
       "      <td>33600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>13572</td>\n",
       "      <td>13572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>38610</td>\n",
       "      <td>38610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>75794</td>\n",
       "      <td>75794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>590114</th>\n",
       "      <td>11848</td>\n",
       "      <td>11848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>591487</th>\n",
       "      <td>N52NB</td>\n",
       "      <td>N52NB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>591917</th>\n",
       "      <td>111141</td>\n",
       "      <td>111141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>592066</th>\n",
       "      <td>560066</td>\n",
       "      <td>560066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>596309</th>\n",
       "      <td>6211 JB</td>\n",
       "      <td>6211 JB</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>31305 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       postalCode code_postal\n",
       "0           31029       31029\n",
       "1           33600       33600\n",
       "2           13572       13572\n",
       "3           38610       38610\n",
       "4           75794       75794\n",
       "...           ...         ...\n",
       "590114      11848       11848\n",
       "591487      N52NB       N52NB\n",
       "591917     111141      111141\n",
       "592066     560066      560066\n",
       "596309    6211 JB     6211 JB\n",
       "\n",
       "[31305 rows x 2 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entities_all[['postalCode', 'code_postal']].drop_duplicates()\n",
    "# entities_all.loc[entities_all.code_postal.isnull(), 'code_postal'] = entities_all.loc[entities_all.code_postal.isnull(), 'postalCode']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = entities_all[['country_code','street_2']]\n",
    "tmp = adr_tag(tmp, ['street_2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "entities_all = pd.concat([entities_all.drop(columns='street_2'), tmp.drop(columns='country_code')], axis=1)\n",
    "\n",
    "entities_all.loc[entities_all.country_code.isin(['FRA','BEL','LUX']), 'city'] = entities_all.city.str.replace(r\"\\bst\\b\", 'saint', regex=True).str.strip()\n",
    "entities_all.loc[entities_all.country_code.isin(['FRA','BEL','LUX']), 'city'] = entities_all.city.str.replace(r\"\\bste\\b\", 'sainte', regex=True).str.strip()\n",
    "\n",
    "entities_all.loc[~entities_all.city.isnull(), 'city_tag'] = entities_all.loc[~entities_all.city.isnull()].city.str.replace(r\"\\s+\", '-', regex=True)\n",
    "\n",
    "entities_all.to_pickle(f'{PATH_MATCH}entities_all.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "work_csv(tmp.loc[(tmp.country_code!='FRA')&(~tmp.street_2_tag.isnull()), 'street_2_tag'].drop_duplicates().sort_values(), 'city')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8655                                            mickiewicza\n",
       "8656                                            mickiewicza\n",
       "8657                                   gran corts catalanes\n",
       "8658                                   gran corts catalanes\n",
       "8659                                          spitalstrasse\n",
       "                                ...                        \n",
       "596733    ronda auguste louis lumiere nave tecnologico v...\n",
       "596734                                             valikatu\n",
       "596735      tecnologico universidad miguel hernandez quorum\n",
       "596736                                               orient\n",
       "596737                                  kesklinn vana louna\n",
       "Name: street_2_tag, Length: 546214, dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entities_all.loc[(entities_all.country_code!='FRA')&(~entities_all.street_2_tag.isnull()),'street_2_tag'] \n",
    "# = entities_all['street_2_tag'].str.strip()\n",
    "# tmp=entities_all.copy()\n",
    "# tmp.loc[(tmp.country_code!='FRA')&(~tmp.street_2_tag.isnull()), 'street_2_tag'] = tmp.loc[(tmp.country_code!='FRA')&(~tmp.street_2_tag.isnull())]['street_2_tag'].str.split(' ').apply(lambda x: [w for w in x if len(w) > 2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>country_code</th>\n",
       "      <th>street</th>\n",
       "      <th>street_2_tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8655</th>\n",
       "      <td>POL</td>\n",
       "      <td>mickiewicza 30</td>\n",
       "      <td>mickiewicza</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8656</th>\n",
       "      <td>POL</td>\n",
       "      <td>mickiewicza 30</td>\n",
       "      <td>mickiewicza</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8657</th>\n",
       "      <td>ESP</td>\n",
       "      <td>gran via de les corts catalanes 587</td>\n",
       "      <td>gran corts catalanes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8658</th>\n",
       "      <td>ESP</td>\n",
       "      <td>gran via de les corts catalanes 587</td>\n",
       "      <td>gran corts catalanes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8659</th>\n",
       "      <td>CHE</td>\n",
       "      <td>spitalstrasse 33</td>\n",
       "      <td>spitalstrasse</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>596733</th>\n",
       "      <td>ESP</td>\n",
       "      <td>ronda auguste y louis lumiere 23 nave 13 parqu...</td>\n",
       "      <td>ronda auguste louis lumiere nave tecnologico v...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>596734</th>\n",
       "      <td>FIN</td>\n",
       "      <td>valikatu 12</td>\n",
       "      <td>valikatu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>596735</th>\n",
       "      <td>ESP</td>\n",
       "      <td>pq tecnologico universidad miguel hernandez sn...</td>\n",
       "      <td>tecnologico universidad miguel hernandez quorum</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>596736</th>\n",
       "      <td>DNK</td>\n",
       "      <td>orient plads 1</td>\n",
       "      <td>orient</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>596737</th>\n",
       "      <td>EST</td>\n",
       "      <td>kesklinn linnaosa vana louna tn 3</td>\n",
       "      <td>kesklinn vana louna</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>546214 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       country_code                                             street  \\\n",
       "8655            POL                                     mickiewicza 30   \n",
       "8656            POL                                     mickiewicza 30   \n",
       "8657            ESP                gran via de les corts catalanes 587   \n",
       "8658            ESP                gran via de les corts catalanes 587   \n",
       "8659            CHE                                   spitalstrasse 33   \n",
       "...             ...                                                ...   \n",
       "596733          ESP  ronda auguste y louis lumiere 23 nave 13 parqu...   \n",
       "596734          FIN                                        valikatu 12   \n",
       "596735          ESP  pq tecnologico universidad miguel hernandez sn...   \n",
       "596736          DNK                                     orient plads 1   \n",
       "596737          EST                  kesklinn linnaosa vana louna tn 3   \n",
       "\n",
       "                                             street_2_tag  \n",
       "8655                                          mickiewicza  \n",
       "8656                                          mickiewicza  \n",
       "8657                                 gran corts catalanes  \n",
       "8658                                 gran corts catalanes  \n",
       "8659                                        spitalstrasse  \n",
       "...                                                   ...  \n",
       "596733  ronda auguste louis lumiere nave tecnologico v...  \n",
       "596734                                           valikatu  \n",
       "596735    tecnologico universidad miguel hernandez quorum  \n",
       "596736                                             orient  \n",
       "596737                                kesklinn vana louna  \n",
       "\n",
       "[546214 rows x 3 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entities_all.loc[(entities_all.country_code!='FRA')&(~entities_all.street_2_tag.isnull()), ['country_code', 'street','street_2_tag']]\n",
    "\n",
    "# tmp['v']=tmp['street_2_tag'].explode()\n",
    "# tmp=tmp.explode('street_2_tag')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp['nb']=tmp.groupby(['country_code','street_2_tag'], as_index=False)['street_2_tag'].transform('count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "work_csv(tmp.sort_values('street_2_tag'), 'city')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lvar=['project_id', 'generalPic', 'role', \n",
    "       'title_clean', 'gender', 'email', 'tel_clean', 'domaine_email',\n",
    "       'orcid_id', 'birth_country_code', 'nationality_country_code',\n",
    "       'host_country_code', 'sending_country_code', 'iso2', 'stage', 'contact',\n",
    "       'country_code', 'shift', 'call_year', 'thema_code', 'destination_code',\n",
    "       'entities_id', 'entities_name', 'id_secondaire', 'country_code_mapping']\n",
    "pp = pd.concat([perso_part.drop_duplicates(), perso_app.drop_duplicates()], ignore_index=True)\n",
    "\n",
    "mask=((pp.country_code=='FRA')|(pp.nationality_country_code=='FRA')|(pp.destination_code.isin(['COG', 'PF', 'STG', 'ADG', 'POC','SyG', 'PERA', 'SJI'])))&(~(pp.contact.isnull()&pp.orcid_id.isnull()))\n",
    "pp=pp.loc[mask, lvar].sort_values(['country_code','orcid_id'], ascending=False).drop_duplicates()\n",
    "pp['contact2']=pp.contact.str.replace('-', ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def persons_files_import(thema, PATH_PERSONS):\n",
    "    import re, os\n",
    "    fname=''.join([filename for filename in os.listdir(PATH_PERSONS) if thema in filename])\n",
    "    print(fname)\n",
    "\n",
    "    if fname:\n",
    "        with open(f\"{PATH_PERSONS}{fname}\", 'rb') as f:\n",
    "            return pickle.load(f)\n",
    "\n",
    "    if fname == []:\n",
    "        fmax=max(int(os.path.splitext(filename)[0].split('_')[-1]) for filename in os.listdir(PATH_PERSONS) if re.search(r\"persons_authors_[0-9]+\",filename))\n",
    "        if fmax:\n",
    "            with open(f\"{PATH_PERSONS}persons_authors_{fmax}.pkl\", 'rb') as f:\n",
    "                return pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'{PATH_PERSONS}persons_authors_erc_{CSV_DATE}.pkl', 'rb') as f:\n",
    "    pers_ers = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'{PATH_PERSONS}persons_authors_{CSV_DATE}.pkl', 'rb') as f:\n",
    "    pers_api = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def persons_api_simplify(df):\n",
    "    pers = [] \n",
    "    for p in df:\n",
    "        # elem = {k: v for k, v in p.items() if (v and v != \"NaT\")}\n",
    "\n",
    "        p['institutions'] = []\n",
    "        if p.get(\"affiliations\"):\n",
    "            for aff in p[\"affiliations\"]:  \n",
    "                res={\"institution_name\":aff.get('institution').get(\"display_name\"),\n",
    "                \"institution_ror\":aff.get('institution').get(\"ror\"),\n",
    "                \"institution_country2\":aff.get('institution').get(\"country_code\"),\n",
    "                \"years\":aff.get(\"years\")}\n",
    "                p['institutions'].append(res)\n",
    "    \n",
    "        p[\"orcid_openalex\"] = p[\"ids\"].get(\"orcid\")            \n",
    "\n",
    "        delete=['display_name_alternatives', 'topics', 'affiliations', 'id', 'last_known_institutions', 'ids']\n",
    "        for field in delete:\n",
    "            if p.get(field):\n",
    "                p.pop(field)\n",
    "\n",
    "        # elem = {k: v for k, v in elem.items() if (v and v != \"NaT\")}\n",
    "        pers.append(p)\n",
    "\n",
    "    print(len(pers))\n",
    "    return pers\n",
    "\n",
    "pers = persons_api_simplify(pers_api)\n",
    "perc = persons_api_simplify(pers_ers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def persons_results_clean(df):\n",
    "    from unidecode import unidecode\n",
    "    from functions_shared import my_country_code, prop_string\n",
    "\n",
    "    # df=pd.json_normalize(df, max_level=1)\n",
    "    df=pd.json_normalize(df, record_path=['institutions'], meta=['match', 'orcid', 'display_name', 'orcid_openalex'], errors='ignore')\n",
    "    df=df[~df.astype(str).duplicated()]\n",
    "    cols = ['display_name']\n",
    "    df = prop_string(df, cols)\n",
    "\n",
    "    df['rows_by_name_orcid'] = df.groupby(['display_name', 'orcid'], dropna=False).transform('size')\n",
    "\n",
    "    # persName_withOrcid_noAff=df[(df.match=='full_name')&(~df.orcid.isnull())&(df.institution_name.isnull())]\n",
    "    # print(f\"size person detect by name with an orcid but no affiliations: {len(persName_withOrcid_noAff)}\")\n",
    "\n",
    "\n",
    "    for i in ['orcid_openalex', 'orcid', 'institution_ror']:\n",
    "        df.loc[~df[i].isnull(), i] = df.loc[~df[i].isnull()][i].str.split(\"/\").str[-1]\n",
    "    df['institution_ror'] = 'R'+ df['institution_ror'].astype(str)\n",
    "\n",
    "    df['years']=df['years'].map(lambda liste: ';'.join(str(x) for x in liste))\n",
    "    df=df[['match', 'display_name', 'orcid_openalex', 'years', 'institution_ror', 'institution_name', 'institution_country2', 'rows_by_name_orcid']]\n",
    "    my_countries=my_country_code()\n",
    "    df=(df.merge(my_countries[['iso2', 'iso3', 'parent_iso3']].drop_duplicates(), \n",
    "                 how='left', left_on='institution_country2', right_on='iso2')\n",
    "        .drop(columns=['iso2'])\n",
    "        .rename(columns={'iso3':'institution_country_map', 'parent_iso3':'institution_country'})\n",
    "        )\n",
    "\n",
    "    from step8_referentiels.paysage import paysage_prep\n",
    "    from config_path import PATH\n",
    "    DUMP_PATH=f'{PATH}referentiel/'\n",
    "    paysage = paysage_prep(DUMP_PATH)\n",
    "    df=(df.merge(paysage[['nom_long', 'numero_ror', 'numero_paysage', 'country_code_map', 'num_nat_struct']].drop_duplicates(), \n",
    "                 how='left', left_on='institution_ror', right_on='numero_ror'))\n",
    "\n",
    "\n",
    "\n",
    "#     print(f\"-3 size {match} cleaned: {len(df)}\")\n",
    "    return df\n",
    "\n",
    "perc=persons_results_clean(perc)\n",
    "pers=persons_results_clean(pers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp=pd.concat([perc, pers], ignore_index=True)\n",
    "temp=temp[~temp.astype(str).duplicated()]\n",
    "pd.to_pickle(temp,f\"{PATH_PERSONS}persons_{CSV_DATE}.pkl\")\n",
    "len(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pers.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# temp=temp[~temp.astype(str).duplicated()]\n",
    "pd.to_pickle(temp,f\"{PATH_PERSONS}persons_{CSV_DATE}.pkl\")\n",
    "len(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# oth=pp.loc[~pp.thema_code.isin(['ERC', 'MSCA'])].drop_duplicates().reset_index(drop=True).sort_values('contact2')\n",
    "# # pers_oth[pers_oth.display_name.str.contains('aubert')].display_name.unique()\n",
    "# df=oth.merge(temp, how='inner', left_on=['contact2', 'country_code'], right_on=['display_name','iso3'])\n",
    "# df=df[~df.astype(str).duplicated()]\n",
    "# # df['years']=df['years'].map(lambda liste: ';'.join(str(x) for x in liste))\n",
    "# df['filt']=df.apply(lambda x: x['call_year'] in x['years'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pers_oth[pers_oth.display_name.str.contains('aubert')].display_name.unique()\n",
    "df=oth.merge(temp, how='inner', left_on=['contact2', 'country_code'], right_on=['display_name','iso3'])\n",
    "df=df[~df.astype(str).duplicated()]\n",
    "# df['years']=df['years'].map(lambda liste: ';'.join(str(x) for x in liste))\n",
    "df['filt']=df.apply(lambda x: x['call_year'] in x['years'], axis=1)\n",
    "\n",
    "df=df[['project_id', 'generalPic', 'role', 'orcid_id', 'orcid_openalex',\n",
    "       'nationality_country_code', 'contact2',\n",
    "        'stage', 'contact', 'display_name','country_code', \n",
    "        'entities_id', 'entities_name',\n",
    "       'id_secondaire', 'country_code_mapping', \n",
    "        'institution_ror', 'institution_name',\n",
    "       'institution_country',  'iso3', 'parent_iso3',\n",
    "       'nom_long', 'numero_ror', 'numero_paysage', 'country_code_map',\n",
    "       'num_nat_struct','rows_by_name_orcid','call_year','years', 'filt','thema_code', 'destination_code']]\n",
    "# \n",
    "\n",
    "#extract entities_id begun by PIC\n",
    "work_csv(df.loc[df.entities_id.str.startswith('pic', na=False), \n",
    "                ['generalPic', 'entities_name','id_secondaire', 'institution_ror', 'institution_name',\n",
    "                'institution_country','numero_ror', 'numero_paysage', 'num_nat_struct']].drop_duplicates(), 'verif_id_struct_from openalex')\n",
    "\n",
    "# #keep only rows with th same year\n",
    "df1=df.loc[(df.filt==True)]\n",
    "\n",
    "\n",
    "# df=df.loc[(df.filt==True)&(~df.num_nat_struct.isnull())]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "work_csv(df.loc[(df.filt==True)&(~df.num_nat_struct.isnull())], 'pers_with_nns')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp = pd.concat([perso_part.drop_duplicates(), perso_app.drop_duplicates()], ignore_index=True)\n",
    "# mask=((pp.country_code=='FRA')|(pp.nationality_country_code=='FRA')|(pp.destination_code.isin(['COG', 'PF', 'STG', 'ADG', 'POC','SyG', 'PERA', 'SJI'])))&(~(pp.contact.isnull()&pp.orcid_id.isnull()))\n",
    "pp=pp.sort_values(['country_code','orcid_id'], ascending=False).drop_duplicates()\n",
    "pp['contact2']=pp.contact.str.replace('-', ' ')\n",
    "len(pp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp.loc[pp.contact=='zoubir khatir', ['stage','project_id', 'generalPic', 'role', \n",
    "       'title_clean', 'gender', 'email', 'tel_clean',\n",
    "       'orcid_id', 'nationality_country_code',\n",
    "       'host_country_code', 'contact',\n",
    "       'country_code', 'call_year', 'thema_code', 'destination_code',\n",
    "       'entities_id', 'entities_name', 'id_secondaire', 'country_code_mapping'\n",
    "       ]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp.loc[temp.display_name=='zoubir khatir']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pp.merge(temp, how='inner', left_on=['contact2', 'country_code'], right_on=['display_name','iso3'])\n",
    "df['filt_year']=df.apply(lambda x: x['call_year'] in x['years'], axis=1)\n",
    "# same year\n",
    "df=df.loc[(df.filt_year==True)]\n",
    "#same entities\n",
    "df1=df.loc[(df.entities_id==df.numero_paysage)]\n",
    "df1=df1.assign(orcid_clean=np.where(df1.orcid_openalex.isnull(), df1.orcid_id, df1.orcid_openalex)).drop(columns='orcid_openalex').drop_duplicates()\n",
    "# df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "work_csv(df1, 'verif_orcid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import time, pickle, requests\n",
    "# from step7_persons.affiliations import openalex_name, openalex_orcid\n",
    "# from config_path import PATH_CLEAN, PATH_API, PATH_WORK\n",
    "# print(time.strftime(\"%H:%M:%S\"))\n",
    "# rlist=[]\n",
    "\n",
    "# try:\n",
    "\n",
    "\n",
    "#     author = {\n",
    "#     \"name\": \"caye pierre'\",\n",
    "#     \"orcid\": \"\"\n",
    "#     }\n",
    "\n",
    "#     if author.get(\"orcid\"):\n",
    "#         result = openalex_orcid(author)\n",
    "#         if result.get('match'):\n",
    "#             rlist.append(result)\n",
    "#         else:\n",
    "#             result = openalex_name(author)\n",
    "#             if result:\n",
    "#                 rlist.extend(result)\n",
    "#     if author.get(\"orcid\")=='':\n",
    "#         result = openalex_name(author)\n",
    "#         if result:\n",
    "#             rlist.extend(result)\n",
    "\n",
    "#     nf=f\"persons_author\"\n",
    "#     with open(f'{PATH_WORK}test.pkl', 'wb') as f:\n",
    "#         pickle.dump(rlist, f)\n",
    "#     print(time.strftime(\"%H:%M:%S\"))\n",
    "\n",
    "# except requests.exceptions.HTTPError as http_err:\n",
    "#     print(f\"\\n{time.strftime(\"%H:%M:%S\")}, {author}-> HTTP error occurred: {http_err}\")\n",
    "#     nf=f\"persons_author\"\n",
    "#     with open(f'{PATH_WORK}test.pkl', 'wb') as f:\n",
    "#         pickle.dump(rlist, f)\n",
    "#     print(time.strftime(\"%H:%M:%S\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open(f'{PATH_WORK}test.pkl', 'rb') as f:\n",
    "    test = pickle.load(f)\n",
    "test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## return results from apenalex\n",
    "\n",
    "1 - à partir d'orcid\n",
    "2 - à partir du nom car orcid pas dans openalex -> vérifier que même personne ?\n",
    "3 - à partir du nom car orcid non renseigné\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pers_api.mask(pers_api=='', inplace=True)\n",
    "tmp1=pers_api.merge(my_countries, how='left', left_on='country_code', right_on='iso2')\n",
    "# pers_api=country_clean(pers_api, ['country_code'])\n",
    "tmp1=pers_api.loc[pers_api.match=='orcid']\n",
    "len(tmp1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{len(perso_part)} ; {perso_part.columns}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_part=perso_part.merge(tmp1, how='inner', left_on=['orcid_id'], right_on=['orcid'], suffixes=('','_api'))\n",
    "tmp_part.loc[tmp_part.country_code!=tmp_part.country_code_api]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [\n",
    "    {\n",
    "        \"state\": \"Florida\",\n",
    "        \"shortname\": \"FL\",\n",
    "        \"info\": {\"governor\": \"Rick Scott\"},\n",
    "        \"counties\": [\n",
    "            {\"name\": \"Dade\", \"population\": 12345},\n",
    "            {\"name\": \"Broward\", \"population\": 40000},\n",
    "            {\"name\": \"Palm Beach\", \"population\": 60000},\n",
    "        ],\n",
    "    },\n",
    "    {\n",
    "        \"state\": \"Ohio\",\n",
    "        \"shortname\": \"OH\",\n",
    "        \"info\": {\"governor\": \"John Kasich\"},\n",
    "        \"counties\": [\n",
    "            {\"name\": \"Summit\", \"population\": 1234},\n",
    "            {\"name\": \"Cuyahoga\", \"population\": 1337},\n",
    "        ],\n",
    "    },\n",
    "]\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def openalex_name(author):\n",
    "    url = f\"https://api.openalex.org/authors?filter=display_name.search:{author.get('name')}\"\n",
    "    nb_openalex=requests.get(url).json().get(\"meta\").get('count')\n",
    "    result={}\n",
    "    if nb_openalex>0:\n",
    "        for n in range(nb_openalex): \n",
    "            author_openalex = requests.get(url).json().get(\"results\")[n]\n",
    "            result.update({'display_name':author_openalex.get('display_name'), 'openalex_id':author_openalex.get('id'), 'affiliations':author_openalex.get('affiliations'), 'topics':author_openalex.get('topics'),  'x_concepts':author_openalex.get('x_concepts'), 'ids':author_openalex.get('ids'), 'display_name_alternatives':author_openalex.get('display_name_alternatives')})\n",
    "            author.update(result)\n",
    "            df=pd.concat([df, pd.json_normalize(author)])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from config_path import PATH_API\n",
    "import os, pandas as pd\n",
    " \n",
    "\n",
    "for racine, repertoires, fichiers in os.walk(PATH_API):\n",
    "    print(f\"{racine}, {repertoires}, {fichiers}\")\n",
    "    for fichier in fichiers:\n",
    "        if fichier.startswith('persons'):\n",
    "            print(os.path.join(racine, fichier))\n",
    "            globals()[f\"{fichier}\"]= pd.read_pickle(os.path.join(racine, fichier))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fl=['persons_author.pkl', 'persons_author_10000.pkl', 'persons_author_12000.pkl', 'persons_author_2000.pkl', 'persons_author_4000.pkl', 'persons_author_6000.pkl', 'persons_author_8000.pkl', 'persons_author_name.pkl', 'persons_author_orcid.pkl']\n",
    "for racine, repertoires, fichiers in os.walk(PATH_API):\n",
    "    for i in fl:\n",
    "        name=f\"{i}\".split('.')[0]\n",
    "        print(name)\n",
    "        globals()[name] = pd.read_pickle(os.path.join(racine, fichier))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pers=pd.concat([persons_author_10000, persons_author_12000, persons_author_2000, persons_author_4000, persons_author_6000, persons_author_8000], ignore_index=True)\n",
    "\n",
    "fl=['persons_author.pkl', 'persons_author_name.pkl', 'persons_author_orcid.pkl']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def openalex_orcid(author):\n",
    "    # from config_api import openalex_usermail\n",
    "    import requests, time\n",
    "    try:\n",
    "        url = f\"https://api.openalex.org/authors/orcid:{author.get('orcid')}?mailto=zmenesr@gmail.com\"\n",
    "        author_openalex = requests.get(url).json()\n",
    "        result = author | {'display_name':author_openalex.get('display_name'), \n",
    "                           'openalex_id':author_openalex.get('id'), \n",
    "                           'affiliations':author_openalex.get('affiliations'), \n",
    "                           'topics':author_openalex.get('topics'),  \n",
    "                           'x_concepts':author_openalex.get('x_concepts'), \n",
    "                           'ids':author_openalex.get('ids'), \n",
    "                           'display_name_alternatives':author_openalex.get('display_name_alternatives'),\n",
    "                           'match':'orcid'}\n",
    "        return result\n",
    "    \n",
    "    except requests.exceptions.HTTPError as http_err:\n",
    "        print(f\"\\n{time.strftime(\"%H:%M:%S\")}, {author}-> HTTP error occurred: {http_err}\")\n",
    "        return author\n",
    "    except requests.exceptions.RequestException as err:\n",
    "        print(f\"\\n{time.strftime(\"%H:%M:%S\")}, {author}-> Error occurred: {err}\")\n",
    "        return author           \n",
    "    except Exception as e:\n",
    "        print(f\"\\n{time.strftime(\"%H:%M:%S\")}, {author}-> An unexpected error occurred: {e}\")\n",
    "        return author"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def openalex_name(author):\n",
    "    import time\n",
    "    try:\n",
    "        url = f\"https://api.openalex.org/authors?filter=display_name.search:{author.get('name')}\"\n",
    "        nb_openalex=requests.get(url).json().get(\"meta\").get('count')\n",
    "        dl=[]\n",
    "        if nb_openalex>0:\n",
    "            print(nb_openalex)\n",
    "            for n in range(nb_openalex): \n",
    "                author_openalex = requests.get(url).json().get(\"results\")[n]\n",
    "                if author_openalex.get('affiliations')!=[]:\n",
    "                    result=author | {'display_name':author_openalex.get('display_name'), \n",
    "                                    'openalex_id':author_openalex.get('id'), \n",
    "                                    'affiliations':author_openalex.get('affiliations'),\n",
    "                                    'topics':author_openalex.get('topics'),\n",
    "                                    'x_concepts':author_openalex.get('x_concepts'), \n",
    "                                    'ids':author_openalex.get('ids'), \n",
    "                                    'display_name_alternatives':author_openalex.get('display_name_alternatives'),\n",
    "                                    'match':'name'}\n",
    "                    dl.append(result)\n",
    "        return dl\n",
    "    \n",
    "    except requests.exceptions.HTTPError as http_err:\n",
    "        print(f\"\\n{time.strftime(\"%H:%M:%S\")}, {author}-> HTTP error occurred: {http_err}\")\n",
    "        return author\n",
    "    except requests.exceptions.RequestException as err:\n",
    "        print(f\"\\n{time.strftime(\"%H:%M:%S\")}, {author}-> Error occurred: {err}\")\n",
    "        return author           \n",
    "    except Exception as e:\n",
    "        print(f\"\\n{time.strftime(\"%H:%M:%S\")}, {author}-> An unexpected error occurred: {e}\")\n",
    "        return author\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from config_path import PATH_CLEAN\n",
    "# from functions_shared import chunkify\n",
    "perso_part = pd.read_pickle(f\"{PATH_CLEAN}persons_participants.pkl\")\n",
    "# pp = pd.concat([perso_part[['contact', 'orcid_id', 'country_code']].drop_duplicates(), perso_app[['contact', 'orcid_id', 'country_code']].drop_duplicates()], ignore_index=True)\n",
    "pp = perso_part[['contact', 'orcid_id', 'country_code']].fillna('')\n",
    "pp=pp.loc[(pp.country_code=='FRA')].sort_values('orcid_id', ascending=False)\n",
    "pp\n",
    "data_chunks=list(chunkify(pp, 10000))\n",
    "for i in range(0, len(data_chunks)):\n",
    "    print(f\"Loop {i}, size data_chunks: {len(data_chunks)}\")\n",
    "    # print(type(data_chunks))\n",
    "    df_temp = data_chunks[i]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle, pandas as pd\n",
    "from config_path import PATH_API\n",
    "pers_api=[]\n",
    "for i in range(1,3):\n",
    "    print(i)\n",
    "    with open(f\"{PATH_API}fr_persons_author_{i}.pkl\", 'rb') as f:\n",
    "        globals()[f\"pers_api{i}\"] = pickle.load(f)\n",
    "    pers_api.extend(globals()[f\"pers_api{i}\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pers_api=pd.json_normalize(pers_api1, record_path=['affiliations'], meta=['name', 'orcid', 'display_name', 'openalex_id',  'match',  [\"ids\", \"orcid\"]],\n",
    "        errors='ignore')\n",
    "\n",
    "pers_api.columns = pers_api.columns.str.replace(r\"[.*_]+\", '_', regex=True)\n",
    "\n",
    "pers_api = (pers_api\n",
    "            .rename(columns={\n",
    "                    'institution_country_code':'country_code'})\n",
    "            .drop(columns=['institution_type','institution_lineage']))\n",
    "\n",
    "for i in ['ids_orcid', 'institution_ror']:\n",
    "    pers_api.loc[~pers_api[i].isnull(), i] = pers_api.loc[~pers_api[i].isnull()][i].str.split(\"/\").str[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from config_path import PATH_CLEAN\n",
    "perso_part = pd.read_pickle(f\"{PATH_CLEAN}persons_participants.pkl\")\n",
    "perso_app = pd.read_pickle(f\"{PATH_CLEAN}persons_applicants.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"{PATH_API}persons_author.pkl\", 'rb') as f:\n",
    "    author_orcid = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "author_orcid=pd.json_normalize(author_orcid, record_path=['affiliations'], meta=['name','orcid', 'display_name', 'ids', 'match'])\n",
    "author_orcid"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
