{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def openalex_name(author):\n",
    "    from config_api import openalex_usermail\n",
    "    import requests, time\n",
    "    try:\n",
    "        url = f\"https://api.openalex.org/authors?filter=display_name.search:{author.get('name')}\"\n",
    "        nb_openalex=requests.get(url).json().get(\"meta\").get('count')\n",
    "        d=[]\n",
    "        if nb_openalex>0:\n",
    "            for n in range(nb_openalex):\n",
    "                author_openalex = requests.get(url).json().get(\"results\")[n]\n",
    "                result = author | {'display_name':author_openalex.get('display_name'),\n",
    "                                'openalex_id':author_openalex.get('id'), \n",
    "                                'affiliations':author_openalex.get('affiliations'), \n",
    "                                'topics':author_openalex.get('topics'), \n",
    "                                'x_concepts':author_openalex.get('x_concepts'), \n",
    "                                'ids':author_openalex.get('ids'), \n",
    "                                'display_name_alternatives':author_openalex.get('display_name_alternatives')}\n",
    "                d.append(result)\n",
    "        return d\n",
    "\n",
    "    except requests.exceptions.HTTPError as http_err:\n",
    "        print(f\"\\n{time.strftime(\"%H:%M:%S\")}, {author}-> HTTP error occurred: {http_err}\")\n",
    "    except requests.exceptions.RequestException as err:\n",
    "        print(f\"\\n{time.strftime(\"%H:%M:%S\")}, {author}-> Error occurred: {err}\")                    \n",
    "    except Exception as e:\n",
    "        print(f\"\\n{time.strftime(\"%H:%M:%S\")}, {author}-> An unexpected error occurred: {e}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def openalex_orcid(author):\n",
    "    from config_api import openalex_usermail\n",
    "    import requests, time\n",
    "    try:\n",
    "        url = f\"https://api.openalex.org/authors/orcid:{author.get('orcid')}?mailto={openalex_usermail}\"\n",
    "        author_openalex = requests.get(url).json()\n",
    "        result = author | {'display_name':author_openalex.get('display_name'), \n",
    "                           'openalex_id':author_openalex.get('id'), \n",
    "                           'affiliations':author_openalex.get('affiliations'), \n",
    "                           'institutions':author_openalex.get('affiliations')[0].get('institutions'),\n",
    "                           'institutions_years':author_openalex.get('affiliations')[0].get('years'), \n",
    "                           'topics':author_openalex.get('topics'),  \n",
    "                           'x_concepts':author_openalex.get('x_concepts'), \n",
    "                           'ids':author_openalex.get('ids'), \n",
    "                           'display_name_alternatives':author_openalex.get('display_name_alternatives')}\n",
    "        return result\n",
    "    \n",
    "    except requests.exceptions.HTTPError as http_err:\n",
    "        print(f\"\\n{time.strftime(\"%H:%M:%S\")}, {author}-> HTTP error occurred: {http_err}\")\n",
    "    except requests.exceptions.RequestException as err:\n",
    "        print(f\"\\n{time.strftime(\"%H:%M:%S\")}, {author}-> Error occurred: {err}\")                    \n",
    "    except Exception as e:\n",
    "        print(f\"\\n{time.strftime(\"%H:%M:%S\")}, {author}-> An unexpected error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df=pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "author = {\n",
    "\"name\": 'adraoui mohamed-ali',\n",
    "\"orcid\": '0000-0001-5602-8625'\n",
    "}\n",
    "\n",
    "if author.get(\"orcid\"):\n",
    "    result = openalex_orcid(author)\n",
    "    if result:\n",
    "        df=pd.concat([df, pd.json_normalize(result)])\n",
    "    elif result is None:\n",
    "        result = openalex_name(author)\n",
    "        if result:\n",
    "            df=pd.concat([df, pd.json_normalize(result)], ignore_index=True)\n",
    "if author.get(\"orcid\")=='':\n",
    "    result = openalex_name(author)\n",
    "    if result:\n",
    "        df=pd.concat([df, pd.json_normalize(result)], ignore_index=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "author = {\n",
    "\"name\": 'adraoui mohamed-ali',\n",
    "\"orcid\": '0000-0001-5602-8625'\n",
    "}\n",
    "url = f\"https://api.openalex.org/authors?filter=display_name.search:{author.get('name')}\"\n",
    "nb_openalex=requests.get(url).json().get(\"meta\").get('count')\n",
    "if nb_openalex>0:\n",
    "    for n in range(nb_openalex):\n",
    "        print(n)\n",
    "        author_openalex = requests.get(url).json().get(\"results\")[n]\n",
    "        result = author | {'display_name':author_openalex.get('display_name'),\n",
    "                        'openalex_id':author_openalex.get('id'), \n",
    "                        'affiliations':author_openalex.get('affiliations'), \n",
    "                        'topics':author_openalex.get('topics'), \n",
    "                        'x_concepts':author_openalex.get('x_concepts'), \n",
    "                        'ids':author_openalex.get('ids'), \n",
    "                'display_name_alternatives':author_openalex.get('display_name_alternatives')}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [\n",
    "    {\n",
    "        \"state\": \"Florida\",\n",
    "        \"shortname\": \"FL\",\n",
    "        \"info\": {\"governor\": \"Rick Scott\"},\n",
    "        \"counties\": [\n",
    "            {\"name\": \"Dade\", \"population\": 12345},\n",
    "            {\"name\": \"Broward\", \"population\": 40000},\n",
    "            {\"name\": \"Palm Beach\", \"population\": 60000},\n",
    "        ],\n",
    "    },\n",
    "    {\n",
    "        \"state\": \"Ohio\",\n",
    "        \"shortname\": \"OH\",\n",
    "        \"info\": {\"governor\": \"John Kasich\"},\n",
    "        \"counties\": [\n",
    "            {\"name\": \"Summit\", \"population\": 1234},\n",
    "            {\"name\": \"Cuyahoga\", \"population\": 1337},\n",
    "        ],\n",
    "    },\n",
    "]\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CSV_DATE='20250121'\n",
    "# def persons_preparation(csv_date):\n",
    "\n",
    "import pandas as pd, numpy as np\n",
    "pd.options.mode.copy_on_write = True\n",
    "from constant_vars import ZIPNAME, FRAMEWORK\n",
    "from config_path import PATH_SOURCE, PATH_CLEAN, PATH_ORG, PATH_WORK\n",
    "from functions_shared import unzip_zip\n",
    "\n",
    "###############################\n",
    "participation = pd.read_pickle(f\"{PATH_CLEAN}participation_current.pkl\") \n",
    "project = pd.read_pickle(f\"{PATH_CLEAN}projects_current.pkl\") \n",
    "countries = pd.read_pickle(f\"{PATH_CLEAN}country_current.pkl\")\n",
    "\n",
    "print(f\"size participation: {len(participation)}\")\n",
    "######################\n",
    "perso_part = unzip_zip(f'he_grants_ecorda_pd_{CSV_DATE}.zip', f\"{PATH_SOURCE}{FRAMEWORK}/\", \"participant_persons.csv\", 'utf-8')\n",
    "perso_part = (perso_part.loc[perso_part.FRAMEWORK=='HORIZON',\n",
    "        ['PROJECT_NBR', 'GENERAL_PIC', 'PARTICIPANT_PIC', 'ROLE', 'FIRST_NAME',\n",
    "        'LAST_NAME', 'TITLE', 'GENDER', 'PHONE', 'EMAIL',\n",
    "        'BIRTH_COUNTRY_CODE', 'NATIONALITY_COUNTRY_CODE', 'HOST_COUNTRY_CODE', 'SENDING_COUNTRY_CODE']]\n",
    "            .rename(columns=str.lower)\n",
    "            .rename(columns={'project_nbr':'project_id', 'general_pic':'generalPic', 'participant_pic':'pic'})\n",
    "            .assign(stage='successful'))\n",
    "\n",
    "######################################\n",
    "    \n",
    "perso_app = unzip_zip(f'he_proposals_ecorda_pd_{CSV_DATE}.zip', f\"{PATH_SOURCE}{FRAMEWORK}/\", \"applicant_persons.csv\", 'utf-8')\n",
    "\n",
    "perso_app = (perso_app.loc[perso_app.FRAMEWORK=='HORIZON',\n",
    "    ['PROPOSAL_NBR', 'GENERAL_PIC', 'APPLICANT_PIC', 'ROLE', 'FIRST_NAME',\n",
    "    'FAMILY_NAME', 'TITLE', 'GENDER', 'PHONE', 'EMAIL',\n",
    "    'RESEARCHER_ID', 'ORCID_ID', 'GOOGLE_SCHOLAR_ID','SCOPUS_AUTHOR_ID']]\n",
    "            .rename(columns=str.lower)\n",
    "            .rename(columns={'proposal_nbr':'project_id', 'general_pic':'generalPic', 'applicant_pic':'pic', 'family_name':'last_name'})\n",
    "            .assign(stage='evaluated'))\n",
    "######################################\n",
    "def country_clean(df, countries):\n",
    "    import json\n",
    "    dict_c = countries.set_index('countryCode')['country_code_mapping'].to_dict()\n",
    "    cl=['birth_country_code','nationality_country_code','host_country_code','sending_country_code']\n",
    "    ccode=json.load(open(\"data_files/countryCode_match.json\"))\n",
    "    for c in cl:\n",
    "        for k,v in ccode.items():\n",
    "            df.loc[df[c]==k, c] = v\n",
    "        for k,v in dict_c.items():\n",
    "            df.loc[df[c]==k, c] = v\n",
    "        \n",
    "        if any(perso_part[c].str.len()<3):\n",
    "            print(f\"ATTENTION ! un {c} non reconnu dans df {perso_part.loc[perso_part[c].str.len()<3, ['project_id', c]]}\")\n",
    "    return df\n",
    "\n",
    "perso_part = country_clean(perso_part, countries)\n",
    "\n",
    "####################################\n",
    "def title_clean(df):\n",
    "    df.loc[~df['title'].isnull(), 'title_clean'] = df.loc[~df['title'].isnull(), 'title'].str.replace(r\"[^\\w\\s]+\", \" \", regex=True)\n",
    "    df.loc[~df['title_clean'].isnull(), 'title_clean'] = df.loc[~df['title_clean'].isnull(), 'title_clean'].str.replace(r\"\\s+\", \" \", regex=True).str.strip()\n",
    "    df.mask(df == '', inplace=True)\n",
    "    return df\n",
    "\n",
    "perso_part = title_clean(perso_part)\n",
    "perso_app = title_clean(perso_app)\n",
    "\n",
    "###############################\n",
    "def prop_contact(tab):\n",
    "    from unidecode import unidecode\n",
    "    cols = ['role', 'first_name', 'last_name','title_clean', 'gender']\n",
    "    tab[cols] = tab[cols].map(lambda s:s.casefold() if type(s) == str else s)\n",
    "    for i in cols:\n",
    "        # tab[i] = tab[i].apply(unicode if type(s) == str else s)\n",
    "        tab.loc[~tab[i].isnull(), i] = tab.loc[~tab[i].isnull(), i].apply(unidecode)\n",
    "    return tab\n",
    "\n",
    "perso_part = prop_contact(perso_part)\n",
    "perso_app = prop_contact(perso_app)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "########## \n",
    "def contact_name(df):\n",
    "    for f in ['first_name', 'last_name']:\n",
    "        df[f] = df[f].fillna('')\n",
    "        df[f] = df[f].str.strip().str.replace(r\"\\s+\", '-', regex=True)\n",
    "        df[f] = df[f].str.strip().str.replace(r\"-{2,}\", '-', regex=True)\n",
    "\n",
    "    df['contact'] = df.last_name.astype(str).str.lower() + ' ' + df.first_name.astype(str).str.lower()\n",
    "    return df\n",
    "\n",
    "perso_app = contact_name(perso_app)\n",
    "perso_part = contact_name(perso_part)\n",
    "\n",
    "\n",
    "# ###########\n",
    "# generalPic empty ; replace by pic or fill by generalPic participation\n",
    "def empty_pic(df, participation, stage):\n",
    "    if any(df.generalPic.isnull()):\n",
    "        print(f\"1 - size rows with generelPic null for {stage}: {len(df[df.generalPic.isnull()])}\")\n",
    "        df.loc[df.generalPic.isnull(), 'generalPic'] = df.loc[df.generalPic.isnull(), 'pic']\n",
    "\n",
    "        # gestion empty generalPic for principal investigator\n",
    "        x=df.loc[(df.generalPic.isnull())&(df.role=='principal investigator')].project_id.unique()\n",
    "        if x.size>0:\n",
    "            y=participation.loc[(participation.project_id.isin(x))&(participation.stage==stage), ['project_id', 'generalPic']]\n",
    "            df=df.merge(y, how='left', on=['project_id'], suffixes=('', '_y'))\n",
    "            df.loc[(df.generalPic.isnull())&(~df.generalPic_y.isnull()), 'generalPic'] = df.loc[(df.generalPic.isnull())&(~df.generalPic_y.isnull()), 'generalPic_y'] \n",
    "            df.drop(columns='generalPic_y', inplace=True)\n",
    "            print(f\"2 - size rows with generelPic null for {stage}: {len(df[df.generalPic.isnull()])}\")\n",
    "    return df\n",
    "\n",
    "perso_part = empty_pic(perso_part, participation, 'successful')\n",
    "perso_app = empty_pic(perso_app, participation, 'evaluated')\n",
    "\n",
    "################\n",
    "def perso_measure(df):\n",
    "    df['nb_pic_unique']=df.groupby(['project_id'])['generalPic'].transform('nunique') #combien de pics / projet\n",
    "    df['nb_name_unique']=df.groupby(['project_id'])['last_name'].transform('nunique') #combien de pics / projet\n",
    "    df['nb_row_by_pic']=df.groupby(['project_id', 'generalPic'])['last_name'].transform('count') #combien de lignes par pic\n",
    "    df['nb_name_unique_by_pic']=df.groupby(['project_id', 'generalPic'])['last_name'].transform('nunique')\n",
    "    df['nb_row_by_pic_name'] = df.groupby(['project_id', 'generalPic','last_name'])['last_name'].transform('count')\n",
    "    df['nb_row_by_pic_name_unique'] = df.groupby(['project_id', 'generalPic','last_name'])['last_name'].transform('nunique')\n",
    "    df['nb_pic_by_contact_unique'] = df.groupby(['project_id','contact'])['generalPic'].transform('count')\n",
    "    \n",
    "    # print(f\"size df: {len(df)}\\ncolumns:{df.columns}\")\n",
    "    print(f\"size df: {len(df)}\")\n",
    "    return df\n",
    "\n",
    "perso_part = perso_measure(perso_part)\n",
    "perso_app = perso_measure(perso_app)\n",
    "\n",
    "################\n",
    "def generaPic_remove(df):\n",
    "    return df.loc[~((df.nb_pic_unique>0)&(df.generalPic.isnull()))]\n",
    "\n",
    "perso_part = generaPic_remove(perso_part)\n",
    "perso_app = generaPic_remove(perso_app)\n",
    "\n",
    "##############################\n",
    "def name_duplicated_remove(df):\n",
    "    #### cleaning name duplicated by project \n",
    "    ## if by project single name but several rows\n",
    "    # x[x.project_id=='101039481']\n",
    "\n",
    "    print(df.role.unique())\n",
    "    keep_order=['principal investigator', 'fellow', 'main_contact']\n",
    "    if len(df.role.unique()) > len(keep_order):\n",
    "        print(f\"2 - Attention ! un role nouveau dans perso -> {set(df.role.unique())-set(keep_order)}\")\n",
    "\n",
    "    tmp=pd.DataFrame()\n",
    "    mask=[(df.nb_row_by_pic_name_unique==1)&(df.nb_row_by_pic_name>1)]\n",
    "    for i in mask:\n",
    "        x=df.loc[i]\n",
    "        print(f\"3 - size x before remove: {len(x)}\")\n",
    "        x=x.groupby(['project_id','generalPic', 'last_name']).apply(lambda i: i.sort_values('role', key=lambda col: pd.Categorical(col, categories=keep_order, ordered=True)), include_groups=True).reset_index(drop=True)\n",
    "        for v in ['title', 'gender','phone','email','birth_country_code','nationality_country_code','host_country_code','sending_country_code']:\n",
    "            if v in x.columns:\n",
    "                x[v]=x.groupby(['project_id', 'generalPic', 'last_name'])[v].bfill()\n",
    "        x=x.groupby(['project_id', 'generalPic', 'last_name']).head(1)\n",
    "        print(f\"3 - size x after remove: {len(x)}\")\n",
    "\n",
    "        tmp=pd.concat([tmp, x], ignore_index=True)\n",
    "\n",
    "    df=df.merge(tmp[['project_id', 'generalPic', 'last_name']].drop_duplicates(), how='outer', on=['project_id', 'generalPic', 'last_name'], indicator=True).query('_merge==\"left_only\"')\n",
    "    df=pd.concat([df, tmp], ignore_index=True)\n",
    "\n",
    "    if len(df)==0:\n",
    "        print(f\"ATTENTION table vide après traitement name_duplicated_remove\")\n",
    "    else:\n",
    "        print(f\"size après traitement name_duplicated_remove: {len(df)}\")\n",
    "\n",
    "    return df.drop(columns=['_merge'])\n",
    "\n",
    "perso_part = name_duplicated_remove(perso_part)\n",
    "perso_app = name_duplicated_remove(perso_app)\n",
    "# ####################################\n",
    "perso_part = perso_measure(perso_part)\n",
    "perso_app = perso_measure(perso_app)\n",
    "\n",
    "def PI_duplicated(df):\n",
    "    if any(df.role=='principal investigator'):\n",
    "        # select if same person and one PI in a single project \n",
    "        mask=(df.nb_pic_by_contact_unique>1)&(df.role=='principal investigator')\n",
    "        pi=df.loc[mask, ['project_id', 'contact']].drop_duplicates().merge(df, how='inner')\n",
    "        pi['role'] = 'principal investigator'\n",
    "        for v in ['title', 'gender','birth_country_code','nationality_country_code','sending_country_code']:\n",
    "            if v in df.columns:\n",
    "                pi=pi.sort_values(v)\n",
    "                pi[v]=pi.groupby(['project_id', 'contact'])[v].ffill()\n",
    "        \n",
    "        df=df.merge(pi[['project_id', 'generalPic', 'contact']].drop_duplicates(), how='outer', on=['project_id', 'generalPic', 'contact'], indicator=True).query('_merge==\"left_only\"')\n",
    "        df=pd.concat([df, pi], ignore_index=True)\n",
    "        print(f\"-size df after cleaning pi_duplicated: {len(df)}\")\n",
    "        return df.drop(columns=['_merge'])\n",
    "    \n",
    "perso_part=PI_duplicated(perso_part)\n",
    "\n",
    "\n",
    "def perso_participation(df, participation, project, stage):\n",
    "    df=df.loc[df.project_id.isin(participation[participation.stage==stage].project_id.unique())]\n",
    "    df=df.merge(participation.loc[participation.stage==stage, ['project_id', 'generalPic', 'country_code']], how='outer', on=['project_id', 'generalPic'], indicator=True).query('_merge!=\"right_only\"')\n",
    "    df.loc[df._merge=='left_only', 'shift'] = 'past'\n",
    "\n",
    "    if stage=='successful':\n",
    "        df.loc[(df._merge=='both')&(df.host_country_code.isnull()), 'host_country_code'] = df.loc[(df._merge=='both')&(df.host_country_code.isnull()), 'country_code']\n",
    "\n",
    "\n",
    "    df=df.merge(project.loc[project.stage==stage, ['project_id', 'call_year', 'thema_name_en', 'destination_name_en']], how='inner', on=['project_id'])\n",
    "\n",
    "    if len(df)==0:\n",
    "        print(f\"ATTENTION table vide après lein avec participation\")\n",
    "    else:\n",
    "        print(f\"size app lien avec participation clean : {len(df)}\\ncolumns:{df.columns}\")\n",
    "    return df\n",
    "\n",
    "perso_part = perso_participation(perso_part, participation, project, 'successful')\n",
    "perso_app = perso_participation(perso_app, participation, project, 'evaluated')\n",
    "\n",
    "# ##################\n",
    "def phone_clean(df):\n",
    "    y = df.loc[(df.country_code=='FRA')&(~df.phone.isnull()), ['phone']]\n",
    "    y['tel_clean']=y.phone.str.replace(r\"(^\\++[0-9]{1,3}\\s+)\", '', regex=True)\n",
    "    y['tel_clean']=y.tel_clean.str.replace(r\"[^0-9]+\", '', regex=True)\n",
    "    y['tel_clean']=y.tel_clean.str.replace(r\"^(33|033|0033)\", '', regex=True).str.rjust(10, '0')\n",
    "    y.loc[(y.tel_clean.str.len()>10)&(y.tel_clean.str[0:1]=='0'), 'tel_clean'] = y.tel_clean.str[0:10]\n",
    "    y['tel_clean']=y.tel_clean.str.replace(r\"^0+$\", '', regex=True)\n",
    "    # work_csv(y, 'tel_perso')\n",
    "    return pd.concat([df, y[['tel_clean']]], axis=1)\n",
    "\n",
    "perso_part = phone_clean(perso_part)\n",
    "perso_app = phone_clean(perso_app)\n",
    "\n",
    "# #######################\n",
    "def mail_clean(df):\n",
    "    mail_del=[\"gmail\", \"yahoo\", \"hotmail\", \"wanadoo\", \"aol\", \"free\", \"skynet\", \"outlook\", \"icloud\", \"googlemail\"]\n",
    "\n",
    "    df['domaine'] = df.email.str.split('@').str[1].str.split('.').str[:-1].fillna('').apply(' '.join)\n",
    "    tmp = df.loc[~df.domaine.isnull(), ['domaine']]\n",
    "\n",
    "    for el in mail_del:\n",
    "        m = r\"^\"+el+r\"($|\\s)\"\n",
    "        tmp.loc[tmp['domaine'].str.contains(m, case=True, flags=0, na=None, regex=True) == True, 'domaine_email'] = ''\n",
    "        tmp.loc[tmp['domaine_email'].isnull(), 'domaine_email'] = tmp['domaine']\n",
    "\n",
    "    return pd.concat([df, tmp], axis=1).drop(columns='domaine')\n",
    "\n",
    "perso_app = mail_clean(perso_app)\n",
    "perso_part = mail_clean(perso_part)\n",
    "##############\n",
    "# add orcid_id (perso_app) into perso_part\n",
    "\n",
    "perso_part=perso_part.merge(perso_app[['project_id', 'contact', 'orcid_id']], how='left', on=['project_id', 'contact']) \n",
    "perso_app=perso_app.merge(perso_part[['project_id', 'contact', 'nationality_country_code']], how='left', on=['project_id', 'contact']) \n",
    "    # perso_app.to_pickle(f\"{PATH_CLEAN}perso_app.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perso_part[['project_id', 'contact', 'gender', ]]\n",
    "\n",
    "x=perso_part[['project_id', 'contact', 'gender','title_clean']].merge(perso_app[['project_id', 'contact', 'gender', 'title_clean']], how='inner', on=['project_id', 'contact'], suffixes=('_x','_y')).drop_duplicates()\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.loc[(x.gender_x.isnull())&(~x.gender_y.isnull())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gender_title_merge(part, app):\n",
    "    x=perso_part[['project_id', 'contact', 'gender','title_clean']].merge(perso_app[['project_id', 'contact', 'gender', 'title_clean']], how='inner', on=['project_id', 'contact'], suffixes=('_x','_y')).drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perso_part[perso_part.project_id=='101189474']\n",
    "perso_part[perso_part.project_id=='101039103']\n",
    "perso_part[perso_part.project_id=='101039481']\n",
    "perso_app[perso_app.project_id=='101039348']\n",
    "# perso_part[perso_part.project_id=='101043645']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perso_app.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "entities_tmp=entities_all.loc[((entities_all.country_code=='FRA')&(entities_all.rnsr_merged.str.len()==0))|((entities_all.country_code!='FRA')&(entities_all.entities_id.str.contains('pic'))), ['project_id','generalPic','country_code', 'entities_id', 'entities_name']]\n",
    "perso = perso[['call_year', 'thema_name_en', 'destination_name_en' ,'project_id', 'country_code', 'generalPic', 'title', 'last_name', 'first_name', 'tel_clean', 'domaine_email', 'contact', 'orcid_id']]\n",
    "\n",
    "pp = perso[['project_id','generalPic' ,'contact', 'orcid_id']].drop_duplicates().merge(entities_tmp, how='inner', on=['project_id','generalPic'])\n",
    "# pp.mask(pp=='', inplace=True)\n",
    "pp = pp.fillna('')\n",
    "# pp = pp.loc[pp.orcid_id=='', ['contact', 'orcid_id']].drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def openalex_name(author):\n",
    "    url = f\"https://api.openalex.org/authors?filter=display_name.search:{author.get('name')}\"\n",
    "    nb_openalex=requests.get(url).json().get(\"meta\").get('count')\n",
    "    result={}\n",
    "    if nb_openalex>0:\n",
    "        for n in range(nb_openalex): \n",
    "            author_openalex = requests.get(url).json().get(\"results\")[n]\n",
    "            result.update({'display_name':author_openalex.get('display_name'), 'openalex_id':author_openalex.get('id'), 'affiliations':author_openalex.get('affiliations'), 'topics':author_openalex.get('topics'),  'x_concepts':author_openalex.get('x_concepts'), 'ids':author_openalex.get('ids'), 'display_name_alternatives':author_openalex.get('display_name_alternatives')})\n",
    "            author.update(result)\n",
    "            df=pd.concat([df, pd.json_normalize(author)])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests, pandas as pd, time\n",
    "df=pd.DataFrame()\n",
    "author = {\n",
    "\"name\": 'elyaakoubi mustapha',\n",
    "# \"name\": 'jeremy peglion',\n",
    "\"orcid\": ''\n",
    "}\n",
    "try:\n",
    "    if author.get(\"orcid\"):\n",
    "    # Get author by Orcid\n",
    "        url = f\"https://api.openalex.org/authors/orcid:{author.get('orcid')}?mailto=bso@recherche.gouv.fr\"\n",
    "        author_openalex = requests.get(url).json().get(\"results\")\n",
    "        author.update({'display_name':author_openalex.get('display_name'), 'openalex_id':author_openalex.get('id'), 'affiliations':author_openalex.get('affiliations'), 'topics':author_openalex.get('topics'),  'x_concepts':author_openalex.get('x_concepts'), 'ids':author_openalex.get('ids'), 'display_name_alternatives':author_openalex.get('display_name_alternatives')})\n",
    "        df=pd.concat([df, pd.json_normalize(author)])\n",
    "    else:\n",
    "        url = f\"https://api.openalex.org/authors?filter=display_name.search:{author.get('name')}\"\n",
    "        nb_openalex=requests.get(url).json().get(\"meta\").get('count')\n",
    "        result={}\n",
    "        if nb_openalex>0:\n",
    "            for n in range(nb_openalex): \n",
    "                author_openalex = requests.get(url).json().get(\"results\")[n]\n",
    "                result.update({'display_name':author_openalex.get('display_name'), 'openalex_id':author_openalex.get('id'), 'affiliations':author_openalex.get('affiliations'), 'topics':author_openalex.get('topics'),  'x_concepts':author_openalex.get('x_concepts'), 'ids':author_openalex.get('ids'), 'display_name_alternatives':author_openalex.get('display_name_alternatives')})\n",
    "                author.update(result)\n",
    "                df=pd.concat([df, pd.json_normalize(author)])\n",
    "except requests.exceptions.HTTPError as http_err:\n",
    "    print(f\"\\n{time.strftime(\"%H:%M:%S\")}-> HTTP error occurred: {http_err}\")\n",
    "except requests.exceptions.RequestException as err:\n",
    "    print(f\"\\n{time.strftime(\"%H:%M:%S\")}-> Error occurred: {err}\")                    \n",
    "except Exception as e:\n",
    "    print(f\"\\n{time.strftime(\"%H:%M:%S\")}-> An unexpected error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if author_openalex:\n",
    "        # author.update({'openalex_id':author_openalex.get('id'),'affiliations':author_openalex.get('affiliations'), 'topics':author_openalex.get('topics'),  'x_concepts':author_openalex.get('x_concepts'), 'orcid_tmp':author_openalex.get('orcid'), 'display_name_alternatives':author_openalex.get('display_name_alternatives')})\n",
    "        result.append(author_openalex)\n",
    "# pd.json_normalize(result)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perso.orcid_id.value_counts()\n",
    "x=perso[['project_id','generalPic' ,'contact', 'orcid_id']].drop_duplicates().merge(entities_tmp, how='inner', on=['project_id','generalPic'])\n",
    "# x=perso[['project_id','generalPic' ,'contact', 'orcid_id']].drop_duplicates()\n",
    "x.orcid_id.value_counts()\n",
    "len(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "print(\"\\n### FP7\")\n",
    "def call_api():\n",
    "    call=pd.read_json(open(f\"data_files/FP7_calls.json\", 'r+', encoding='utf-8'))\n",
    "    call = pd.DataFrame(call)\n",
    "    call['call_budget'] = call['call_budget'].str.replace(',', '').astype('float')\n",
    "    return call\n",
    "call=call_api()\n",
    "\n",
    "def ref_select(FP):\n",
    "    ref_source = ref_source_load('ref')\n",
    "    # traitement ref select le FP, id non null ou/et ZONAGE non null\n",
    "    ref = ref_source_2d_select(ref_source, FP)\n",
    "    return ref\n",
    "ref, genPic_to_new=ref_select('FP7')\n",
    "\n",
    "def FP7_load():\n",
    "    FP7_PATH=f'{PATH}FP7/2022/'\n",
    "    _FP7 = pd.read_pickle(f\"{FP7_PATH}FP7_data.pkl\")\n",
    "    _FP7.rename(columns={'name_source':'legalName', 'acronym_source':'businessName'}, inplace=True)\n",
    "    print(f\"size _FP7 load: {len(_FP7)}\\n{_FP7.columns}\")\n",
    "    return _FP7\n",
    "_FP7=FP7_load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "country = pd.read_csv(f\"{PATH_SOURCE}H2020/country_current.csv\", sep=';', encoding='utf-8')\n",
    "def FP7_cleaning(_FP7, country):\n",
    "    _FP7 = _FP7.loc[~_FP7.status_code.isin(['INELIGIBLE','WITHDRAWN'])]\n",
    "    _FP7.loc[_FP7.status_code=='Project Closed', 'status_code'] = 'CLOSED'\n",
    "    _FP7.loc[_FP7.status_code=='Project Terminated', 'status_code'] = 'TERMINATED'\n",
    "\n",
    "    _FP7.loc[_FP7.participant_type_code=='N/A', 'participant_type_code'] = 'NA'\n",
    "    _FP7['role'] = _FP7['role'].str.lower()\n",
    "    _FP7.loc[_FP7.role=='participant', 'role'] = 'partner'\n",
    "    _FP7['coordination_number']=np.where(_FP7['role']=='coordinator', 1, 0)\n",
    "    _FP7.loc[(_FP7.generalPic=='998133396')&(_FP7.countryCode=='ZZ'), 'country_code_mapping'] = 'USA' # bristol meyer\n",
    "    print(f\"- size _FP7 after clean status: {len(_FP7)}, size with id: {len(_FP7.loc[~_FP7.id.isnull()])}\")\n",
    "    \n",
    "    zz = _FP7.loc[(_FP7.country_code_mapping=='ZZZ')]\n",
    "    print(f\"- size _FP7 sans country_code: {len(zz)}\")\n",
    "    zz = ref.loc[ref.generalPic.isin(zz.generalPic.unique())]\n",
    "    _FP7 = _FP7.merge(zz, how='left', on='generalPic', suffixes=['','_ref'])\n",
    "    for i in ['id', 'country_code_mapping', 'ZONAGE']:\n",
    "        _FP7.loc[~_FP7[f\"{i}_ref\"].isnull(), i] = _FP7[f\"{i}_ref\"]\n",
    "    _FP7 = _FP7.drop(_FP7.filter(regex='_ref$').columns, axis=1)\n",
    "    print(f\"- size _FP7 with country: {len(_FP7)}, {_FP7.loc[_FP7.stage=='successful', 'funding'].sum()}\")\n",
    "    \n",
    "    p = _FP7[['generalPic', 'country_code_mapping','country_code']].drop_duplicates()\n",
    "    print(f\"- size de p: {len(p)}\")\n",
    "    #lien part et ref\n",
    "    p = p.merge(ref, how='outer', on=['generalPic', 'country_code_mapping'], indicator=True).drop_duplicates()\n",
    "    p = p.loc[p._merge.isin(['both', 'left_only'])]\n",
    "    # print(f\"cols de p: {p.columns}\")\n",
    "\n",
    "    # p1 pic+ccm commun\n",
    "    p1 = p.loc[p['_merge']=='both'].drop(columns=['_merge', 'country_code'])\n",
    "    print(f\"- size p1 pic+cc: {len(p1)}\")\n",
    "\n",
    "    # p2 pic cc\n",
    "    p2 = (p.loc[p['_merge']=='left_only'].drop(columns=['_merge', 'id', 'ZONAGE'])\n",
    "        .merge(ref.rename(columns={'country_code_mapping':'country_code'}), \n",
    "                how='inner', on=['generalPic', 'country_code']).drop_duplicates()\n",
    "        .drop(columns='country_code'))\n",
    "    print(f\"- size p2 pic cc_parent: {len(p2)}\")\n",
    "\n",
    "    # acteurs sans identifiant dont le pic à plusieurs pays ou le pic certaines participations ont un identifiant et pas d'autres \n",
    "    p3 = (p.loc[p['_merge']=='left_only'].drop(columns=['_merge', 'country_code_mapping', 'id', 'ZONAGE'])\n",
    "        .merge(ref, how='inner', on=['generalPic']).drop_duplicates())\n",
    "    if not p3.empty:\n",
    "        print(f\"1 - A faire si possible, vérifier pourquoi des participations avec pic identiques ont un id ou pas nb pic: {len(p3.generalPic.unique())}\")\n",
    "\n",
    "    if 'p2' in globals() or 'p2' in locals():\n",
    "        p1 = pd.concat([p1,p2], ignore_index=True).drop_duplicates()\n",
    "        print(f\"2 - size de new p: {len(p)}, cols: {p.columns}\") \n",
    "\n",
    "    FP7 = (_FP7.drop(columns=['id', 'ZONAGE', 'country_code'])\n",
    "            .merge(p1[['generalPic', 'country_code_mapping', 'id', 'ZONAGE']], \n",
    "                how='left', on=['generalPic', 'country_code_mapping']))\n",
    "    print(f\"- size _FP7 with ref: {len(_FP7)}, size FP7: {len(FP7)},  size with id: {len(FP7.loc[~FP7.id.isnull()])}\")\n",
    "    \n",
    "    FP7 = FP7.merge(country[['country_code_mapping', 'country_name_mapping', 'country_code']].drop_duplicates(), how='left', on='country_code_mapping')\n",
    "    # FP7.loc[~FP7.ZONAGE.isnull(), 'country_code'] = FP7.ZONAGE\n",
    "    if any(FP7.country_code.isnull()):\n",
    "        print(f\"country_code null {FP7.loc[FP7.country_code.isnull(), ['country_code_mapping', 'country_name_mapping']].drop_duplicates()}\")\n",
    "        FP7.loc[FP7.country_code_mapping=='GUF', 'country_code'] = 'FRA'\n",
    "        FP7.loc[FP7.country_code_mapping=='GUF', 'country_name_mapping'] = 'French Guiana'\n",
    "        FP7.loc[FP7.country_code_mapping.isin(['SGS', 'IOT']), 'country_code'] = 'GBR'\n",
    "        FP7.loc[FP7.country_code_mapping=='IOT', 'country_name_mapping'] = 'British Indian Ocean Territory'\n",
    "        FP7.loc[FP7.country_code_mapping=='SGS', 'country_name_mapping'] = 'South Georgia and the South Sandwich Islands'\n",
    "\n",
    "    cc = country.drop(columns=['country_code_mapping', 'country_name_mapping', 'countryCode', 'countryCode_parent']).drop_duplicates()\n",
    "    FP7 = FP7.merge(cc, how='left', on='country_code')\n",
    "    FP7.loc[FP7.country_code_mapping=='ZOE', 'country_name_mapping'] = 'European organisations area'\n",
    "\n",
    "    FP7.loc[FP7.country_code_mapping=='ZOE', 'country_code'] = 'ZOE'\n",
    "    FP7.loc[FP7.country_code=='ZOE', 'country_name_fr'] = 'Union Européenne'\n",
    "    FP7.loc[FP7.country_code=='ZOE', 'country_name_en'] = 'European organisations area'\n",
    "\n",
    "    print(f\"size FP7 with country assoc: {len(FP7)},\\ncols: {FP7.columns}\")    \n",
    "    return FP7\n",
    "FP7=FP7_cleaning(_FP7, country)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FP7.loc[(FP7.country_code=='DEU')&(FP7.stage=='successful')&(FP7.pilier!='EURATOM')].funding.sum()\n",
    "# FP7.pilier.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def FP7_entities(FP7, country):\n",
    "    print(\"\\n## FP7 entities\")\n",
    "    # part.country_code.unique()\n",
    "    entities = FP7.loc[~FP7.id.isnull(), ['generalPic','id', 'country_code_mapping']].drop_duplicates()\n",
    "    print(f\"- size entities {len(entities)}\")\n",
    "    if any(entities.id.str.contains(';')):\n",
    "        entities = entities.assign(id_extend=entities.id.str.split(';')).explode('id_extend')\n",
    "        entities.loc[(entities.id.str.contains(';', na=False))&(entities.id_extend.str.len()==14), 'id_extend'] = entities.loc[(entities.id.str.contains(';', na=False))&(entities.id_extend.str.len()==14)].id_extend.str[:9]\n",
    "        entities = entities.drop_duplicates()\n",
    "        entities_size_to_keep = len(entities)\n",
    "        print(f\"2 - size entities si multi id -> entities_size_to_keep = {entities_size_to_keep}\")\n",
    "\n",
    "    ror = pd.read_pickle(f\"{PATH_REF}ror_df.pkl\")\n",
    "    entities_tmp = merge_ror(entities, ror, country)\n",
    "    print(f\"size entities_tmp after add ror_info: {len(entities_tmp)}, entities_size_to_keep: {entities_size_to_keep}\")\n",
    "\n",
    "\n",
    "    # PAYSAGE\n",
    "    ### si besoin de charger paysage pickle\n",
    "    paysage = pd.read_pickle(f\"{PATH_REF}paysage_df.pkl\")\n",
    "    if any(paysage.groupby('id')['id_clean'].transform('count')>1):\n",
    "        print(f\"1 - paysage doublon oublié: {paysage[paysage.groupby('id')['id_clean'].transform('count')>1][['id', 'id_clean']].sort_values('id')}\")\n",
    "        paysage = paysage.loc[~((paysage.id_clean=='vey7g')&(paysage.id.str.contains('265100057', na=False)))]    \n",
    "    \n",
    "    paysage_category = pd.read_pickle(f\"{PATH_SOURCE}paysage_category.pkl\")\n",
    "    cat_filter = category_paysage(paysage_category)\n",
    "    entities_tmp = merge_paysage(entities_tmp, paysage, cat_filter)\n",
    "\n",
    "    sirene = pd.read_pickle(f\"{PATH_REF}sirene_df.pkl\")\n",
    "    entities_tmp = merge_sirene(entities_tmp, sirene)\n",
    "\n",
    "    # traitement des id identifiés mais sans referentiels liés\n",
    "    entities_tmp.loc[(entities_tmp.entities_id.isnull())&(~entities_tmp.id_extend.str.contains('-', na=False)), 'entities_id'] = entities_tmp['id_extend']\n",
    "\n",
    "    entities_tmp['siren']=entities_tmp.loc[entities_tmp.entities_id.str.contains('^[0-9]{9}$|^[0-9]{14}$', na=False)].entities_id.str[:9]\n",
    "    entities_tmp.loc[entities_tmp.siren.isnull(), 'siren']=entities_tmp.paysage_siren\n",
    "\n",
    "    #groupe\n",
    "\n",
    "    # recuperation tous les siren pour lien avec groupe -> creation var SIREN \n",
    "    entities_tmp.loc[~entities_tmp.siren.isnull(), \"siren\"] = entities_tmp.loc[~entities_tmp.siren.isnull(), \"siren\"].str.split().apply(set).str.join(\";\")\n",
    "\n",
    "    if any(entities_tmp.siren.str.contains(';', na=False)):\n",
    "        print(\"1 - ATTENTION faire code pour traiter deux siren différents -> ce qui serait bizarre qu'il y ait 2 siren\")\n",
    "    else:\n",
    "        ### si besoin de charger groupe\n",
    "        file_name = f\"{PATH_REF}H20_groupe.pkl\"\n",
    "        groupe = pd.read_pickle(file_name)\n",
    "        print(f\"2 - taille de entities_tmp avant groupe:{len(entities_tmp)}\")\n",
    "\n",
    "        entities_tmp=entities_tmp.merge(groupe, how='left', on='siren')\n",
    "\n",
    "        # entities_tmp.loc[~entities_tmp.groupe_id.isnull(), 'entities_id']= entities_tmp.groupe_id\n",
    "        # entities_tmp.loc[~entities_tmp.groupe_id.isnull(), 'entities_acronym'] = entities_tmp.groupe_acronym\n",
    "        # entities_tmp.loc[~entities_tmp.groupe_id.isnull(), 'entities_name'] = entities_tmp.groupe_name\n",
    "\n",
    "        # entities_tmp.loc[entities_tmp.entities_id.str.contains('gent', na=False), 'siren_cj'] = 'GE_ENT'\n",
    "        \n",
    "        # entities_tmp = entities_tmp.drop(['groupe_id','groupe_name','groupe_acronym'], axis=1).drop_duplicates()\n",
    "        print(f\"- size entities_tmp after groupe {len(entities_tmp)}\")\n",
    "\n",
    "    entities_tmp = entities_tmp.merge(get_source_ID(entities_tmp, 'entities_id'), how='left', on='entities_id')\n",
    "        # traitement catégorie\n",
    "    # entities_tmp = category_cleaning(entities_tmp, sirene)\n",
    "    entities_tmp = category_woven(entities_tmp, sirene)\n",
    "    entities_tmp = category_agreg(entities_tmp)\n",
    "    return  entities_tmp\n",
    "entities_tmp=FP7_entities(FP7, country)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n## FP7 calculation\")\n",
    "print(f\"- size part before: {len(FP7)}\")\n",
    "part1 = (FP7[['project_id', 'participant_order', 'role', 'generalPic', 'global_costs',\n",
    "    'participant_type_code', 'legalName', 'businessName', 'countryCode', 'nutsCode',\n",
    "    'funding', 'status.x', 'ADRESS', 'city', 'post_code', 'pme', 'stage', 'nom', 'countryCode_parent', 'vat_id',\n",
    "    'country_code_mapping', 'participant_id', 'number_involved', 'coordination_number', 'id', 'ZONAGE',\n",
    "    'country_name_mapping', 'country_code', 'country_name_en','country_association_code', 'country_association_name_en',\n",
    "    'country_group_association_code', 'country_group_association_name_en','country_group_association_name_fr', \n",
    "    'country_name_fr', 'article1', 'article2']]\n",
    "        .merge(entities_tmp, how='left', on=['generalPic', 'country_code_mapping', 'id']))\n",
    "print(f\"- size part before: {len(part1)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "part1 = (FP7[['project_id', 'participant_order', 'role', 'generalPic', 'global_costs',\n",
    "    'participant_type_code', 'legalName', 'businessName', 'countryCode', 'nutsCode',\n",
    "    'funding', 'status.x', 'ADRESS', 'city', 'post_code', 'pme', 'stage', 'nom', 'countryCode_parent', 'vat_id',\n",
    "    'country_code_mapping', 'participant_id', 'number_involved', 'coordination_number', 'id', 'ZONAGE',\n",
    "    'country_name_mapping', 'country_code', 'country_name_en','country_association_code', 'country_association_name_en',\n",
    "    'country_group_association_code', 'country_group_association_name_en','country_group_association_name_fr', \n",
    "    'country_name_fr', 'article1', 'article2']]\n",
    "        .merge(entities_tmp, how='left', on=['generalPic', 'country_code_mapping', 'id']))\n",
    "\n",
    "part2=(part1.loc[part1.entities_name.isnull()].drop_duplicates())\n",
    "part3=(part2.sort_values(['legalName', 'businessName'], ascending=False)\n",
    "    .groupby(['generalPic', 'country_code_mapping'])\n",
    "    .first().reset_index()[['generalPic', 'country_code_mapping', 'legalName', 'businessName']]\n",
    "    .rename(columns={'legalName':'entities_name', 'businessName':'entities_acronym'}))\n",
    "\n",
    "part2 = (part2.drop(columns=['entities_name', 'entities_acronym', 'nom'])\n",
    "        .merge(part3, how='left', on=['generalPic', 'country_code_mapping']))\n",
    "part2['entities_name'] = part2.entities_name.str.capitalize().str.strip()\n",
    "part2['entities_id'] = \"pic\"+part2.generalPic.map(str)\n",
    "\n",
    "part1=part1.loc[~part1.entities_name.isnull()].drop_duplicates()\n",
    "\n",
    "part1=pd.concat([part1, part2], ignore_index=True).assign(number_involved=1)\n",
    "\n",
    "part1['nb'] = part1.id.str.split(';').str.len()\n",
    "for i in ['funding', 'coordination_number', 'number_involved']:\n",
    "    part1[i] = np.where(part1['nb']>1, part1[i]/part1['nb'], part1[i])\n",
    "\n",
    "# 'requestedGrant'\n",
    "print(f\"- size part after: {len(part1)}\")\n",
    "\n",
    "if any(part1.entities_id=='nan')|any(part1.entities_id.isnull()):\n",
    "    print(f\"1 - attention il reste des entities sans entities_id valides\")\n",
    "\n",
    "type_entity = pd.read_json(open('data_files/legalEntityType.json', 'r', encoding='UTF-8'))\n",
    "# part1.loc[part1.participant_type_code=='N/A', 'participant_type_code'] = 'NA'\n",
    "part1 = (part1.merge(type_entity, how='left', left_on='participant_type_code', right_on='cordis_type_entity_code')\n",
    ".drop(columns='participant_type_code'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "part1.loc[(part1.country_code=='DEU')&(part1.stage=='successful')&(part1.project_id.isin(pp))].funding.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(part1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# # # gestion code nuts\n",
    "# nuts = pd.read_pickle(\"data_files/nuts_complet.pkl\")\n",
    "# nuts = (nuts[['nuts_code_2013','nutsCode', 'lvl1Description', 'lvl2Description', 'lvl3Description']]\n",
    "#         .drop_duplicates()\n",
    "#         .rename(columns={'nuts_code_2013':'nuts_code_tmp', 'nutsCode':'nuts_code','lvl1Description':'region_1_name', 'lvl2Description': 'region_2_name', 'lvl3Description':'regional_unit_name'}))\n",
    "# # nuts['region_1_name'] = nuts['region_1_name'].str.title()\n",
    "# print(len(nuts))\n",
    "\n",
    "part1['nuts_code_tmp'] = np.where(part1.nutsCode.str.len()<3, np.nan, part1.nutsCode)\n",
    "\n",
    "print(f\"- size part1 with code after cleanup nuts: {len(part1[~part1.nuts_code_tmp.isnull()])}\")\n",
    "\n",
    "nuts = nuts.loc[(nuts.nuts_code_tmp.isin(part1.nuts_code_tmp.unique()))&(~nuts.nuts_code_tmp.isnull())]\n",
    "part1 = part1.merge(nuts, how='left', on='nuts_code_tmp').drop_duplicates()\n",
    "print(f\"- nuts code without name: {len(part1[(~part1.nuts_code.isnull())&(part1.region_1_name.isnull())])}\")\n",
    "\n",
    "# print(part1.groupby(['stage'], dropna=True )['nuts_code'].size())\n",
    "print(part1.loc[part1.stage=='successful', 'funding'].sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "instr = pd.read_csv('data_files/instru_nomenclature.csv', sep=';')\n",
    "act=pd.read_json(open(\"data_files/actions_name.json\", 'r', encoding='utf-8'))\n",
    "msca_correspondence = pd.read_table('data_files/msca_correspondence.csv', sep=\";\").drop(columns='framework')\n",
    "erc_correspondence = pd.read_json(open(\"data_files/ERC_correspondance.json\", 'r', encoding='utf-8'))\n",
    "thema = pd.read_json(open(\"data_files/thema.json\", 'r', encoding='utf-8'))\n",
    "destination = pd.read_json(open(\"data_files/destination.json\", 'r', encoding='utf-8'))\n",
    "\n",
    "def themes_cleaning(FP7):\n",
    "    print(\"## FP7 themes\")\n",
    "    print(f\"- size proj before cleaning: {len(FP7[['project_id', 'stage']].drop_duplicates())}\")\n",
    "    proj = (FP7.assign(stage_name=np.where(FP7.stage=='successful', 'projets lauréats', 'projets évalués'))\n",
    "            [['project_id', 'stage', 'acronym', 'abstract', 'title', 'call_id', 'stage_name',\n",
    "            'call_deadline', 'instrument',  'panel_code', 'panel_name', 'call_year', 'duration', 'status_code', \n",
    "        'cost_total', 'eu_reqrec_grant', 'free_keywords', 'number_involved', 'submission_date',\n",
    "        'start_date', 'signature_date', 'end_date',  'pilier', 'prog_abbr', 'prog_lib', 'area_abbr', 'area_lib']]\n",
    "            .drop_duplicates())\n",
    "\n",
    "    proj.loc[(proj.prog_abbr=='ERC')&(proj.instrument=='POC'), 'instrument'] = 'ERC-POC'\n",
    "    proj.loc[proj.prog_abbr=='PEOPLE', 'thema_code'] = 'MSCA'\n",
    "    proj.loc[proj.prog_abbr=='ERC', 'thema_code'] = 'ERC'\n",
    "\n",
    "    # print(f\"- size proj: {len(proj)}\")\n",
    "\n",
    "    proj = proj.merge(instr, how='left', on='instrument').drop(columns=['name'])\n",
    "    proj.loc[proj.instrument.str.contains('MC-'), 'action_code'] = 'MSCA'        \n",
    "\n",
    "    if any(proj.action_code.isnull()):\n",
    "        print(proj[proj.action_code.isnull()].instrument.unique())   \n",
    "        \n",
    "    print(f\"- size proj after instru: {len(proj)}\")\n",
    "\n",
    "    # ERC\n",
    "    proj = proj.merge(erc_correspondence, how='left', left_on=['instrument'], right_on=['old'])\n",
    "\n",
    "    proj.loc[(proj.thema_code=='ERC')&(proj.destination_code.isnull()), 'destination_code'] = 'ERC-OTHER'\n",
    "\n",
    "    proj.loc[proj.thema_code=='ERC', 'programme_code'] = 'ERC'\n",
    "    proj.loc[proj.thema_code=='ERC', 'programme_name_en'] = 'European Research Council (ERC)'\n",
    "    print(f\"- size proj after ERC: {len(proj)}\")\n",
    "\n",
    "    # MSCA\n",
    "    proj = proj.merge(msca_correspondence, how='left', left_on=['instrument'], right_on=['old'])\n",
    "    proj.loc[proj.call_id.str.contains('NIGHT'), 'destination_detail_code'] = 'CITIZENS'\n",
    "    proj.loc[~proj.destination_detail_code.isnull(), 'destination_code'] = proj.destination_detail_code.str.split('-').str[0]\n",
    "    proj.loc[(proj.destination_code.isnull())&(proj.thema_code=='MSCA'), 'destination_code'] = 'MSCA-OTHER'\n",
    "    proj.loc[proj.thema_code=='MSCA', 'programme_code'] = 'MSCA'\n",
    "    proj.loc[proj.thema_code=='MSCA', 'programme_name_en'] = 'Marie Skłodowska-Curie Actions (MSCA)'\n",
    "\n",
    "    proj.rename(columns={'instrument':'fp_specific_instrument'}, inplace=True)\n",
    "\n",
    "    print(f\"- size proj success after msca: {proj.loc[proj.stage=='successful'].project_id.nunique()}, nb project_id: {len(proj.loc[proj.stage=='successful'])}\")\n",
    "    print(f\"- size proj after msca: {len(proj)}\")\n",
    "    #euratom\n",
    "    proj.loc[(proj.pilier.isin(['EURATOM']))&(proj.prog_abbr=='Fission'), 'programme_code'] = 'NFRP'\n",
    "    proj.loc[(proj.pilier.isin(['EURATOM']))&(proj.programme_code=='NFRP'), 'programme_name_en'] = 'Nuclear fission and radiation protection'\n",
    "    proj.loc[proj.prog_abbr=='Fusion', 'programme_code'] = 'Fusion'\n",
    "    proj.loc[proj.prog_abbr=='Fusion', 'programme_name_en'] = 'Fusion Energy'\n",
    "\n",
    "    euratom = pd.read_csv('data_files/euratom_thema_all_FP.csv', sep=';', na_values='')\n",
    "    proj = proj.merge(euratom[['topic_area', 'thema_code', 'thema_name_en']], how='left', left_on='area_abbr', right_on='topic_area', suffixes=['', '_t'])\n",
    "    proj.loc[(~proj.thema_code_t.isnull()), 'thema_code'] = proj.loc[(~proj.thema_code_t.isnull()), 'thema_code_t']\n",
    "    proj = proj.filter(regex=r'.*(?<!_t)$')\n",
    "    print(f\"- size proj after euratom: {len(proj)}\")\n",
    "\n",
    "    #ju_jti\n",
    "    proj.loc[proj.prog_abbr=='SP1-JTI', 'thema_code'] = 'JU-JTI'\n",
    "    proj.loc[proj.prog_abbr=='SP1-JTI', 'destination_code'] = proj.area_abbr.str.split('-').str[-1]\n",
    "    proj.loc[proj.area_abbr=='JTI-CS', 'destination_code'] = 'CLEAN-AVIATION'\n",
    "\n",
    "    proj.loc[(proj.destination_code=='CLEAN-SKY'), 'destination_code'] = 'CLEAN-AVIATION'\n",
    "    proj.loc[(proj.destination_code=='FCH'), 'destination_code'] = 'CLEANH2'\n",
    "    proj.loc[(proj.destination_code=='IMI'), 'destination_code'] = 'IHI'\n",
    "    proj.loc[(proj.destination_code.isin(['ENIAC','ARTEMIS'])), 'destination_code'] = 'Chips'\n",
    "    proj.loc[proj.thema_code=='JU-JTI', 'action_code'] = proj.fp_specific_instrument.str.split('-').str[1]\n",
    "    print(f\"- size proj after ju-jti: {len(proj)}\")\n",
    "\n",
    "    # WIDENING COST\n",
    "    proj.loc[proj.area_abbr.str.contains('COST', na=False), 'thema_code'] = 'COST'\n",
    "    proj.loc[proj.area_abbr.str.contains('COST', na=False), 'programme_code'] = 'Widening'\n",
    "    proj.loc[proj.area_abbr.str.contains('COST', na=False), 'programme_name_en'] = 'Widening participation and spreading excellence'\n",
    "    print(f\"- size proj after cost: {len(proj)}\")\n",
    "\n",
    "    proj.loc[proj.pilier=='EURATOM', 'pilier_name_en'] = 'Euratom'\n",
    "    proj.loc[(proj.prog_abbr.isin(['PEOPLE','ERC']))|(proj.prog_abbr=='INFRA'), 'pilier_name_en'] = 'Excellent Science'\n",
    "    proj.loc[proj.pilier_name_en.isnull(), 'pilier_name_en'] = proj.pilier.str.capitalize()\n",
    "    print(f\"- size proj after cost: {len(proj)}\")\n",
    "\n",
    "    proj.loc[proj.programme_code.isnull(), 'programme_code'] = proj.prog_abbr\n",
    "    proj.loc[proj.programme_name_en.isnull(), 'programme_name_en'] = proj.prog_lib\n",
    "    print(f\"- size proj after cost: {len(proj)}\")\n",
    "\n",
    "    proj.loc[(~proj.thema_code.isin(['MSCA','ERC']))&(proj.destination_code.isnull()), 'destination_code'] = proj.area_abbr\n",
    "    proj.loc[proj.destination_code.isnull(), 'destination_code'] = proj.thema_code+'-OTHER'\n",
    "    proj = proj.merge(destination[['destination_code', 'destination_name_en']], how='left', on='destination_code')\n",
    "    proj = (proj\n",
    "            .merge(destination.rename(columns={'destination_code':'destination_detail_code', 'destination_name_en':'destination_detail_name_en'})\n",
    "            [['destination_detail_code', 'destination_detail_name_en']], how='left', on='destination_detail_code')\n",
    "            .drop_duplicates())\n",
    "    print(f\"- size proj after cost: {len(proj)}\")\n",
    "\n",
    "    proj.loc[(~proj.thema_code.isin(['MSCA','ERC']))&(proj.destination_name_en.isnull()), 'destination_name_en'] = proj.area_lib\n",
    "    proj.loc[proj.thema_code.isnull(), 'thema_code'] = proj.prog_abbr\n",
    "    proj = proj.merge(thema[['thema_code', 'thema_name_en']], how='left', on='thema_code', suffixes=['', '_t'])\n",
    "    proj.loc[proj.thema_name_en.isnull(), 'thema_name_en'] = proj.thema_name_en_t\n",
    "    proj.loc[proj.thema_name_en.isnull(),'thema_name_en'] = proj.prog_lib\n",
    "    proj = proj.filter(regex=r'.*(?<!_t)$')\n",
    "    print(f\"- size proj after thema_code: {len(proj)}\")\n",
    "\n",
    "    proj = (proj.drop(columns=['area_abbr', 'area_lib'])\n",
    "            .rename(columns={'prog_lib':'fp_specific_programme', 'pilier':'fp_specific_pilier'}))\n",
    "    \n",
    "    print(proj[['programme_code',\n",
    "    'programme_name_en', 'thema_name_en', 'destination_code', 'destination_name_en',\n",
    "    'destination_detail_code','destination_detail_name_en']].drop_duplicates())\n",
    "    print(len(proj))\n",
    "    return proj\n",
    "proj=themes_cleaning(FP7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def proj_cleaning(proj):\n",
    "    print(f\"- size proj before cleaning: {len(proj)}\")\n",
    "    proj = proj.merge(act, how='left', on='action_code')\n",
    "    proj = proj.merge(call, how='left', on='call_id').assign(ecorda_date=pd.to_datetime('2021-04-30'), framework='FP7')\n",
    "    proj = proj.assign(ecorda_date=pd.to_datetime('2021-04-30'), framework='FP7')\n",
    "    for i in ['title', 'abstract', 'free_keywords']:\n",
    "        proj[i]=proj[i].str.replace('\\\\n|\\\\t|\\\\r|\\\\s+', ' ', regex=True).str.strip()\n",
    "\n",
    "    kw = proj[['project_id', 'free_keywords']]\n",
    "    kw = kw.assign(free_keywords = kw.free_keywords.str.split(';|,')).explode('free_keywords')\n",
    "    kw = kw.loc[kw.free_keywords.str.len()>3].drop_duplicates()\n",
    "    kw.free_keywords = kw.free_keywords.groupby(level=0).apply(lambda x: '|'.join(x.str.strip().unique()))\n",
    "\n",
    "    proj = proj.drop(columns='free_keywords').merge(kw.drop_duplicates(), how='left', on='project_id')\n",
    "    proj.mask(proj=='', inplace=True)  \n",
    "\n",
    "    for d in ['call_deadline', 'signature_date',  'start_date',  'end_date', 'submission_date']:\n",
    "        proj[d] = pd.to_datetime(proj[d],format='%d/%m/%Y %H:%M:%S')\n",
    "    print(f\"- size proj cleaned: {len(proj)}\")\n",
    "    return proj\n",
    "proj=proj_cleaning(proj)\n",
    "# def proj_ods(proj, part1):\n",
    "#     country=(part1.loc[part1.stage=='successful',\n",
    "#                 ['project_id','country_code','country_name_fr','country_code_mapping', 'ZONAGE',\n",
    "#                     'country_name_mapping', 'nuts_code', 'region_1_name', 'region_2_name','regional_unit_name']]\n",
    "#         .drop_duplicates()\n",
    "#         .groupby(['project_id'], as_index = False).agg(lambda x: ';'.join(map(str,filter(None, x))))\n",
    "#         .drop_duplicates())\n",
    "\n",
    "#     prop = (proj.loc[proj.stage=='evaluated', ['project_id', 'cost_total', 'eu_reqrec_grant', 'number_involved']]\n",
    "#         .rename(columns={'number_involved':'proposal_numberofapplicants', 'eu_reqrec_grant':'proposal_requestedgrant', 'cost_total':'proposal_budget'})\n",
    "#         .drop_duplicates())\n",
    "\n",
    "#     p = (proj.loc[proj.stage=='successful', ['project_id', 'eu_reqrec_grant', 'number_involved', 'cost_total']]\n",
    "#         .rename(columns={'eu_reqrec_grant':'project_eucontribution', 'number_involved':'project_numberofparticipants','cost_total':'project_totalcost'})\n",
    "#         .drop_duplicates())\n",
    "\n",
    "#     # # PROVISOIRE quand def call refonctionnera\n",
    "#     # proj=proj.assign(call_budget=np.nan)\n",
    "\n",
    "#     project = (proj.loc[proj.stage=='successful', \n",
    "#             ['abstract', 'acronym', 'action_code', 'action_name', 'call_budget','call_deadline', 'call_id', 'call_year',\n",
    "#             'destination_code','destination_detail_code', 'destination_detail_name_en', 'destination_name_en', \n",
    "#             'duration', 'ecorda_date', 'end_date', 'fp_specific_instrument', 'framework', 'free_keywords', \n",
    "#             'panel_code', 'panel_name', 'fp_specific_programme', 'fp_specific_pilier',\n",
    "#             'pilier_name_en', 'programme_code', 'programme_name_en', 'project_id', 'signature_date', 'stage', 'stage_name', \n",
    "#             'start_date', 'status_code', 'submission_date', 'thema_code', 'thema_name_en', 'title']]\n",
    "            \n",
    "#         .drop_duplicates())\n",
    "\n",
    "#     project = project.merge(p, how='left', on='project_id').merge(country, how='inner', on='project_id').merge(prop, how='left' , on='project_id')\n",
    "\n",
    "#     print(f\"1 - size project lauréats: {len(project)}, {len(p)}, fund: {'{:,.1f}'.format(p['project_eucontribution'].sum())}\")\n",
    "\n",
    "#     with open(f\"{PATH_CLEAN}FP7_successful_projects.pkl\", 'wb') as file:\n",
    "#         pd.to_pickle(project, file)\n",
    "#     return project\n",
    "# proj_ods(proj, part1)\n",
    "\n",
    "def FP7_all(proj, part1):\n",
    "    t = (proj.drop(columns=['cost_total', 'duration', 'end_date', 'eu_reqrec_grant', 'fp_specific_instrument', \n",
    "                        'fp_specific_programme', 'fp_specific_pilier',\n",
    "                        'number_involved', 'signature_date', 'start_date', 'submission_date'])\n",
    "        .merge(part1, how='inner', on=['project_id', 'stage'])\n",
    "        .rename(columns={'funding':'calculated_fund', 'ZONAGE':'extra_joint_organization'}))\n",
    "    \n",
    "    t = (t.assign(is_ejo=np.where(t.extra_joint_organization.isnull(), 'Sans', 'Avec')))\n",
    "\n",
    "    t.loc[(t.destination_code.isin(['PF', 'ERARESORG', 'GA']))|((t.thema_code.isin(['ERC', 'COST']))&(t.destination_code!='SyG')), 'coordination_number'] = 0\n",
    "    t=t.assign(with_coord=True)\n",
    "    t.loc[(t.destination_code.isin(['PF', 'ERARESORG', 'GA']))|((t.thema_code.isin(['ERC', 'COST']))&(t.destination_code!='SyG')), 'with_coord'] = False\n",
    "\n",
    "    t.loc[t.thema_code=='ERC', 'erc_role'] = 'partner'\n",
    "\n",
    "    t.loc[(t.destination_code=='SyG'), 'erc_role'] = 'PI'\n",
    "    t.loc[(t.action_code=='ERC')&(t.destination_code!='SyG')&(t.role=='coordinator'), 'erc_role'] = 'PI'\n",
    "    t.loc[(t.destination_code=='ERC-OTHER'), 'erc_role'] = np.nan\n",
    "\n",
    "\n",
    "    file_name = f\"{PATH_CLEAN}FP7_data.pkl\"\n",
    "    with open(file_name, 'wb') as file:\n",
    "        pd.to_pickle(t, file)\n",
    "\n",
    "    print(f\"size proj: {t.loc[t.stage=='successful'].project_id.nunique()}, nb project_id: {len(t.loc[t.stage=='successful'])}, {t.loc[t.stage=='successful', 'calculated_fund'].sum()}\")\n",
    "    return t\n",
    "t=FP7_all(proj, part1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t.loc[(t.country_code=='DEU')&(t.stage=='successful')&(t.call_year=='2007')&(t.pilier_name_en!='Euratom')].calculated_fund.sum()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functions_shared import *\n",
    "t=t.drop_duplicates().loc[(t.stage=='successful')&(t.pilier_name_en!='Euratom')]\n",
    "print(len(t))\n",
    "x=pd.crosstab(t['country_code'], t['call_year'], values=t['calculated_fund'], aggfunc='sum',margins=True, margins_name= 'All').reset_index()\n",
    "work_csv(x, 'fp7_count')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
