{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3b64eb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd, numpy as np, json\n",
    "from step3_entities.references import ref_source_load, ref_source_2d_select\n",
    "from step3_entities.merge_referentiels import merge_paysage, merge_ror, merge_sirene\n",
    "from step3_entities.categories import category_agreg, category_paysage,category_woven, cordis_type, mires\n",
    "from step3_entities.ID_getSourceRef import get_source_ID\n",
    "from step4_calculations.collaborations import collab_base, collab_cross\n",
    "from config_path import PATH_SOURCE, PATH_CLEAN, PATH_REF, PATH_CONNECT\n",
    "from functions_shared import unzip_zip, my_country_code\n",
    "\n",
    "def h20_nom_load():\n",
    "    destination = pd.read_json(open(\"data_files/destination.json\", 'r', encoding='utf-8'))\n",
    "    thema = pd.read_json(open(\"data_files/thema.json\", 'r', encoding='utf-8'))\n",
    "    act = pd.read_json(open(\"data_files/actions_name.json\", 'r', encoding='utf-8'))\n",
    "    topics = unzip_zip('H2020_2022-12-05.json.zip', f\"{PATH_SOURCE}H2020/\", 'topics.json', encode='utf-8')\n",
    "    pilier_fr = pd.read_json(open(\"data_files/H20_pilier.json\", 'r', encoding='utf-8'))\n",
    "    # countries = pd.read_csv(f\"{PATH_SOURCE}H2020/country_current.csv\", sep=';')\n",
    "    countries = pd.read_pickle(f\"{PATH_CLEAN}country_current.pkl\")\n",
    "    actions = pd.read_table(f\"{PATH_CLEAN}actions_current.csv\", sep=\";\")\n",
    "    nuts = pd.read_pickle(f'{PATH_REF}nuts_complet.pkl')\n",
    "    return destination, thema, act, topics, pilier_fr, countries, actions, nuts\n",
    "destination, thema, act, topics, pilier_fr, countries, actions, nuts = h20_nom_load()\n",
    "\n",
    "def h20_load():\n",
    "    print(\"## LOAD bases\")\n",
    "    _proj=pd.read_pickle(f\"{PATH_SOURCE}H2020/H2020_projects.pickle\")\n",
    "    _proj=pd.DataFrame(_proj)\n",
    "    _proj=_proj.replace('#', np.nan)\n",
    "    print(f\"size _proj: {len(_proj)}\")\n",
    "    part=pd.read_pickle(f\"{PATH_SOURCE}H2020/H2020_participation.pickle\")\n",
    "    part=pd.DataFrame(part)\n",
    "    part=part.replace('#', np.nan)\n",
    "    print(f\"- size part: {len(part)}\")\n",
    "    entities = unzip_zip('H2020_2022-12-05.json.zip', f\"{PATH_SOURCE}H2020/\", \"legalEntities.json\", encode='utf-8')\n",
    "    status = pd.read_csv(f\"{PATH_SOURCE}H2020/redressement_status_code.csv\", sep=';', usecols=['project_id','stat_code'], dtype='str')\n",
    "    return _proj, part, entities, status\n",
    "_proj, part, entities, status = h20_load()\n",
    "print(f\"involved successful:{'{:,.1f}'.format(part.loc[(part.stage=='successful'), 'generalPic'].count())}\\nsubv_net_laureat:{'{:,.1f}'.format(part.loc[(part.stage=='successful'), 'subv_net'].sum())}\\nsubv_laureat:{'{:,.1f}'.format(part.loc[(part.stage=='successful'), 'subv'].sum())}\\nsubv_prop:{'{:,.1f}'.format(part.loc[(part.stage=='evaluated'), 'requestedGrant'].sum())}\")\n",
    "\n",
    "country_h20 = my_country_code()\n",
    "\n",
    "part.loc[part.role=='participant', 'role'] = 'partner'\n",
    "# part.loc[part.countryCode=='ZZ', 'country_code_mapping'] = 'ZZZ'\n",
    "part = part[part.participates_as!='utro']\n",
    "part.rename(columns={'order_number':'orderNumber'}, inplace=True)\n",
    "print(f\"size part: {len(part)}\")\n",
    "part_init = part[['project_id', 'orderNumber', 'generalPic_old', 'pic', 'participates_as',\n",
    "    'role', 'legalName', 'part_total_cost', 'subv', 'subv_net',\n",
    "    'partner_status', 'countryCode', 'legalEntityTypeCode', 'isSme',\n",
    "    'nutsCode', 'stage', 'shortName', 'requestedGrant', 'budget', 'url', 'generalPic']]\n",
    "print(f\"size part_init with major cols: {len(part_init)}\")\n",
    "\n",
    "\n",
    "part_init=(part_init.merge(country_h20[['iso2', 'iso3', 'parent_iso3']], how='left', left_on='countryCode', right_on='iso2')\n",
    ".rename(columns={'iso3':'country_code_mapping', 'parent_iso3':'country_code'})\n",
    ".drop(columns='iso2'))\n",
    "\n",
    "if any(part_init[part_init.country_code_mapping.isnull()].countryCode.unique()):\n",
    "    print(part_init[part_init.country_code_mapping.isnull()].countryCode.unique())\n",
    "\n",
    "##status\n",
    "_proj = _proj.merge(status, how='inner', on='project_id')\n",
    "_proj.loc[_proj.stage=='evaluated', 'status_code'] = _proj.stat_code\n",
    "_proj.drop(columns=['stat_code'], inplace=True)\n",
    "\n",
    "l=['RIA','IA','CSA']\n",
    "tmp=_proj.loc[(~_proj.action_id.isin(['MSCA','ERC'])&(~_proj.action_2_id.isnull())&(_proj.action_id!='SME')),\n",
    "['action_2_id']].drop_duplicates()\n",
    "tmp['action_code'] = tmp['action_2_id'].str.extract(\"(\" + \"|\".join(l) +\")\", expand=False)\n",
    "_proj = _proj.merge(tmp, how='left', on='action_2_id')\n",
    "_proj.loc[_proj.action_code.isnull(), 'action_code'] = _proj.action_id\n",
    "\n",
    "\n",
    "def h20_topics(df, act, actions, destination, pilier_fr, thema):\n",
    "\n",
    "    proj = (_proj.rename(columns={'pilier':'pilier_name_en', 'topicCode':'topic_code','topicDescription':'topic_name',\n",
    "                                    'action_2_id':'action_code2', 'action_2_name':'action_name2', \n",
    "                                    'action_3_id':'action_code3', 'action_3_name':'action_name3'})\n",
    "        .drop(columns=['action_name'])\n",
    "        .merge(pilier_fr, how='left', on='pilier_name_en')\n",
    "        .merge(act, how='left', on='action_code'))\n",
    "\n",
    "    #euratom\n",
    "    proj.loc[proj.pilier_name_fr=='Euratom', 'pilier_name_en'] = 'Euratom'\n",
    "    proj.loc[(proj.pilier_name_fr=='Euratom')&(proj.topic_code.str.contains('NFRP')), 'programme_code'] = 'NFRP'\n",
    "    proj.loc[(proj.pilier_name_fr=='Euratom')&(proj.programme_code=='NFRP'), 'programme_name_en'] = 'Nuclear fission and radiation protection'\n",
    "    proj.loc[proj.call_id=='EURATOM-Adhoc-2014-20', 'programme_code'] = 'Fusion'\n",
    "    proj.loc[proj.call_id=='EURATOM-Adhoc-2014-20', 'programme_name_en'] = 'Fusion Energy'\n",
    "    proj.loc[(proj.pilier_name_fr=='Euratom')&(proj.call_id!='EURATOM-Adhoc-2014-20')&(proj.programme_code!='NFRP'), 'programme_code'] = 'Euratom-other'\n",
    "    proj.loc[(proj.pilier_name_fr=='Euratom')&(proj.call_id!='EURATOM-Adhoc-2014-20')&(proj.programme_code!='NFRP'), 'programme_name_en'] = 'Euratom other actions'\n",
    "\n",
    "    euratom = pd.read_csv('data_files/euratom_thema_all_FP.csv', sep=';', na_values='')\n",
    "    proj = proj.merge(euratom[['topic_area', 'thema_code', 'thema_name_en']], how='left', left_on='topic_code', right_on='topic_area', suffixes=['', '_t'])\n",
    "    proj.loc[(~proj.thema_code_t.isnull()), 'thema_code'] = proj.loc[(~proj.thema_code_t.isnull()), 'thema_code_t']\n",
    "    proj.loc[(~proj.thema_name_en_t.isnull()), 'thema_name_en'] = proj.loc[(~proj.thema_name_en_t.isnull()), 'thema_name_en_t']\n",
    "    # proj = proj.filter(regex=r'.*(?<!_t)$').drop(columns='topic_area')\n",
    "\n",
    "    # JU-JTI\n",
    "    proj.loc[proj.action_code=='Art185', 'destination_code'] = proj.loc[proj.action_code=='Art185'].thema_code\n",
    "\n",
    "    proj.loc[proj.thema_code.str.contains('JU', na=False), 'destination_code'] = proj.thema_code.str.replace('JU','').str.strip()\n",
    "    proj.loc[(proj.destination_code=='Eurostars2'), 'destination_next_fp'] = \"INNOVSMES\"\n",
    "\n",
    "    proj.loc[(proj.call_id.str.contains('PPP',na=False))|(proj.call_id.str.contains('JTI',na=False))|(proj.topic_code.str.contains('JTI',na=False)), 'destination_code'] = proj['action_code2'].str.split('-').str[0]\n",
    "    proj.loc[proj.call_id.str.contains('JTI',na=False)&(proj.action_code2.isnull()), 'destination_code'] = proj['call_id'].str.split('-').str[2]\n",
    "    proj.loc[(proj.thema_code=='CS2'), 'destination_code'] = 'CS2'\n",
    "    proj.loc[(proj.destination_code.str.contains('BBI', na=False)), 'destination_next_fp'] = 'CBE'\n",
    "    proj.loc[(proj.thema_code=='BBI'), 'thema_code'] = np.nan\n",
    "    proj.loc[(proj.destination_code=='EuroHPC'), 'destination_next_fp'] = 'EUROHPC'\n",
    "    proj.loc[(proj.thema_code=='ECSEL'), 'destination_code'] = 'ECSEL'\n",
    "    proj.loc[(proj.destination_code=='ECSEL'), 'destination_next_fp'] = 'CHIPS'\n",
    "    proj.loc[(proj.destination_code=='CS2'), 'destination_next_fp'] = 'CLEAN-AVIATION'\n",
    "    proj.loc[(proj.destination_code=='FCH2'), 'destination_next_fp'] = 'CLEANH2'\n",
    "    proj.loc[(proj.destination_code=='IMI2'), 'destination_next_fp'] = 'IHI'\n",
    "    proj.loc[(proj.destination_code=='Shift2Rail'), 'destination_next_fp'] = \"EU-RAIL\"\n",
    "    proj.loc[(~proj.destination_code.isnull())&(proj.destination_next_fp.isnull()), 'destination_next_fp'] = proj.loc[(~proj.destination_code.isnull())&(proj.destination_next_fp.isnull())].destination_code\n",
    "    l=['KDT', 'CBE','EUROHPC', 'CLEAN-AVIATION', 'CLEANH2', 'IHI', 'CHIPS', \"EU-RAIL\"]\n",
    "    proj.loc[(~proj.destination_next_fp.isnull()), 'thema_code'] = 'JU-JTI'\n",
    "\n",
    "    # MSCA / ERC\n",
    "    for col in ['thema_code','programme_next_fp']:\n",
    "        proj.loc[proj.programme_code=='MSCA', col] = 'MSCA'\n",
    "        proj.loc[proj.programme_code=='ERC', col] = 'ERC'\n",
    "\n",
    "    # ### ajustement MSCA\n",
    "    msca_correspondence = pd.read_table('data_files/msca_correspondence.csv', sep=\";\")\n",
    "\n",
    "    msca_correspondence = msca_correspondence[msca_correspondence.framework=='H2020'].drop(columns='framework')\n",
    "    proj.loc[(proj.thema_code=='MSCA')&(proj.action_code3.isnull()), 'action_code3'] = proj.action_code2\n",
    "\n",
    "    proj.loc[(proj.thema_code=='MSCA'), 'destination_code'] = proj.loc[(proj.thema_code=='MSCA')].action_code3.str.replace('MSCA-', '').str.strip()\n",
    "    proj.loc[(proj.thema_code=='MSCA'), 'destination_name_en'] = proj.loc[(proj.thema_code=='MSCA')].action_name2.str.replace('Marie Skłodowska-Curie', '').str.strip() +'-'+ proj.loc[(proj.thema_code=='MSCA')].action_name3.dropna()\n",
    "\n",
    "    # m = proj.loc[(proj.action_code=='MSCA'), ['action_code3']].drop_duplicates()\n",
    "    proj = proj.merge(msca_correspondence, how='left', left_on='action_code3', right_on='old')\n",
    "    proj.loc[~proj.new.isnull(), 'destination_next_fp'] = proj.loc[~proj.new.isnull()].new\n",
    "    proj.loc[(proj.thema_code=='MSCA')&(proj.destination_next_fp.isnull()),'destination_next_fp'] = 'MSCA-OTHER'\n",
    "    # m = m.merge(actions[['destination_detail_code','destination_detail_name_en']].drop_duplicates(), how='left', on='destination_detail_code')\n",
    "    proj.loc[proj.destination_code=='NIGHT', 'destination_name_en'] = \"European researchers' Night\"\n",
    "    proj.loc[proj.destination_code=='RISE', 'destination_name_en'] = \"Research and innovation staff exchange\"\n",
    "\n",
    "    # proj.loc[proj.programme_code=='MSCA', 'programme_name_en'] = 'Marie Skłodowska-Curie Actions (MSCA)'\n",
    "\n",
    "\n",
    "    ### ajustement ERC\n",
    "    proj.loc[proj.thema_code=='ERC', 'destination_code'] = proj.loc[proj.thema_code=='ERC'].action_code2.str.split('-').str[1]\n",
    "    proj.loc[proj.destination_code=='POC-LS', 'destination_code'] = \"POC\"\n",
    "    proj.loc[(proj.thema_code=='ERC')&(proj.destination_code.isnull()), 'destination_code'] = 'ERC-OTHER'\n",
    "    proj.loc[(proj.action_code=='ERC'), 'action_code2'] = np.nan\n",
    "    proj.loc[(proj.action_code=='ERC'), 'action_name2'] = np.nan\n",
    "\n",
    "    # FET\n",
    "    proj.loc[proj.programme_code=='FET', 'thema_code'] = 'PATHFINDER'\n",
    "    proj.loc[(proj.programme_code=='FET')&(proj.action_code=='SGA'), 'destination_code'] = proj.loc[(proj.programme_code=='FET')&(proj.action_code=='SGA')].topic_code.str.split('-').str[2].str.upper()\n",
    "    proj.loc[(proj.programme_code=='FET')&(proj.destination_code.isnull()), 'destination_code'] = proj.loc[(proj.programme_code=='FET')&(proj.destination_code.isnull())].topic_code.str.split('-').str[0].str.upper()\n",
    "    proj.loc[(proj.programme_code=='FET')&(proj.topic_code.str.contains('BAT-')), 'destination_code'] = 'BATTERY'\n",
    "\n",
    "    proj.loc[proj.programme_code=='FET', 'thema_name_en'] = np.nan\n",
    "\n",
    "    proj.loc[(proj.call_id.str.contains(\"FETOPEN-2018-2019-2020\"))|(proj.topic_code.str.contains(\"FETPROACT-EIC\")), 'destination_next_fp'] = 'ACCELERATOR'\n",
    "\n",
    "    # SMEInst\n",
    "    proj.loc[(proj.topic_code.str.contains('EIC-SMEInst')), 'destination_next_fp'] = 'ACCELERATOR'\n",
    "\n",
    "    proj.loc[proj.destination_next_fp=='ACCELERATOR', 'programme_next_fp'] = 'EIC'\n",
    "\n",
    "    proj.loc[(proj.programme_code=='SME')&(proj.thema_code!='JU-JTI'), 'thema_name_en'] = np.nan\n",
    "\n",
    "    # INFRA\n",
    "    proj.loc[proj.programme_code=='INFRA', 'thema_code'] = proj.programme_code\n",
    "    proj.loc[proj.programme_code=='INFRA', 'destination_code'] = proj.loc[proj.programme_code=='INFRA'].topic_code.str.split('-').str[0]\n",
    "    proj.loc[(proj.programme_code=='INFRA')&(proj.destination_code=='LC'), 'destination_code'] = 'GREEN-DEAL'\n",
    "    proj.loc[proj.destination_code.isin(['LC','IBA','SGA']), 'destination_code'] = np.nan\n",
    "\n",
    "    # EIT\n",
    "    proj.loc[proj.action_code=='KICS', 'pilier_name_en'] = 'Innovative Europe'\n",
    "    proj.loc[proj.action_code=='KICS', 'programme_code'] = 'EIT'\n",
    "    proj.loc[proj.action_code=='KICS', 'programme_name_en'] = 'The European Institute of Innovation and Technology (EIT)'\n",
    "\n",
    "    # # WIDENING COST\n",
    "    proj.loc[proj.programme_code.str.contains('TWINING|WIDESPREAD|NCPNET', na=False), 'thema_code'] = 'ACCESS'\n",
    "    proj.loc[proj.programme_code.str.contains('ERA', na=False), 'thema_code'] = 'TALENTS'\n",
    "    proj.loc[proj.programme_code.str.contains('INTNET', na=False), 'thema_code'] = 'COST'\n",
    "    proj.loc[(proj.pilier_name_en=='Spreading excellence and widening participation')&(proj.programme_code!='ERA'), 'programme_code'] = 'Widening'\n",
    "    proj.loc[proj.programme_code=='Widening', 'programme_name_en'] = 'Widening participation and spreading excellence'\n",
    "\n",
    "    proj.loc[(proj.programme_code=='Widening')&(proj.thema_code.isnull()), 'thema_code'] = 'WIDENING-OTHER'\n",
    "\n",
    "    proj.loc[(proj.programme_code.isin(['BIOTECH','ADVMANU','ADVMAT', 'NMP']))&(proj.thema_code.isnull()), 'thema_code'] = proj.loc[(proj.programme_code.isin(['BIOTECH','ADVMANU','ADVMAT', 'NMP']))&(proj.thema_code.isnull())].programme_code\n",
    "    proj.loc[(proj.programme_code.isin(['BIOTECH','ADVMANU','ADVMAT', 'NMP']))&(proj.thema_name_en.isnull()), 'thema_name_en'] = proj.loc[(proj.programme_code.isin(['BIOTECH','ADVMANU','ADVMAT', 'NMP']))&(proj.thema_name_en.isnull())].programme_name_en\n",
    "    proj.loc[(proj.programme_code.isin(['BIOTECH','ADVMANU','ADVMAT', 'NMP'])), 'programme_code'] = 'NMBP'\n",
    "    proj.loc[proj.programme_code=='NMBP', 'programme_name_en'] = 'Nanotechnologies, Advanced Materials, Advanced Manufacturing and Processing, and Biotechnology'\n",
    "\n",
    "    dest = destination[['destination_code', 'destination_name_en']]\n",
    "    proj = proj.merge(dest, how='left', on='destination_code', suffixes=('', '_x'))\n",
    "    proj.loc[proj.destination_name_en.isnull(), 'destination_name_en'] = proj.loc[proj.destination_name_en.isnull()].destination_name_en_x\n",
    "\n",
    "    proj = proj.merge(thema.loc[~thema.dest_h20.isnull(), ['thema_code', 'dest_h20']], how='left', left_on='thema_code', right_on='dest_h20', suffixes=['','_x'])\n",
    "    proj.loc[~proj.thema_code_x.isnull(), 'thema_code'] = proj.thema_code_x\n",
    "    proj.drop(columns=['thema_code_x','dest_h20'], inplace=True)\n",
    "    proj = proj.merge(thema, how='left', on='thema_code', suffixes=['','_x'])\n",
    "    proj.loc[~proj.thema_name_en_x.isnull(), 'thema_name_en'] = proj.thema_name_en_x\n",
    "\n",
    "    proj.drop(columns=['thema_name_en_x','dest_h20', 'destination_name_en_x', 'thema_code_t', 'thema_name_en_t', 'new', 'old'], inplace=True)\n",
    "    return proj\n",
    "\n",
    "def euro_partnerships(proj):\n",
    "    from step5_frameworks.functions_shared import ju_jti_parterships, eranet_partnerships\n",
    "    # proj.loc[proj.action_code=='Art185', 'euro_partnerships_type'] = 'Art-185'\n",
    "    # proj.loc[(proj.thema_code=='JU-JTI')&(proj.euro_partnerships_type.isnull()), 'euro_partnerships_type'] = 'Art-187'\n",
    "    # proj.loc[proj.thema_code=='JU-JTI', 'euro_partnerships_type_next_fp'] = 'JU-JTI'\n",
    "    proj=ju_jti_parterships(proj, 'H20')\n",
    "    proj.loc[proj.programme_code=='EIT', 'euro_partnerships_type'] = 'EIT KICs'\n",
    "    proj.loc[proj.programme_code=='EIT', 'euro_partnerships_type_next_fp'] = 'EIT KICs'\n",
    "\n",
    "    # proj.loc[proj.action_code=='ERA-NET-Cofund', 'euro_partnerships_type'] = 'ERA-NET-COFUND'\n",
    "    # proj.loc[proj.action_code=='ERA-NET-Cofund', 'euro_partnerships_type_next_fp'] = 'co-funded'\n",
    "    proj=eranet_partnerships(proj, 'H20')\n",
    "    proj.loc[proj.acronym=='CoBioTech', 'euro_ps_name'] = 'ERA CoBioTech'\n",
    "    \n",
    "    proj.loc[(proj.topic_code.isin(['NFRP-2018-6', 'SC1-PM-05-2016', 'EURATOM', 'NFRP-07-2015']))&(proj.action_code=='COFUND'), 'euro_partnerships_type'] = 'EJP-COFUND'\n",
    "    proj.loc[(proj.action_code=='COFUND')&(proj.acronym.str.contains('EJP')), 'euro_partnerships_type'] = 'EJP-COFUND'\n",
    "    proj.loc[(proj.action_code=='COFUND')&(proj.acronym.str.contains('EJP')), 'euro_partnerships_type_next_fp'] = 'co-funded'\n",
    "\n",
    "    proj.loc[proj.topic_name.str.contains('PPP'), 'euro_partnerships_type'] = 'cPPP'\n",
    "    proj.loc[(proj.topic_code.str.contains('GV-', regex=True, na=False))&(proj.programme_code=='TPT')&(proj.euro_partnerships_type.isnull()), 'euro_partnerships_type'] = 'cPPP'\n",
    "    proj.loc[(proj.topic_name.str.contains('photonics', case=False, na=False))&(proj.programme_code=='ICT')&(proj.euro_partnerships_type.isnull()), 'euro_partnerships_type'] = 'cPPP'\n",
    "    robotics=[\"ict-27-2017\", \"ict-28-2017\", \"ict-25-2016\", \"ict-26-2016\", \"ict-24-2015\", \"ict-23-2014\"]\n",
    "    for i in robotics:        \n",
    "        proj.loc[(proj.programme_code=='ICT')&(proj.topic_code.str.contains(r\"^\"+i, case=False, regex=True)), 'euro_partnerships_type'] = 'cPPP'\n",
    "\n",
    "    proj.loc[proj.euro_partnerships_type.str.contains('cPPP', na=False), 'euro_partnerships_type_next_fp'] = 'co-programmed'\n",
    "\n",
    "    proj.loc[proj.euro_partnerships_type=='EIT KICs', 'euro_ps_name'] = proj.loc[proj.euro_partnerships_type=='EIT KICs'].thema_name_en\n",
    "    # proj.loc[proj.euro_partnerships_type=='ERA-NET-COFUND', 'euro_ps_name'] = proj.loc[proj.euro_partnerships_type=='ERA-NET-COFUND'].acronym\n",
    "    \n",
    "    # proj.loc[proj.euro_partnerships_type_next_fp=='JU-JTI', 'euro_ps_name'] = proj.loc[proj.euro_partnerships_type_next_fp=='JU-JTI'].destination_code\n",
    "    proj.loc[(proj.euro_partnerships_type_next_fp=='co-funded')&(proj.euro_ps_name.isnull())&(proj.destination_code.isnull()), 'euro_ps_name'] = proj.loc[(proj.euro_partnerships_type_next_fp=='co-funded')&(proj.euro_ps_name.isnull())&(proj.destination_code.isnull())].acronym\n",
    "    proj.loc[(proj.euro_partnerships_type_next_fp=='co-funded')&(proj.euro_ps_name.isnull())&(~proj.destination_code.isnull()), 'euro_ps_name'] = proj.loc[(proj.euro_partnerships_type_next_fp=='co-funded')&(proj.euro_ps_name.isnull())&(~proj.destination_code.isnull())].destination_code\n",
    "    proj.loc[(proj.euro_partnerships_type=='cPPP')&(proj.call_id.str.contains('EEB|SPIRE|FOF|EE', regex=True, case=False, na=False)), 'euro_ps_name'] = proj.loc[(proj.euro_partnerships_type=='cPPP')&(proj.call_id.str.contains('EEB|SPIRE|FOF|EE', regex=True, case=False, na=False))].call_id.str.split('-').str[1]\n",
    "    proj.loc[(proj.euro_partnerships_type=='cPPP')&(proj.euro_ps_name.isnull()), 'euro_ps_name'] = proj.loc[(proj.euro_partnerships_type=='cPPP')&(proj.euro_ps_name.isnull())].topic_name.str.extract(r\"^(.+ PPP)\", expand=False)\n",
    "    proj.loc[(proj.euro_partnerships_type=='cPPP')&(proj.euro_ps_name=='EE'), 'euro_ps_name'] = proj.loc[(proj.euro_partnerships_type=='cPPP')&(proj.euro_ps_name=='EE')].topic_name.str.extract(r\"^(?:\\()(EeB|SPIRE|FoF)\", expand=False)\n",
    "    proj.loc[(proj.euro_partnerships_type=='cPPP')&(proj.topic_code.str.contains('GV', na=False)), 'euro_ps_name'] = 'EGVI'\n",
    "    proj.loc[(proj.euro_partnerships_type=='cPPP')&(proj.euro_ps_name=='Big data PPP'), 'euro_ps_name'] = 'BDVA'\n",
    "    proj.loc[(proj.euro_partnerships_type=='cPPP')&(proj.topic_name.str.contains('photonics', case=False, na=False))&(proj.programme_code=='ICT'), 'euro_ps_name'] = 'Photonics'\n",
    "    proj.loc[(proj.euro_partnerships_type=='cPPP')&(proj.programme_code=='ICT')&(proj.euro_ps_name.isnull()), 'euro_ps_name'] = 'Robotics SPARC'\n",
    "    return proj.assign(euro_partnerships_flag=np.where(proj.euro_partnerships_type.isnull(), False, True)) \n",
    "\n",
    "\n",
    "def cPPP_destination_name(proj):\n",
    "    mask=(proj.euro_partnerships_type=='cPPP')\n",
    "    proj.loc[mask&(proj.euro_ps_name=='EGVI'), 'destination_name'] = 'European Green Vehicles Initiative'\n",
    "    proj.loc[mask&(proj.euro_ps_name=='SPIRE'), 'destination_name'] = 'Sustainable Process Industry'\n",
    "    proj.loc[mask&(proj.euro_ps_name=='FoF'), 'destination_name'] = 'Factories of the future'\n",
    "    proj.loc[mask&(proj.euro_ps_name=='EeB'), 'destination_name'] = 'Energy-Efficient Buildings'\n",
    "    proj.loc[mask&(proj.euro_ps_name=='5G PPP'), 'destination_name'] = 'Advanced 5G Network for the future'\n",
    "    proj.loc[mask&(proj.euro_ps_name=='BDVA'), 'destination_name'] = 'Big Data Value Association '\n",
    "    proj.loc[mask&(proj.euro_ps_name=='Cybersecurity PPP'), 'destination_name'] = 'Connected digital single market'\n",
    "    proj.loc[mask&(proj.euro_ps_name=='Photonics'), 'destination_name'] = 'Photonics21 Association'\n",
    "    proj.loc[mask&(proj.euro_ps_name=='Robotics SPARC'), 'destination_name'] = 'Robotics SPARC'\n",
    "    return proj\n",
    "\n",
    "def proj_cleaning(proj):\n",
    "    print(\"## PROJ cleaning\")\n",
    "    from functions_shared import website_to_clean\n",
    "    for i in ['title','abstract', 'free_keywords', 'eic_panels', 'url_project']:\n",
    "        proj[i]=proj[i].str.replace('\\\\n|\\\\t|\\\\r|\\\\s+', ' ', regex=True).str.strip()\n",
    "        \n",
    "    kw = proj[['project_id','stage','free_keywords']].drop_duplicates()\n",
    "    kw = kw.assign(free_keywords = kw.free_keywords.str.split(';|,')).explode('free_keywords')\n",
    "    kw['free_keywords'] = kw.free_keywords.str.replace('\\\\.+', '', regex=True)\n",
    "    kw = kw.loc[kw.free_keywords.str.len()>3].drop_duplicates()\n",
    "    kw.free_keywords = kw.free_keywords.groupby(level=0).apply(lambda x: '|'.join(x.str.strip().unique()))\n",
    "    kw = kw.drop_duplicates()\n",
    "\n",
    "    proj = proj.drop(columns='free_keywords').merge(kw, how='left', on=['project_id','stage']).drop_duplicates()    \n",
    "        \n",
    "    proj.loc[proj.url_project.str.contains('project/rcn', na=False), 'url_project']=np.nan\n",
    "\n",
    "    proj.mask(proj=='', inplace=True)  \n",
    "    for i,row in proj.iterrows():\n",
    "        if row.loc['url_project'] is not None:\n",
    "            proj.at[i, 'project_webpage'] = website_to_clean(row['url_project'])\n",
    "\n",
    "    proj.mask(proj=='', inplace=True)  \n",
    "\n",
    "    for d in ['call_deadline', 'signature_date',  'start_date', 'end_date', 'submission_date', 'ecorda_date']:\n",
    "        proj[d] = proj[d].astype('datetime64[ns]')\n",
    "\n",
    "    proj['proposal_expected_number'] = proj['proposal_expected_number'].astype('float')\n",
    "    return proj\n",
    "\n",
    "\n",
    "def entities_cleaning(df, country_h20, p):\n",
    "    print(\"## ENTITIES cleaning\")\n",
    "    from functions_shared import gps_col, num_to_string\n",
    "    df = pd.DataFrame(df)\n",
    "    df = gps_col(df)\n",
    "    df = df.loc[~df.generalPic.isnull()]\n",
    "    \n",
    "    df = (df.merge(country_h20[['iso2', 'iso3', 'parent_iso3']], how='left', left_on='countryCode', right_on='iso2')\n",
    "        .drop(columns='iso2')\n",
    "        .rename(columns={'parent_iso3':'country_code', 'iso3': 'country_code_mapping'}))\n",
    "    print(f\"parent_iso missing : {df[df.country_code.isnull()].countryCode.unique()}\")\n",
    "    df.loc[df.country_code.isnull(), 'country_code'] = df.loc[df.country_code.isnull()].country_code_mapping \n",
    "\n",
    "    c = ['pic', 'generalPic']\n",
    "    df[c] = df[c].map(num_to_string)\n",
    "    print(f\"- size entities {len(df)}\")\n",
    "\n",
    "    if len(df[df.generalState.isnull()])>0:\n",
    "        print(\"- entities source generalState -> new state (processing into entities_single)\")\n",
    "    else:\n",
    "        print(\"- ok entities source generalState not null\")\n",
    "\n",
    "    lien_genCalcPic = p[['generalPic_old', 'pic']].drop_duplicates()\n",
    "    print(f\"size part without country: {len(p[['generalPic_old', 'pic']].drop_duplicates())}\\nsize part with country: {len(p[['generalPic_old', 'pic', 'countryCode']].drop_duplicates())}\")\n",
    "    df = lien_genCalcPic.merge(df, how='inner', left_on=['generalPic_old','pic'], right_on=['generalPic','pic']).drop_duplicates()\n",
    "    return df\n",
    "\n",
    "proj = h20_topics(_proj, act, actions, destination, pilier_fr, thema)\n",
    "proj = euro_partnerships(proj)\n",
    "proj = cPPP_destination_name(proj)\n",
    "proj = proj_cleaning(proj)\n",
    "entities = entities_cleaning(entities, country_h20, part_init)\n",
    "\n",
    "def ref_select(FP):\n",
    "    ref_source = ref_source_load('ref')\n",
    "    # traitement ref select le FP, id non null ou/et ZONAGE non null\n",
    "    ref, genPic_to_new = ref_source_2d_select(ref_source, FP)\n",
    "    ror = pd.read_pickle(f\"{PATH_REF}ror_df.pkl\")\n",
    "    paysage = pd.read_pickle(f\"{PATH_REF}paysage_df.pkl\")\n",
    "    sirene = pd.read_pickle(f\"{PATH_REF}sirene_df.pkl\")\n",
    "    ### si besoin de charger groupe\n",
    "    groupe = pd.read_pickle(f\"{PATH_REF}H20_groupe.pkl\")\n",
    "    return ref, genPic_to_new, ror, paysage, sirene, groupe\n",
    "ref, genPic_to_new, ror, paysage, sirene, groupe = ref_select('H20')\n",
    "\n",
    "print(f\"- si ++id pour un generalPic: {ref[ref.id.str.contains(';', na=False)]}\")\n",
    "ref = (ref.merge(country_h20[['iso3', 'parent_iso3']], how='left', left_on='country_code_mapping', right_on='iso3')\n",
    "    .drop(columns='iso3')\n",
    "    .rename(columns={'parent_iso3':'country_code'}))\n",
    "print(f\"parent_iso missing : {ref[ref.country_code.isnull()].country_code_mapping.unique()}\")\n",
    "ref.loc[ref.country_code.isnull(), 'country_code'] = ref.loc[ref.country_code.isnull()].country_code_mapping \n",
    "\n",
    "\n",
    "########################################################################\n",
    "p=part_init[['generalPic', 'country_code_mapping', 'country_code']].drop_duplicates()\n",
    "print(f\"size de p: {len(p)}\")\n",
    "p = p.merge(ref, how='left', on=['generalPic', 'country_code_mapping', 'country_code'], indicator=True).drop_duplicates()\n",
    "print(f\"cols de p: {p.columns}\") #168 978\n",
    "\n",
    "# p1 pic+ccm commun\n",
    "p1 = p.loc[p['_merge']=='both'].drop(columns=['_merge'])\n",
    "print(f\"size p1 pic+cc: {len(p1)}\")# 62 928\n",
    "\n",
    "\n",
    "p2 = (p.loc[p['_merge']=='left_only'].drop(columns=['_merge', 'id', 'ZONAGE', 'id_secondaire'])\n",
    "    .merge(ref.drop(columns=['country_code_mapping']), \n",
    "            how='inner', left_on=['generalPic', 'country_code'], right_on=['generalPic', 'country_code']).drop_duplicates()\n",
    "    )\n",
    "print(f\"size p2 pic cc_parent: {len(p2)}\")\n",
    "\n",
    "\n",
    "# acteurs sans identifiant dont le pic à plusieurs pays ou le pic certaines participations ont un identifiant et pas d'autres \n",
    "p3 = (p.loc[p['_merge']=='left_only'].drop(columns=['_merge', 'country_code_mapping', 'id', 'ZONAGE'])\n",
    "    .merge(ref, how='inner', on=['generalPic']).drop_duplicates())\n",
    "if not p3.empty:\n",
    "    print(f\"A faire si possible, vérifier pourquoi des participations avec pic identiques ont un id ou pas nb pic: {len(p3.generalPic.unique())}\")\n",
    "\n",
    "p = pd.concat([p1,p2], ignore_index=True).drop_duplicates()\n",
    "print(f\"size de new p: {len(p)}, cols: {p.columns}\")\n",
    "\n",
    "part1 = part_init.merge(p, how='left', on=['generalPic', 'country_code_mapping', 'country_code'])\n",
    "print(f\"size part1: {len(part1)}, part: {len(part_init)}\")\n",
    "\n",
    "# gestion code nuts\n",
    "part1.loc[(part1.nutsCode.str.len()>2), 'nuts_code'] = part1.nutsCode\n",
    "part1 = (part1.merge(nuts, how='left', on='nuts_code')\n",
    "            .drop_duplicates()\n",
    "            .rename(columns={'nuts_code':'participation_nuts'}))\n",
    "print(f\"size participation after add nuts: {len(part1)}, sans nuts name: {len(part1.loc[(~part1.participation_nuts.isnull())&(part1.region_1_name.isnull())])}\")\n",
    "\n",
    "\n",
    "### entities\n",
    "entities_tmp = part1.loc[~part1.id.isnull(), ['generalPic','id','country_code_mapping']].drop_duplicates()\n",
    "print(f\"- size entities {len(entities_tmp)}\")\n",
    "if any(entities_tmp.id.str.contains(';')):\n",
    "    entities_tmp = entities_tmp.assign(id_extend=entities_tmp.id.str.split(';')).explode('id_extend')\n",
    "    ent_size_to_keep = len(entities_tmp)\n",
    "    print(f\"1- size ent si multi id -> ent_size_to_keep = {ent_size_to_keep}\\n{entities_tmp.columns}\")\n",
    "\n",
    "entities_tmp = merge_ror(entities_tmp, ror)\n",
    "print(f\"size entities_tmp after add ror_info: {len(entities_tmp)}, entities_size_to_keep: {ent_size_to_keep}\")\n",
    "\n",
    "# PAYSAGE\n",
    "### si besoin de charger paysage pickle\n",
    "paysage_category = pd.read_pickle(f\"{PATH_SOURCE}paysage_category.pkl\")\n",
    "cat_filter = category_paysage(paysage_category)\n",
    "entities_tmp = merge_paysage(entities_tmp, paysage, cat_filter)\n",
    "\n",
    "# SIRENE\n",
    "### si besoin de charger paysage pickle\n",
    "entities_tmp = merge_sirene(entities_tmp, sirene)\n",
    "entities_tmp['nb']=entities_tmp.groupby(['generalPic', 'id_extend', 'country_code_mapping'])['entities_id'].transform('count')\n",
    "if any(entities_tmp['nb']>1):\n",
    "    print(f\"doublons: {entities_tmp.loc[entities_tmp['nb']>1, ['generalPic', 'id_extend', 'country_code_mapping', 'entities_id', 'nb']]}\")\n",
    "    entities_tmp=entities_tmp.loc[~entities_tmp.entities_id.isin(['889664413', '808994164'])]\n",
    "\n",
    "entities_tmp.loc[(~entities_tmp.id.isnull())&(entities_tmp.entities_id.isnull()), 'entities_id'] = entities_tmp.id\n",
    "entities_tmp['siren']=entities_tmp.loc[entities_tmp.entities_id.str.contains('^[0-9]{9}$|^[0-9]{14}$', na=False)].entities_id.str[:9]\n",
    "entities_tmp.loc[entities_tmp.siren.isnull(), 'siren']=entities_tmp.paysage_siren\n",
    "\n",
    "#groupe entreprises\n",
    "# recuperation tous les siren pour lien avec groupe -> creation var SIREN \n",
    "entities_tmp.loc[~entities_tmp.siren.isnull(), \"siren\"] = entities_tmp.loc[~entities_tmp.siren.isnull(), \"siren\"].str.split().apply(set).str.join(\";\")\n",
    "\n",
    "if any(entities_tmp.siren.str.contains(';', na=False)):\n",
    "    print(f\"ATTENTION faire code pour traiter deux siren différents -> ce qui serait bizarre qu'il y ait 2 siren\\n{entities_tmp[entities_tmp.siren.str.contains(';', na=False)]}\")\n",
    "# else:\n",
    "print(f\"taille de entities_tmp avant groupe:{len(entities_tmp)}\")\n",
    "entities_tmp=entities_tmp.merge(groupe, how='left', on='siren')\n",
    "\n",
    "print(f\"taille de entities_tmp après groupe {len(entities_tmp)}\")\n",
    "entities_tmp = entities_tmp.merge(get_source_ID(entities_tmp, 'entities_id'), how='left', on='entities_id')\n",
    "\n",
    "# traitement catégorie\n",
    "entities_tmp = category_woven(entities_tmp, sirene)\n",
    "entities_tmp = category_agreg(entities_tmp)\n",
    "entities_tmp = mires(entities_tmp)\n",
    "\n",
    "print(f\"size part1 avant: {len(part1)}\")\n",
    "part_tmp = part1.merge(genPic_to_new, how='left', on=['generalPic', 'country_code_mapping'])\n",
    "part_tmp = part_tmp.rename(columns={'generalPic':'pic_old', 'pic_new':'generalPic'})\n",
    "part_tmp.loc[part_tmp.generalPic.isnull(), 'generalPic'] = part_tmp.loc[part_tmp.generalPic.isnull(), 'pic_old']\n",
    "part_tmp = part_tmp.merge(entities_tmp.drop(columns='id'), how='left', on=['generalPic', 'country_code_mapping'])\n",
    "print(f\"size part1 -> part_tmp: {len(part_tmp)}\\n{part_tmp.columns}\")\n",
    "\n",
    "print(len(part_tmp[(part_tmp.entities_name.isnull())]))\n",
    "part2=part_tmp.loc[(part_tmp.entities_name.isnull()), ['generalPic','entities_id', 'country_code_mapping', 'source_id']]\n",
    "part2.loc[part2.entities_id.str.contains('-', na=False), 'pic_d'] = part2.loc[part2.entities_id.str.contains('-', na=False)].entities_id.str.split('-').str[0]\n",
    "part2.loc[part2.pic_d.isnull(), 'pic_d'] = part2.loc[part2.pic_d.isnull()].generalPic\n",
    "\n",
    "part2 = part2.drop_duplicates()\n",
    "print(f\"size part2: {len(part2)}, nb unique pic_d: {part2.pic_d.nunique()}\")\n",
    "part2 = (part2.merge(entities, how='inner', left_on='pic_d', right_on='generalPic')[\n",
    "            ['pic_d','entities_id','legalName', 'businessName', 'legalEntityTypeCode', 'generalState']]\n",
    "        .rename(columns={'businessName':'shortName'})\n",
    "        .drop_duplicates()\n",
    "        )\n",
    "\n",
    "gen_state=['VALIDATED', 'DECLARED', 'DEPRECATED', 'SLEEPING', 'SUSPENDED', 'BLOCKED']\n",
    "part2=part2.groupby(['pic_d']).apply(lambda x: x.sort_values('generalState', key=lambda col: pd.Categorical(col, categories=gen_state, ordered=True)), include_groups=True).reset_index(drop=True)\n",
    "part2=part2.groupby(['pic_d']).head(1).drop(columns='generalState')\n",
    "print(f\"size part2: {len(part2)}, nb unique pic_d: {part2.pic_d.nunique()}\")\n",
    "\n",
    "part3=(part_tmp.loc[(~part_tmp.generalPic.isin(part2.pic_d.unique()))&(part_tmp.entities_name.isnull())]\n",
    "    .sort_values(['generalPic','legalName', 'shortName'], ascending=False))\n",
    "print(part3.generalPic.nunique())\n",
    "\n",
    "part3=(part3.groupby(['generalPic', 'country_code_mapping'])\n",
    "    .first().reset_index()[['generalPic', 'country_code_mapping', 'legalName', 'shortName', 'legalEntityTypeCode']]\n",
    "    .reset_index(drop=True)\n",
    "    .drop_duplicates()\n",
    "    )\n",
    "print(part3.generalPic.nunique())\n",
    "\n",
    "part_tmp = part_tmp.merge(part2, how='left', left_on='generalPic', right_on='pic_d', suffixes=['', '_x'])\n",
    "part_tmp.loc[~part_tmp.legalName_x.isnull(), 'legalName'] = part_tmp.legalName_x\n",
    "part_tmp.loc[~part_tmp.shortName_x.isnull(), 'shortName'] = part_tmp.shortName_x\n",
    "part_tmp.loc[~part_tmp.legalEntityTypeCode_x.isnull(), 'legalEntityTypeCode'] = part_tmp.legalEntityTypeCode_x\n",
    "print(f\"size part_tmp after merge part2: {len(part_tmp)}\")\n",
    "\n",
    "part_tmp = part_tmp.merge(part3, how='left', on=['generalPic', 'country_code_mapping'], suffixes=['', '_y'])\n",
    "part_tmp.loc[~part_tmp.legalName_y.isnull(), 'legalName'] = part_tmp.legalName_y\n",
    "part_tmp.loc[~part_tmp.shortName_y.isnull(), 'shortName'] = part_tmp.shortName_y\n",
    "part_tmp.loc[~part_tmp.legalEntityTypeCode_y.isnull(), 'legalEntityTypeCode'] = part_tmp.legalEntityTypeCode_y\n",
    "part_tmp.drop(part_tmp.columns[part_tmp.columns.str.endswith(('_x','_y'))], axis=1, inplace=True)\n",
    "print(f\"size part_tmp after merge part2: {len(part_tmp)}\")\n",
    "\n",
    "liste=['legalName', 'shortName']\n",
    "for i in liste:\n",
    "    part_tmp[i] = part_tmp[i].apply(lambda x: x.capitalize().strip() if isinstance(x, str) else x)\n",
    "\n",
    "part_tmp.loc[part_tmp.entities_name.isnull(), 'entities_name'] = part_tmp.legalName\n",
    "part_tmp.loc[part_tmp.entities_acronym.isnull(), 'entities_acronym'] = part_tmp.shortName\n",
    "part_tmp.loc[part_tmp.entities_id.isnull(), 'entities_id'] = \"pic\"+part_tmp.generalPic.map(str)\n",
    "\n",
    "part_tmp.rename(columns={'legalName':'entities_name_source',\n",
    "                        'shortName':'entities_acronym_source'}, inplace=True)\n",
    "\n",
    "for i in ['entities_acronym', 'entities_name','entities_acronym_source', 'entities_name_source']:\n",
    "    part_tmp[i] = part_tmp[i].str.replace('\\\\n|\\\\t|\\\\r|\\\\s+', ' ', regex=True).str.strip()\n",
    "print(f\"size part_tmp after clean string: {len(part_tmp)}\")\n",
    "\n",
    "##########################################################\n",
    "\n",
    "# create calculated_fund and coordination_number\n",
    "part_tmp = (part_tmp\n",
    "            .assign(calculated_fund=np.where(part_tmp.stage=='successful', part_tmp['subv_net'], part_tmp['requestedGrant']), \n",
    "                    coordination_number=np.where(part_tmp.role=='coordinator', 1, 0)))\n",
    "\n",
    "\n",
    "#############################################################\n",
    "### ERC\n",
    "\n",
    "proj_erc=proj.loc[proj.action_id=='ERC', ['project_id', 'destination_code']].drop_duplicates()\n",
    "part_tmp=part_tmp.merge(proj_erc, how='left', on='project_id', indicator=True)\n",
    "part_tmp.loc[part_tmp._merge=='both', 'fund_ent_erc'] = part_tmp.loc[part_tmp._merge=='both'].calculated_fund\n",
    "\n",
    "# traitement erc ROLE\n",
    "part_tmp['erc_role'] = 'other'\n",
    "mask=(~part_tmp.destination_code.isnull())\n",
    "part_tmp.loc[mask&(part_tmp.stage=='evaluated')&(part_tmp.destination_code=='SyG')&((part_tmp.participates_as=='host')|(part_tmp.role=='coordinator')), 'erc_role'] = 'PI'\n",
    "part_tmp.loc[mask&(part_tmp.stage=='successful')&(part_tmp.destination_code=='SyG')&(part_tmp.participates_as=='beneficiary')&(pd.to_numeric(part_tmp.orderNumber, errors='coerce')<5.), 'erc_role'] = 'PI'\n",
    "part_tmp.loc[mask&(part_tmp.role=='coordinator')&(part_tmp.destination_code!='SyG'), 'erc_role'] = 'PI'\n",
    "part_tmp.loc[mask&(part_tmp.destination_code=='SyG')&(part_tmp.role=='coordinator'), 'role'] = 'CO-PI'\n",
    "part_tmp.loc[mask&(part_tmp.erc_role=='PI')&(part_tmp.role!='CO-PI'), 'role'] = 'PI'\n",
    "\n",
    "# traitement subv pour ERC\n",
    "    #calcul budget ERC\n",
    "pt = part_tmp.loc[(part_tmp._merge=='both')&(part_tmp.destination_code!='SyG')]\n",
    "pt['calculated_fund'] = np.where(pt.stage=='successful', pt['subv'], pt['requestedGrant'])\n",
    "spt = pt.loc[pt.stage=='evaluated', ['project_id', 'requestedGrant']].groupby(['project_id'])['requestedGrant'].sum().reset_index()\n",
    "pt = pt.merge(spt, how='left', on='project_id', suffixes=('', '_y'))\n",
    "pt.loc[pt.stage=='evaluated', 'calculated_fund'] = pt.loc[pt.stage=='evaluated'].requestedGrant_y\n",
    "pt.loc[pt.erc_role!='PI', 'calculated_fund'] = 0\n",
    "\n",
    "from functions_shared import work_csv\n",
    "work_csv(pt, 'pt_20')\n",
    "############################################\n",
    "\n",
    "part_tmp = pd.concat([part_tmp[~part_tmp.project_id.isin(pt.project_id.unique())], pt], ignore_index=True)\n",
    "print(f\"size part_tmp after concat with erc: {len(part_tmp)}\")\n",
    "\n",
    "part_tmp.drop(columns=['destination_code','requestedGrant_y', '_merge'], inplace=True)\n",
    "\n",
    "part_tmp = part_tmp.assign(number_involved=1)\n",
    "part_tmp['nb'] = part_tmp.id.str.split(';').str.len()\n",
    "for i in ['subv', 'subv_net', 'requestedGrant', 'calculated_fund', 'fund_ent_erc']:\n",
    "    part_tmp[i] = np.where(part_tmp['nb']>1, part_tmp[i]/part_tmp['nb'], part_tmp[i])\n",
    "print(f\"involved successful:{'{:,.1f}'.format(part_tmp.loc[(part_tmp.stage=='successful'), 'number_involved'].sum())}\\nsubv_net_laureat:{'{:,.1f}'.format(part_tmp.loc[(part_tmp.stage=='successful'), 'subv_net'].sum())}\\nsubv_laureat:{'{:,.1f}'.format(part_tmp.loc[(part_tmp.stage=='successful'), 'subv'].sum())}\\nsubv_prop:{'{:,.1f}'.format(part_tmp.loc[(part_tmp.stage=='evaluated'), 'requestedGrant'].sum())}\")\n",
    "\n",
    "print(proj.destination_code.unique())\n",
    "proj_no_coord = proj[(proj.thema_code.isin(['ACCELERATOR','COST']))|(proj.destination_code.str.startswith('SNLS|IF'))|(proj.action_code3.str.contains('SNLS', na=False))|(proj.thema_code=='ERC')].project_id.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14915245",
   "metadata": {},
   "outputs": [],
   "source": [
    "proj[(proj.destination_code.str.startswith('IF'))|(proj.action_code3.str.contains('SNLS', na=False))][['action_code3', 'destination_code']].drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec3011ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "cc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4e0251e",
   "metadata": {},
   "outputs": [],
   "source": [
    "instr = pd.read_csv('data_files/instru_nomenclature.csv', sep=';')\n",
    "act=pd.read_json(open(\"data_files/actions_name.json\", 'r', encoding='utf-8'))\n",
    "\n",
    "erc_correspondence = pd.read_json(open(\"data_files/ERC_correspondance.json\", 'r', encoding='utf-8'))\n",
    "thema = pd.read_json(open(\"data_files/thema.json\", 'r', encoding='utf-8'))\n",
    "destination = pd.read_json(open(\"data_files/destination.json\", 'r', encoding='utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5191bda2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def themes_cleaning(proj):\n",
    "    print(f\"## FP7 themes\\n- size proj before themes-action cleaning:{len(proj)}\")\n",
    "\n",
    "\n",
    "    # # ERC\n",
    "    erc_correspondence = pd.read_json(open(\"data_files/ERC_correspondance.json\", 'r', encoding='utf-8'))\n",
    "\n",
    "    proj.loc[proj.prog_abbr=='ERC', 'thema_code'] = 'ERC'\n",
    "    proj.loc[(proj.prog_abbr=='ERC')&(proj.instrument.str.contains('POC', na=False)), 'instrument'] = 'ERC-POC'\n",
    "    proj = (proj.merge(erc_correspondence, how='left', left_on=['instrument'], right_on=['old'])\n",
    "            .rename(columns={'new':'destination_code'})\n",
    "            .drop(columns='old'))\n",
    "    proj.loc[(proj.thema_code=='ERC')&(proj.destination_code.isnull()), 'destination_code'] = 'ERC-OTHER'\n",
    "\n",
    "    proj.loc[proj.thema_code=='ERC', 'programme_next_fp'] = 'ERC'\n",
    "\n",
    "\n",
    "    # # MSCA\n",
    "    df = proj.loc[(proj.prog_abbr=='PEOPLE')|(proj.instrument.str.startswith('MC-')), ['prog_abbr', 'call_id', 'instrument']].drop_duplicates()\n",
    "    df['inst'] = df['instrument'].str.replace('MC-', '')\n",
    "\n",
    "    df=thema_msca_cleaning(df, 'FP7')\n",
    "    proj = proj.merge(df, how='left', on=['prog_abbr', 'call_id', 'instrument'], suffixes=('', '_t'))\n",
    "\n",
    "    selected_columns = [col[:-2] for col in proj.columns if col.endswith('_t')]\n",
    "    for i in selected_columns:\n",
    "        proj.loc[~proj[f\"{i}_t\"].isnull(), i] = proj.loc[~proj[f\"{i}_t\"].isnull()][f\"{i}_t\"]\n",
    "    proj = proj.filter(regex=r'.*(?<!_t)$')\n",
    "    proj.loc[proj.thema_code=='MSCA', 'programme_next_fp'] = 'MSCA'\n",
    "    print(f\"- size proj after msca: {proj.loc[proj.stage=='successful'].project_id.nunique()}, nb project_id: {len(proj.loc[proj.stage=='successful'])}\")\n",
    "\n",
    "    # #euratom\n",
    "    df = proj.loc[proj.pilier.isin(['EURATOM']), ['prog_abbr', 'area_abbr']].assign(topic_area=proj.area_abbr)\n",
    "    df = thema_euratom_cleaning(df, 'FP7')\n",
    "    proj = proj.merge(df, how='left', on=['prog_abbr', 'area_abbr'], suffixes=('', '_t'))\n",
    "\n",
    "    selected_columns = [col[:-2] for col in proj.columns if col.endswith('_t')]\n",
    "    for i in selected_columns:\n",
    "        proj.loc[~proj[f\"{i}_t\"].isnull(), i] = proj.loc[~proj[f\"{i}_t\"].isnull()][f\"{i}_t\"]\n",
    "    proj = proj.filter(regex=r'.*(?<!_t)$')\n",
    "\n",
    "    #ju_jti\n",
    "    proj.loc[proj.prog_abbr.str.contains('JTI', na=False), 'thema_code'] = 'JU-JTI'\n",
    "    proj.loc[proj.prog_abbr.str.contains('JTI', na=False),  'destination_code'] = proj.loc[proj.prog_abbr.str.contains('JTI', na=False)].instrument.str.split('-').str[-1]\n",
    "    proj.loc[proj.area_abbr=='JTI-CS', 'destination_code'] = 'CLEAN-SKY'\n",
    "\n",
    "    proj.loc[(proj.destination_code=='CLEAN-SKY'), 'destination_next_fp'] = 'CLEAN-AVIATION'\n",
    "    proj.loc[(proj.destination_code=='FCH'), 'destination_next_fp'] = 'CLEANH2'\n",
    "    proj.loc[(proj.destination_code=='IMI'), 'destination_next_fp'] = 'IHI'\n",
    "    proj.loc[(proj.destination_code.isin(['ENIAC','ARTEMIS'])), 'destination_next_fp'] = 'Chips'\n",
    "    # proj.loc[proj.thema_code=='JU-JTI', 'action_code'] = proj.fp_specific_instrument.str.split('-').str[1]\n",
    "\n",
    "    # WIDENING COST\n",
    "    proj.loc[proj.area_abbr.str.contains('COST', na=False), 'thema_code'] = 'COST'\n",
    "    proj.loc[proj.area_abbr.str.contains('COST', na=False), 'programme_next_fp'] = 'Widening'\n",
    "\n",
    "    destination = pd.read_json(open(\"data_files/destination.json\", 'r', encoding='utf-8'))\n",
    "    proj.loc[(~proj.thema_code.isin(['MSCA','ERC']))&(proj.destination_code.isnull()), 'destination_code'] = proj.area_abbr\n",
    "    # proj.loc[proj.destination_code.isnull(), 'destination_code'] = proj.thema_code+'-OTHER'\n",
    "    proj = proj.merge(destination[['destination_code', 'destination_name_en']], how='left', on='destination_code')\n",
    "    proj.loc[(~proj.destination_code.isnull())&(proj.destination_name_en.isnull()), 'destination_name_en'] = proj.area_lib\n",
    "\n",
    "    thema = pd.read_json(open(\"data_files/thema.json\", 'r', encoding='utf-8'))\n",
    "    proj = proj.merge(thema[['thema_code', 'thema_name_en']], how='left', on='thema_code')\n",
    "    proj.loc[(~proj.thema_code.isnull())&(proj.thema_name_en.isnull()), 'destination_name_en'] = proj.prog_lib\n",
    "\n",
    "\n",
    "    proj.loc[proj.programme_code.isnull(), 'programme_code'] = proj.prog_abbr\n",
    "    proj.loc[proj.programme_name_en.isnull(), 'programme_name_en'] = proj.prog_lib\n",
    "\n",
    "    proj['pilier_name_en'] = proj.pilier.str.capitalize()\n",
    "\n",
    "    # action\n",
    "    instr = pd.read_csv('data_files/instru_nomenclature.csv', sep=';')\n",
    "    proj = proj.merge(instr, how='left', on='instrument').drop(columns=['instrument_name']).rename(columns={'name':'action_name'})\n",
    "    proj.loc[proj.destination_code=='NIGHT', 'action_next_fp'] = 'MSCA'   \n",
    "\n",
    "\n",
    "    if any(proj.action_code.isnull()):\n",
    "        print(proj[proj.action_code.isnull()].instrument.unique())   \n",
    "        \n",
    "    print(f\"- size proj: {len(proj.drop_duplicates())}\")\n",
    "\n",
    "    return proj.drop_duplicates()\n",
    "test=themes_cleaning(proj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25fbb0c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "test.columns\n",
    "# test.loc[~test.programme_code.isnull(),['pilier_name_en', 'pilier', 'prog_abbr', 'prog_lib',\n",
    "#        'area_abbr', 'area_lib', 'thema_code', 'destination_code', 'destination_name_en',\n",
    "#        'destination_next_fp', 'programme_name_en', 'programme_code']].drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bada1c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functions_shared import work_csv\n",
    "work_csv(test[['pilier', 'prog_abbr', 'prog_lib', 'programme_code','programme_name_en', 'thema_code','thema_name_en',\n",
    "       'area_abbr', 'area_lib',  'destination_code', 'destination_name_en',\n",
    "       'destination_next_fp', \n",
    "       'programme_next_fp']].drop_duplicates(), 'fp7')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a946fb6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test.loc[test.thema_code=='ERC',['pilier', 'prog_abbr', 'prog_lib', 'area_abbr', 'area_lib', 'thema_code', 'destination_code']].drop_duplicates()\n",
    "test[[ 'instrument', 'action_code', 'action_name', 'destination_code','action_next_fp']].drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3107908e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"- size FP6 after clean thema: {len(x.loc[x.stage=='successful'])}, fund: {'{:,.1f}'.format(x.loc[x.stage=='successful', 'subv_obt'].sum())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08ef7d0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged[merged.topic_code=='HORIZON-CL4-2021-TWIN-TRANSITION-01-03'].topic_name.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4683533e",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged[merged.euro_partnerships_type=='co-programmed'][['topic_name', 'euro_ps_name']].drop_duplicates()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
