{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## LOAD bases\n",
      "size _proj: 342190\n",
      "- size part: 1311188\n",
      "involved successful:195,503.0\n",
      "subv_net_laureat:71,730,094,005.4\n",
      "subv_laureat:68,609,787,841.5\n",
      "subv_prop:687,415,105,572.9\n",
      "- def(my_country_code) size df: 255\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\zfriant\\OneDrive\\Github\\pcri\\functions_shared.py:247: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value 'AU' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  df.loc[df[c]==k, 'parent_iso2'] = v\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size part: 1308763\n",
      "size part_init with major cols: 1308763\n",
      "## PROJ cleaning\n",
      "## ENTITIES cleaning\n",
      "#FCT gps_col\n",
      "parent_iso missing : [None]\n",
      "- size entities 226938\n",
      "- ok entities source generalState not null\n",
      "size part without country: 174712\n",
      "size part with country: 175681\n",
      "\n",
      "### LOADING REF_SOURCE\n",
      "- size of ref_source : 503464\n",
      "### 2d - REF_SOURCE -> REF\n",
      "- size remplacement pic: 446\n",
      "- longueur de ref:63002\n",
      "- nb id: 62875\n",
      "- nb id after fill: 62875\n",
      "- si ++id pour un generalPic:           generalPic                     id id_secondaire  \\\n",
      "306964     955557-13        265906719;j5sy1           NaN   \n",
      "298317     859889-18        266900273;etBz7           NaN   \n",
      "306949     955325-18        266900273;etBz7           NaN   \n",
      "313146     927638258        552059024;n2X5f    199519291V   \n",
      "299080  888896512000            7Uu6D;bZiTA           NaN   \n",
      "305790     949629419            9wAap;n2X5f           NaN   \n",
      "299133  888898163000      IXHyv;n2X5f;bZiTA           NaN   \n",
      "439851     940565157  R0307bxz06;R04zgvy449           NaN   \n",
      "299555     892207359        Uxr7Z;775665789    199213070W   \n",
      "312839     917388559            n2X5f;IXJPr    200711920F   \n",
      "302824     917395446            n2X5f;u13se    200612829Y   \n",
      "305846     950071642            n2X5f;zO154    198819289Y   \n",
      "309141     999899378            qPCgk;n2X5f    199812849E   \n",
      "299090  888896738000            tIJ02;FL57b           NaN   \n",
      "299091     888896745            tIJ02;FL57b           NaN   \n",
      "299092  888896745000            tIJ02;FL57b           NaN   \n",
      "299287     889002436            tIJ02;MB44Z           NaN   \n",
      "309142     999907235            u13se;n2X5f           NaN   \n",
      "\n",
      "       country_code_mapping ZONAGE  \n",
      "306964                  FRA    NaN  \n",
      "298317                  FRA    NaN  \n",
      "306949                  FRA    NaN  \n",
      "313146                  FRA    NaN  \n",
      "299080                  FRA    NaN  \n",
      "305790                  FRA    NaN  \n",
      "299133                  FRA    NaN  \n",
      "439851                  CZE    NaN  \n",
      "299555                  FRA    NaN  \n",
      "312839                  FRA    NaN  \n",
      "302824                  FRA    NaN  \n",
      "305846                  FRA    NaN  \n",
      "309141                  FRA    NaN  \n",
      "299090                  FRA    NaN  \n",
      "299091                  FRA    NaN  \n",
      "299092                  FRA    NaN  \n",
      "299287                  FRA    NaN  \n",
      "309142                  FRA    NaN  \n",
      "parent_iso missing : ['ZOE' 'ZOI']\n",
      "size de p: 168978\n",
      "cols de p: Index(['generalPic', 'country_code_mapping', 'country_code', 'id',\n",
      "       'id_secondaire', 'ZONAGE', '_merge'],\n",
      "      dtype='object')\n",
      "size p1 pic+cc: 62928\n",
      "size p2 pic cc_parent: 13\n",
      "A faire si possible, vérifier pourquoi des participations avec pic identiques ont un id ou pas nb pic: 365\n",
      "size de new p: 62941, cols: Index(['generalPic', 'country_code_mapping', 'country_code', 'id',\n",
      "       'id_secondaire', 'ZONAGE'],\n",
      "      dtype='object')\n",
      "size part1: 1308763, part: 1308763\n",
      "size participation after add nuts: 1308763, sans nuts name: 196\n",
      "- size entities 38029\n",
      "1- size ent si multi id -> ent_size_to_keep = 38048\n",
      "Index(['generalPic', 'id', 'country_code_mapping', 'id_extend'], dtype='object')\n",
      "### merge ROR\n",
      "- End size entities_tmp+ror_info: 38048\n",
      "size entities_tmp after add ror_info: 38048, entities_size_to_keep: 38048\n",
      "### CATEGORY paysage\n",
      "### merge PAYSAGE\n",
      "- End size entities_tmp+paysage_info: 38048\n",
      "### merge SIRENE\n",
      "- first size sirene : 15655\n",
      "- size sirene : 15655\n",
      "1 - A vérifier -> liste des noms à traiter:\n",
      "                            ens denom_us  \\\n",
      "0                          NaN     None   \n",
      "1                          NaN     None   \n",
      "2                          NaN     None   \n",
      "3                          NaN     None   \n",
      "4                          NaN     None   \n",
      "5                          NaN     None   \n",
      "6                          NaN     None   \n",
      "7   DCNS NAVIRES ARMES LORIENT     None   \n",
      "8       ECOL PRIM JEANNE D ARC     None   \n",
      "9         CLINIQUE BEAU-SOLEIL     None   \n",
      "10                         NaN     None   \n",
      "11                         NaN     None   \n",
      "12                         NaN     None   \n",
      "13                         NaN     None   \n",
      "14                HOPITAL NORD     None   \n",
      "15                         NaN     None   \n",
      "16                         NaN     None   \n",
      "17        GRETA ROUEN MARITIME     None   \n",
      "18                         NaN     None   \n",
      "19                         NaN     None   \n",
      "\n",
      "                                               nom_ul  \n",
      "0                                           MODUL-BIO  \n",
      "1                                            PHYTODIA  \n",
      "2                                            SOLIANCE  \n",
      "3                            BIO RAD LABORATORIES SAS  \n",
      "4                                 LINCOLN ELECTRIC SA  \n",
      "5                                        AXESS EUROPE  \n",
      "6                         CENTRE INTERNAT RECH CANCER  \n",
      "7                                         NAVAL GROUP  \n",
      "8                                   COMMUNE DE MENTON  \n",
      "9                            AESIO SANTE MEDITERRANEE  \n",
      "10                                 PRESTWICK CHEMICAL  \n",
      "11            EUROPEAN SYNCHROTRON RADIATION FACILITY  \n",
      "12                                                BEL  \n",
      "13                                             THALES  \n",
      "14            CENTRE HOSPITALIER REGIONAL DE GRENOBLE  \n",
      "15                              CORDOUAN TECHNOLOGIES  \n",
      "16  AGENCE DE L ENVIRONNEMENT ET DE LA MAITRISE DE...  \n",
      "17              LYCEE ENS GEN TECHNO GUSTAVE FLAUBERT  \n",
      "18                        SOLETANCHE BACHY ENTREPRISE  \n",
      "19                                      CRYPTOEXPERTS  \n",
      "#####\n",
      "- End size entities_tmp+sirene: 38048\n",
      "taille de entities_tmp avant groupe:38048\n",
      "taille de entities_tmp après groupe 38048\n",
      "### sourcer les identifiants pour getInformations\n",
      "\n",
      "## category woven\n",
      "- categorization missing\n",
      "      source_id                                      entities_name  \\\n",
      "4076      siren                                                NaN   \n",
      "7644      siren                                                NaN   \n",
      "7803    paysage  Fondation universitaire Institut méditerranéen...   \n",
      "13624     siren                                                NaN   \n",
      "14999   paysage     Institut des sciences et technologies de Paris   \n",
      "17911     siren                              Gendarmerie nationale   \n",
      "18630     siret                                                NaN   \n",
      "19722     siren                              Gendarmerie nationale   \n",
      "19909     siren                              Gendarmerie nationale   \n",
      "27640     siren                                                NaN   \n",
      "30980     siren            Sce compt distincite eaux ablainzevelle   \n",
      "37183     siren                              Gendarmerie nationale   \n",
      "37238     siren                                                NaN   \n",
      "\n",
      "          entities_id siren_cj paysage_category  \n",
      "4076        444285357      NaN              NaN  \n",
      "7644        912672615      NaN              NaN  \n",
      "7803            dxBdz      NaN              NaN  \n",
      "13624       900285297      NaN              NaN  \n",
      "14999           Flp7H      NaN              NaN  \n",
      "17911       786262410      NaN              NaN  \n",
      "18630  15100002300466      NaN              NaN  \n",
      "19722       786262410      NaN              NaN  \n",
      "19909       786262410      NaN              NaN  \n",
      "27640       152000014      NaN              NaN  \n",
      "30980       276200045      NaN              NaN  \n",
      "37183       786262410      NaN              NaN  \n",
      "37238       152000014      NaN              NaN  \n",
      "- taille de df après cat: 38048\n",
      "\n",
      "### MIRES\n",
      "size part1 avant: 1308763\n",
      "size part1 -> part_tmp: 1308812\n",
      "403195\n",
      "size part2: 131096, nb unique pic_d: 130670\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zfriant\\AppData\\Local\\Temp\\ipykernel_7108\\104296064.py:402: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  part2=part2.groupby(['pic_d']).apply(lambda x: x.sort_values('generalState', key=lambda col: pd.Categorical(col, categories=gen_state, ordered=True)), include_groups=True).reset_index(drop=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size part2: 119406, nb unique pic_d: 119406\n",
      "11265\n",
      "11265\n",
      "size part_tmp after merge part2: 1308812\n",
      "size part_tmp after merge part2: 1308812\n",
      "size part_tmp after clean string: 1308812\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zfriant\\AppData\\Local\\Temp\\ipykernel_7108\\104296064.py:471: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  pt['calculated_fund'] = np.where(pt.stage=='successful', pt['subv'], pt['requestedGrant'])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size part_tmp after concat with erc: 1308812\n",
      "involved successful:193,082.0\n",
      "subv_net_laureat:71,562,252,054.7\n",
      "subv_laureat:68,609,700,856.6\n",
      "subv_prop:687,414,885,969.0\n",
      "### CORDIS type\n",
      "- size entities_info: 1308812\n",
      "size part_tmp after clean codis legal type: 1308812\n",
      "size part_tmp avant: 1308812\n",
      "size part_tmp after merge countries: 1308812\n",
      "involved successful:193,082.0\n",
      "subv_laureat:71,562,252,054.7\n",
      "subv_prop:687,414,885,969.0\n",
      "involved successful:179,528.0\n",
      "subv_laureat:67,649,481,798.1\n",
      "subv_prop:537,223,988,592.2\n",
      "1284634\n",
      "evaluated -> nb_involved 1105303, nb_type_null 0, 'part_involved_null' 0.0 fund_type 537223988592.2101, fund_type_null 0.0, 0.0\n",
      "successful -> nb_involved 179528, nb_type_null 0, 'part_involved_null' 0.0 fund_type 67649481798.11, fund_type_null 0.0, 0.0\n",
      "- size project lauréats: 35462, 37770, fund: 70,898,580,031.2\n"
     ]
    }
   ],
   "source": [
    "def H2020_process():\n",
    "    import pandas as pd, numpy as np, gc\n",
    "    from step3_entities.references import ref_source_load, ref_source_2d_select\n",
    "    from step3_entities.merge_referentiels import merge_paysage, merge_ror, merge_sirene\n",
    "    from step3_entities.categories import category_agreg, category_paysage,category_woven, cordis_type, mires\n",
    "    from step3_entities.ID_getSourceRef import get_source_ID\n",
    "    from step4_calculations.collaborations import collab_base, collab_cross\n",
    "    from config_path import PATH_SOURCE, PATH_CLEAN, PATH_REF, PATH_CONNECT\n",
    "    from functions_shared import unzip_zip, my_country_code\n",
    "\n",
    "    def h20_nom_load():\n",
    "        destination = pd.read_json(open(\"data_files/destination.json\", 'r', encoding='utf-8'))\n",
    "        thema = pd.read_json(open(\"data_files/thema.json\", 'r', encoding='utf-8'))\n",
    "        act = pd.read_json(open(\"data_files/actions_name.json\", 'r', encoding='utf-8'))\n",
    "        topics = unzip_zip('H2020_2022-12-05.json.zip', f\"{PATH_SOURCE}H2020/\", 'topics.json', encode='utf-8')\n",
    "        pilier_fr = pd.read_json(open(\"data_files/H20_pilier.json\", 'r', encoding='utf-8'))\n",
    "        # countries = pd.read_csv(f\"{PATH_SOURCE}H2020/country_current.csv\", sep=';')\n",
    "        countries = pd.read_pickle(f\"{PATH_CLEAN}country_current.pkl\")\n",
    "        actions = pd.read_table(f\"{PATH_CLEAN}actions_current.csv\", sep=\";\")\n",
    "        nuts = pd.read_pickle(f'{PATH_REF}nuts_complet.pkl')\n",
    "        return destination, thema, act, topics, pilier_fr, countries, actions, nuts\n",
    "    destination, thema, act, topics, pilier_fr, countries, actions, nuts = h20_nom_load()\n",
    "\n",
    "    def h20_load():\n",
    "        print(\"## LOAD bases\")\n",
    "        _proj=pd.read_pickle(f\"{PATH_SOURCE}H2020/H2020_projects.pickle\")\n",
    "        _proj=pd.DataFrame(_proj)\n",
    "        _proj=_proj.replace('#', np.nan)\n",
    "        print(f\"size _proj: {len(_proj)}\")\n",
    "        part=pd.read_pickle(f\"{PATH_SOURCE}H2020/H2020_participation.pickle\")\n",
    "        part=pd.DataFrame(part)\n",
    "        part=part.replace('#', np.nan)\n",
    "        print(f\"- size part: {len(part)}\")\n",
    "        entities = unzip_zip('H2020_2022-12-05.json.zip', f\"{PATH_SOURCE}H2020/\", \"legalEntities.json\", encode='utf-8')\n",
    "        status = pd.read_csv(f\"{PATH_SOURCE}H2020/redressement_status_code.csv\", sep=';', usecols=['project_id','stat_code'], dtype='str')\n",
    "        return _proj, part, entities, status\n",
    "    _proj, part, entities, status = h20_load()\n",
    "    print(f\"involved successful:{'{:,.1f}'.format(part.loc[(part.stage=='successful'), 'generalPic'].count())}\\nsubv_net_laureat:{'{:,.1f}'.format(part.loc[(part.stage=='successful'), 'subv_net'].sum())}\\nsubv_laureat:{'{:,.1f}'.format(part.loc[(part.stage=='successful'), 'subv'].sum())}\\nsubv_prop:{'{:,.1f}'.format(part.loc[(part.stage=='evaluated'), 'requestedGrant'].sum())}\")\n",
    "\n",
    "    country_h20 = my_country_code()\n",
    "\n",
    "    part.loc[part.role=='participant', 'role'] = 'partner'\n",
    "    # part.loc[part.countryCode=='ZZ', 'country_code_mapping'] = 'ZZZ'\n",
    "    part = part[part.participates_as!='utro']\n",
    "    part.rename(columns={'order_number':'orderNumber'}, inplace=True)\n",
    "    print(f\"size part: {len(part)}\")\n",
    "    part_init = part[['project_id', 'orderNumber', 'generalPic_old', 'pic', 'participates_as',\n",
    "        'role', 'legalName', 'part_total_cost', 'subv', 'subv_net',\n",
    "        'partner_status', 'countryCode', 'legalEntityTypeCode', 'isSme',\n",
    "        'nutsCode', 'stage', 'shortName', 'requestedGrant', 'budget', 'url', 'generalPic']]\n",
    "    print(f\"size part_init with major cols: {len(part_init)}\")\n",
    "\n",
    "\n",
    "    part_init=(part_init.merge(country_h20[['iso2', 'iso3', 'parent_iso3']], how='left', left_on='countryCode', right_on='iso2')\n",
    "    .rename(columns={'iso3':'country_code_mapping', 'parent_iso3':'country_code'})\n",
    "    .drop(columns='iso2'))\n",
    "\n",
    "    if any(part_init[part_init.country_code_mapping.isnull()].countryCode.unique()):\n",
    "        print(part_init[part_init.country_code_mapping.isnull()].countryCode.unique())\n",
    "\n",
    "    ##status\n",
    "    _proj = _proj.merge(status, how='inner', on='project_id')\n",
    "    _proj.loc[_proj.stage=='evaluated', 'status_code'] = _proj.stat_code\n",
    "    _proj.drop(columns=['stat_code'], inplace=True)\n",
    "\n",
    "    l=['RIA','IA','CSA']\n",
    "    tmp=_proj.loc[(~_proj.action_id.isin(['MSCA','ERC'])&(~_proj.action_2_id.isnull())&(_proj.action_id!='SME')),\n",
    "    ['action_2_id']].drop_duplicates()\n",
    "    tmp['action_code'] = tmp['action_2_id'].str.extract(\"(\" + \"|\".join(l) +\")\", expand=False)\n",
    "    _proj = _proj.merge(tmp, how='left', on='action_2_id')\n",
    "    _proj.loc[_proj.action_code.isnull(), 'action_code'] = _proj.action_id\n",
    "\n",
    "    def proj_cleaning(proj):\n",
    "        print(\"## PROJ cleaning\")\n",
    "        from functions_shared import website_to_clean\n",
    "        for i in ['title','abstract', 'free_keywords', 'eic_panels', 'url_project']:\n",
    "            proj[i]=proj[i].str.replace('\\\\n|\\\\t|\\\\r|\\\\s+', ' ', regex=True).str.strip()\n",
    "            \n",
    "        kw = proj[['project_id','stage','free_keywords']].drop_duplicates()\n",
    "        kw = kw.assign(free_keywords = kw.free_keywords.str.split(';|,')).explode('free_keywords')\n",
    "        kw['free_keywords'] = kw.free_keywords.str.replace('\\\\.+', '', regex=True)\n",
    "        kw = kw.loc[kw.free_keywords.str.len()>3].drop_duplicates()\n",
    "        kw.free_keywords = kw.free_keywords.groupby(level=0).apply(lambda x: '|'.join(x.str.strip().unique()))\n",
    "        kw = kw.drop_duplicates()\n",
    "\n",
    "        proj = proj.drop(columns='free_keywords').merge(kw, how='left', on=['project_id','stage']).drop_duplicates()    \n",
    "            \n",
    "        proj.loc[proj.url_project.str.contains('project/rcn', na=False), 'url_project']=np.nan\n",
    "\n",
    "        proj.mask(proj=='', inplace=True)  \n",
    "        for i,row in proj.iterrows():\n",
    "            if row.loc['url_project'] is not None:\n",
    "                proj.at[i, 'project_webpage'] = website_to_clean(row['url_project'])\n",
    "\n",
    "        proj.mask(proj=='', inplace=True)  \n",
    "\n",
    "        for d in ['call_deadline', 'signature_date',  'start_date', 'end_date', 'submission_date', 'ecorda_date']:\n",
    "            proj[d] = proj[d].astype('datetime64[ns]')\n",
    "\n",
    "        proj['proposal_expected_number'] = proj['proposal_expected_number'].astype('float')\n",
    "        return proj\n",
    "\n",
    "    def h20_topics(df, act, actions, destination, pilier_fr, thema):\n",
    "        proj = (df.assign(fp_specific_pilier=df.pilier, fp_specific_programme=df.programme_name_en, fp_specific_instrument=df.action_2_id)\n",
    "            .rename(columns={'pilier':'pilier_name_en', 'topicCode':'topic_code','topicDescription':'topic_name',\n",
    "                                    'action_2_id':'action_code2', 'action_2_name':'action_name2', \n",
    "                                    'action_3_id':'action_code3', 'action_3_name':'action_name3'})\n",
    "            .drop(columns=['action_name'])\n",
    "            .merge(pilier_fr, how='left', on='pilier_name_en')\n",
    "            .merge(act, how='left', on='action_code'))\n",
    "\n",
    "        #euratom\n",
    "        proj.loc[proj.pilier_name_fr=='Euratom', 'pilier_name_en'] = 'Euratom'\n",
    "        proj.loc[(proj.pilier_name_fr=='Euratom')&(proj.topic_code.str.contains('NFRP')), 'programme_code'] = 'NFRP'\n",
    "        proj.loc[(proj.pilier_name_fr=='Euratom')&(proj.programme_code=='NFRP'), 'programme_name_en'] = 'Nuclear fission and radiation protection'\n",
    "        proj.loc[proj.call_id=='EURATOM-Adhoc-2014-20', 'programme_code'] = 'Fusion'\n",
    "        proj.loc[proj.call_id=='EURATOM-Adhoc-2014-20', 'programme_name_en'] = 'Fusion Energy'\n",
    "        proj.loc[(proj.pilier_name_fr=='Euratom')&(proj.call_id!='EURATOM-Adhoc-2014-20')&(proj.programme_code!='NFRP'), 'programme_code'] = 'Euratom-other'\n",
    "        proj.loc[(proj.pilier_name_fr=='Euratom')&(proj.call_id!='EURATOM-Adhoc-2014-20')&(proj.programme_code!='NFRP'), 'programme_name_en'] = 'Euratom other actions'\n",
    "\n",
    "        euratom = pd.read_csv('data_files/euratom_thema_all_FP.csv', sep=';', na_values='')\n",
    "        proj = proj.merge(euratom[['topic_area', 'thema_code', 'thema_name_en']], how='left', left_on='topic_code', right_on='topic_area', suffixes=['', '_t'])\n",
    "        proj.loc[(~proj.thema_code_t.isnull()), 'thema_code'] = proj.loc[(~proj.thema_code_t.isnull()), 'thema_code_t']\n",
    "        proj.loc[(~proj.thema_name_en_t.isnull()), 'thema_name_en'] = proj.loc[(~proj.thema_name_en_t.isnull()), 'thema_name_en_t']\n",
    "        proj = proj.filter(regex=r'.*(?<!_t)$').drop(columns='topic_area')\n",
    "\n",
    "        #piler JU-JTI\n",
    "        proj.loc[proj.thema_code.str.contains('JU', na=False), 'destination_code'] = proj.thema_code.str.replace('JU','').str.strip()\n",
    "        proj.loc[proj.thema_code.str.contains('JU', na=False), 'thema_code'] = 'JU-JTI'\n",
    "        proj.loc[proj.call_id.str.contains('JTI',na=False), 'destination_code'] = proj['action_code2'].str.split('-').str[0]\n",
    "        proj.loc[proj.call_id.str.contains('JTI',na=False)&(proj.action_code2.isnull()), 'destination_code'] = proj['call_id'].str.split('-').str[2]\n",
    "        proj.loc[(proj.action_code2.str.contains('BBI', na=False)), 'destination_code'] = 'CBE'\n",
    "        proj.loc[(proj.destination_code=='EuroHPC'), 'destination_code'] = 'EUROHPC'\n",
    "        proj.loc[(proj.thema_code=='ECSEL'), 'destination_code'] = 'Chips'\n",
    "        proj.loc[(proj.destination_code=='CS2'), 'destination_code'] = 'CLEAN-AVIATION'\n",
    "        proj.loc[(proj.destination_code=='FCH2'), 'destination_code'] = 'CLEANH2'\n",
    "        proj.loc[(proj.destination_code=='IMI2'), 'destination_code'] = 'IHI'\n",
    "        proj.loc[(proj.destination_code=='Shift2Rail'), 'destination_code'] = \"EU-Rail\"\n",
    "        l=['KDT', 'CBE','EUROHPC', 'CLEAN-AVIATION', 'CLEANH2', 'IHI']\n",
    "        proj.loc[(proj.call_id.str.contains('JTI',na=False))|(proj.destination_code.isin(l)), 'thema_code'] = 'JU-JTI'\n",
    "\n",
    "        # MSCA / ERC\n",
    "        proj.loc[proj.programme_code=='MSCA', 'thema_code'] = 'MSCA'\n",
    "        proj.loc[proj.programme_code=='ERC', 'thema_code'] = 'ERC'\n",
    "\n",
    "        # ### ajustement MSCA\n",
    "        msca_correspondence = pd.read_table('data_files/msca_correspondence.csv', sep=\";\")\n",
    "\n",
    "        msca_correspondence = msca_correspondence[msca_correspondence.framework=='H2020'].rename(columns={'EsCodeL2':'destination_detail_code'}).drop(columns='framework')\n",
    "        proj.loc[(proj.thema_code=='MSCA')&(proj.action_code3.isnull()), 'action_code3'] = proj.action_code2\n",
    "\n",
    "\n",
    "        m = proj.loc[(proj.action_code=='MSCA'), ['action_code3']].drop_duplicates()\n",
    "        m = m.merge(msca_correspondence, how='left', left_on='action_code3', right_on='old')\n",
    "        m = m.merge(actions[['destination_detail_code','destination_detail_name_en']].drop_duplicates(), how='left', on='destination_detail_code')\n",
    "        m.loc[m.destination_detail_code=='CITIZENS', 'destination_detail_name_en'] = \"European Researchers' Night\"\n",
    "        m.loc[m.destination_detail_code=='COFUND', 'destination_detail_name_en'] = \"Co-funding of regional, national and international programmes\"\n",
    "        m.loc[m.destination_detail_code.str.contains('-', na=False) ,'destination_code'] = m.destination_detail_code.str.split('-').str[0]\n",
    "\n",
    "        m = m.merge(proj.drop(columns=['destination_code']), how='inner', on='action_code3')\n",
    "        proj = proj.loc[~proj.action_code3.isin(m.action_code3.unique())]\n",
    "\n",
    "        proj = pd.concat([proj, m], ignore_index=True)\n",
    "\n",
    "        proj.loc[(proj.thema_code=='MSCA')&(proj.destination_code.isnull()), 'destination_code'] = proj.destination_detail_code\n",
    "        proj.loc[(proj.thema_code=='MSCA')&(proj.destination_code.isnull()),'destination_code'] = 'MSCA-OTHER'\n",
    "        proj.loc[(proj.thema_code=='MSCA'), ['destination_code', 'action_code3','destination_detail_code']].drop_duplicates()\n",
    "        proj.loc[proj.programme_code=='MSCA', 'programme_name_en'] = 'Marie Skłodowska-Curie Actions (MSCA)'\n",
    "\n",
    "        proj.loc[(proj.action_code=='MSCA'), 'action_code2'] = np.nan\n",
    "        proj.loc[(proj.action_code=='MSCA'), 'action_name2'] = np.nan\n",
    "\n",
    "        proj.drop(columns='old', inplace=True)\n",
    "\n",
    "        ### ajustement ERC\n",
    "        proj.loc[proj.thema_code=='ERC', 'destination_code'] = proj.loc[proj.thema_code=='ERC'].action_code2.str.split('-').str[1]\n",
    "        proj.loc[proj.destination_code=='POC-LS', 'destination_code'] = \"POC\"\n",
    "        proj.loc[(proj.thema_code=='ERC')&(proj.destination_code.isnull()), 'destination_code'] = 'ERC-OTHER'\n",
    "        proj.loc[(proj.action_code=='ERC'), 'action_code2'] = np.nan\n",
    "        proj.loc[(proj.action_code=='ERC'), 'action_name2'] = np.nan\n",
    "\n",
    "        # FET\n",
    "        proj.loc[proj.programme_code=='FET', 'thema_code'] = 'PATHFINDER'\n",
    "        # proj.loc[proj.programme_code=='FET', 'thema_name_en'] = 'European Innovation Council'\n",
    "        proj.loc[proj.programme_code=='FET', 'destination_code'] = proj.thema_name_en.str.split().str[1].str.upper()\n",
    "        proj.loc[proj.programme_code=='FET', 'thema_name_en'] = np.nan\n",
    "        proj.loc[proj.programme_code=='FET', 'programme_name_en'] = \"European Innovation Council\"\n",
    "        proj.loc[proj.programme_code=='FET', 'programme_code'] = \"EIC\"\n",
    "        # proj.loc[proj.programme_code=='FET', 'destination_name_en'] = 'EIC Pathfinder'\n",
    "\n",
    "        # programme SME\n",
    "        proj.loc[proj.programme_code=='SME', 'thema_code'] = 'ACCELERATOR'\n",
    "        proj.loc[proj.programme_code=='FET', 'thema_name_en'] = np.nan\n",
    "        proj.loc[proj.programme_code=='SME', 'programme_name_en'] = 'European Innovation Council'\n",
    "        # proj.loc[proj.programme_code=='SME', 'destination_code'] = 'ACCELERATOR'\n",
    "        # proj.loc[proj.programme_code=='SME', 'destination_name_en'] = 'EIC Accelerator'\n",
    "        proj.loc[proj.topic_code=='H2020-Art185-Eurostars2', 'thema_code'] = 'INNOVSMES'\n",
    "        proj.loc[proj.thema_code=='INNOVSMES', 'programme_name_en'] = 'European Innovation Ecosystems'\n",
    "        proj.loc[proj.programme_name_en=='European Innovation Ecosystems', 'programme_code'] = 'EIE'\n",
    "\n",
    "        # INFRA\n",
    "        proj.loc[proj.programme_code=='INFRA', 'thema_code'] = proj.programme_code\n",
    "        proj.loc[proj.programme_code=='INFRA', 'destination_code'] = proj.loc[proj.programme_code=='INFRA'].call_id.str.split('-').str[1]\n",
    "        proj.loc[(proj.programme_code=='INFRA')&(~proj.destination_code.isin(destination.destination_code.unique())), 'destination_code'] = 'DESTINATION-OTHER'\n",
    "                \n",
    "        # EIT\n",
    "        proj.loc[proj.action_code=='KICS', 'pilier_name_en'] = 'Innovative Europe'\n",
    "        proj.loc[proj.action_code=='KICS', 'programme_code'] = 'EIT'\n",
    "        proj.loc[proj.action_code=='KICS', 'programme_name_en'] = 'The European Institute of Innovation and Technology (EIT)'\n",
    "\n",
    "        # t = thema.loc[~thema.dest_h20.isnull(), ['thema_code','dest_h20']]\n",
    "\n",
    "        # proj = proj.merge(t, how='left', left_on='thema_code', right_on='dest_h20', suffixes=['', '_x'])\n",
    "\n",
    "        # proj.loc[proj.action_code=='KICS', 'destination_code'] = proj.destination_code_x\n",
    "        # # proj.loc[proj.action_code=='KICS', 'destination_name_en'] = proj.destination_name_en_x\n",
    "        # proj.loc[(proj.action_code=='KICS'), 'thema_code'] = 'EIT'\n",
    "        # proj.loc[(proj.action_code=='KICS'), 'thema_name_en'] = 'European Institute of Innovation and Technology'\n",
    "        # proj.loc[(proj.action_code=='KICS'), 'action_code'] = 'KICS'\n",
    "        # proj.loc[(proj.action_code=='KICS'), 'action_name'] = 'Knowledge and Innovation Communities'\n",
    "        # proj.loc[(proj.programme_code=='EIT')&(proj.action_code.isnull()), 'action_code'] = 'EIT'\n",
    "        # proj.loc[(proj.programme_code=='EIT')&(proj.action_code.isnull()), 'action_name'] = 'EIT actions'\n",
    "\n",
    "        # proj.drop(columns=['destination_code_x', 'dest_h20'], inplace=True)\n",
    "\n",
    "        # # WIDENING COST\n",
    "        proj.loc[proj.programme_code.str.contains('TWINING|WIDESPREAD|NCPNET', na=False), 'thema_code'] = 'ACCESS'\n",
    "        proj.loc[proj.programme_code.str.contains('ERA', na=False), 'thema_code'] = 'TALENTS'\n",
    "        proj.loc[proj.programme_code.str.contains('INTNET', na=False), 'thema_code'] = 'COST'\n",
    "        proj.loc[(proj.pilier_name_en=='Spreading excellence and widening participation')&(proj.programme_code!='ERA'), 'programme_code'] = 'Widening'\n",
    "        proj.loc[proj.programme_code=='Widening', 'programme_name_en'] = 'Widening participation and spreading excellence'\n",
    "\n",
    "        proj.loc[(proj.programme_code=='Widening')&(proj.thema_code.isnull()), 'thema_code'] = 'WIDENING-OTHER'\n",
    "\n",
    "        dest = destination[['destination_code', 'destination_name_en']]\n",
    "        proj = proj.merge(dest, how='left', on='destination_code')\n",
    "\n",
    "        proj = proj.merge(thema, how='left', on='thema_code', suffixes=['','_x'])\n",
    "        proj.loc[~proj.thema_name_en_x.isnull(), 'thema_name_en'] = proj.thema_name_en_x\n",
    "        proj.drop(columns=['thema_name_en_x','dest_h20'], inplace=True)\n",
    "        return proj\n",
    "\n",
    "    def entities_cleaning(df, country_h20, p):\n",
    "        print(\"## ENTITIES cleaning\")\n",
    "        from functions_shared import gps_col, num_to_string\n",
    "        df = pd.DataFrame(df)\n",
    "        df = gps_col(df)\n",
    "        df = df.loc[~df.generalPic.isnull()]\n",
    "        \n",
    "        df = (df.merge(country_h20[['iso2', 'iso3', 'parent_iso3']], how='left', left_on='countryCode', right_on='iso2')\n",
    "            .drop(columns='iso2')\n",
    "            .rename(columns={'parent_iso3':'country_code', 'iso3': 'country_code_mapping'}))\n",
    "        print(f\"parent_iso missing : {df[df.country_code.isnull()].countryCode.unique()}\")\n",
    "        df.loc[df.country_code.isnull(), 'country_code'] = df.loc[df.country_code.isnull()].country_code_mapping \n",
    "\n",
    "        c = ['pic', 'generalPic']\n",
    "        df[c] = df[c].map(num_to_string)\n",
    "        print(f\"- size entities {len(df)}\")\n",
    "\n",
    "        if len(df[df.generalState.isnull()])>0:\n",
    "            print(\"- entities source generalState -> new state (processing into entities_single)\")\n",
    "        else:\n",
    "            print(\"- ok entities source generalState not null\")\n",
    "\n",
    "        lien_genCalcPic = p[['generalPic_old', 'pic']].drop_duplicates()\n",
    "        print(f\"size part without country: {len(p[['generalPic_old', 'pic']].drop_duplicates())}\\nsize part with country: {len(p[['generalPic_old', 'pic', 'countryCode']].drop_duplicates())}\")\n",
    "        df = lien_genCalcPic.merge(df, how='inner', left_on=['generalPic_old','pic'], right_on=['generalPic','pic']).drop_duplicates()\n",
    "        return df\n",
    "\n",
    "    proj = h20_topics(_proj, act, actions, destination, pilier_fr, thema)\n",
    "    proj = proj_cleaning(proj)\n",
    "    entities = entities_cleaning(entities, country_h20, part_init)\n",
    "\n",
    "    def ref_select(FP):\n",
    "        ref_source = ref_source_load('ref')\n",
    "        # traitement ref select le FP, id non null ou/et ZONAGE non null\n",
    "        ref, genPic_to_new = ref_source_2d_select(ref_source, FP)\n",
    "        ror = pd.read_pickle(f\"{PATH_REF}ror_df.pkl\")\n",
    "        paysage = pd.read_pickle(f\"{PATH_REF}paysage_df.pkl\")\n",
    "        sirene = pd.read_pickle(f\"{PATH_REF}sirene_df.pkl\")\n",
    "        ### si besoin de charger groupe\n",
    "        groupe = pd.read_pickle(f\"{PATH_REF}H20_groupe.pkl\")\n",
    "        return ref, genPic_to_new, ror, paysage, sirene, groupe\n",
    "    ref, genPic_to_new, ror, paysage, sirene, groupe = ref_select('H20')\n",
    "\n",
    "    print(f\"- si ++id pour un generalPic: {ref[ref.id.str.contains(';', na=False)]}\")\n",
    "    ref = (ref.merge(country_h20[['iso3', 'parent_iso3']], how='left', left_on='country_code_mapping', right_on='iso3')\n",
    "        .drop(columns='iso3')\n",
    "        .rename(columns={'parent_iso3':'country_code'}))\n",
    "    print(f\"parent_iso missing : {ref[ref.country_code.isnull()].country_code_mapping.unique()}\")\n",
    "    ref.loc[ref.country_code.isnull(), 'country_code'] = ref.loc[ref.country_code.isnull()].country_code_mapping \n",
    "\n",
    "\n",
    "    ########################################################################\n",
    "    p=part_init[['generalPic', 'country_code_mapping', 'country_code']].drop_duplicates()\n",
    "    print(f\"size de p: {len(p)}\")\n",
    "    p = p.merge(ref, how='left', on=['generalPic', 'country_code_mapping', 'country_code'], indicator=True).drop_duplicates()\n",
    "    print(f\"cols de p: {p.columns}\") #168 978\n",
    "\n",
    "    # p1 pic+ccm commun\n",
    "    p1 = p.loc[p['_merge']=='both'].drop(columns=['_merge'])\n",
    "    print(f\"size p1 pic+cc: {len(p1)}\")# 62 928\n",
    "\n",
    "\n",
    "    p2 = (p.loc[p['_merge']=='left_only'].drop(columns=['_merge', 'id', 'ZONAGE', 'id_secondaire'])\n",
    "        .merge(ref.drop(columns=['country_code_mapping']), \n",
    "                how='inner', left_on=['generalPic', 'country_code'], right_on=['generalPic', 'country_code']).drop_duplicates()\n",
    "        )\n",
    "    print(f\"size p2 pic cc_parent: {len(p2)}\")\n",
    "\n",
    "\n",
    "    # acteurs sans identifiant dont le pic à plusieurs pays ou le pic certaines participations ont un identifiant et pas d'autres \n",
    "    p3 = (p.loc[p['_merge']=='left_only'].drop(columns=['_merge', 'country_code_mapping', 'id', 'ZONAGE'])\n",
    "        .merge(ref, how='inner', on=['generalPic']).drop_duplicates())\n",
    "    if not p3.empty:\n",
    "        print(f\"A faire si possible, vérifier pourquoi des participations avec pic identiques ont un id ou pas nb pic: {len(p3.generalPic.unique())}\")\n",
    "\n",
    "    p = pd.concat([p1,p2], ignore_index=True).drop_duplicates()\n",
    "    print(f\"size de new p: {len(p)}, cols: {p.columns}\")\n",
    "\n",
    "    part1 = part_init.merge(p, how='left', on=['generalPic', 'country_code_mapping', 'country_code'])\n",
    "    print(f\"size part1: {len(part1)}, part: {len(part_init)}\")\n",
    "\n",
    "    # gestion code nuts\n",
    "    part1.loc[(part1.nutsCode.str.len()>2), 'nuts_code'] = part1.nutsCode\n",
    "    part1 = (part1.merge(nuts, how='left', on='nuts_code')\n",
    "                .drop_duplicates()\n",
    "                .rename(columns={'nuts_code':'participation_nuts'}))\n",
    "    print(f\"size participation after add nuts: {len(part1)}, sans nuts name: {len(part1.loc[(~part1.participation_nuts.isnull())&(part1.region_1_name.isnull())])}\")\n",
    "\n",
    "\n",
    "    ### entities\n",
    "    entities_tmp = part1.loc[~part1.id.isnull(), ['generalPic','id','country_code_mapping']].drop_duplicates()\n",
    "    print(f\"- size entities {len(entities_tmp)}\")\n",
    "    if any(entities_tmp.id.str.contains(';')):\n",
    "        entities_tmp = entities_tmp.assign(id_extend=entities_tmp.id.str.split(';')).explode('id_extend')\n",
    "        ent_size_to_keep = len(entities_tmp)\n",
    "        print(f\"1- size ent si multi id -> ent_size_to_keep = {ent_size_to_keep}\\n{entities_tmp.columns}\")\n",
    "\n",
    "    entities_tmp = merge_ror(entities_tmp, ror)\n",
    "    print(f\"size entities_tmp after add ror_info: {len(entities_tmp)}, entities_size_to_keep: {ent_size_to_keep}\")\n",
    "\n",
    "    # PAYSAGE\n",
    "    ### si besoin de charger paysage pickle\n",
    "    paysage_category = pd.read_pickle(f\"{PATH_SOURCE}paysage_category.pkl\")\n",
    "    cat_filter = category_paysage(paysage_category)\n",
    "    entities_tmp = merge_paysage(entities_tmp, paysage, cat_filter)\n",
    "\n",
    "    # SIRENE\n",
    "    ### si besoin de charger paysage pickle\n",
    "    entities_tmp = merge_sirene(entities_tmp, sirene)\n",
    "    entities_tmp['nb']=entities_tmp.groupby(['generalPic', 'id_extend', 'country_code_mapping'])['entities_id'].transform('count')\n",
    "    if any(entities_tmp['nb']>1):\n",
    "        print(f\"doublons: {entities_tmp.loc[entities_tmp['nb']>1, ['generalPic', 'id_extend', 'country_code_mapping', 'entities_id', 'nb']]}\")\n",
    "        entities_tmp=entities_tmp.loc[~entities_tmp.entities_id.isin(['889664413', '808994164'])]\n",
    "\n",
    "    entities_tmp.loc[(~entities_tmp.id.isnull())&(entities_tmp.entities_id.isnull()), 'entities_id'] = entities_tmp.id\n",
    "    entities_tmp['siren']=entities_tmp.loc[entities_tmp.entities_id.str.contains('^[0-9]{9}$|^[0-9]{14}$', na=False)].entities_id.str[:9]\n",
    "    entities_tmp.loc[entities_tmp.siren.isnull(), 'siren']=entities_tmp.paysage_siren\n",
    "\n",
    "    #groupe entreprises\n",
    "    # recuperation tous les siren pour lien avec groupe -> creation var SIREN \n",
    "    entities_tmp.loc[~entities_tmp.siren.isnull(), \"siren\"] = entities_tmp.loc[~entities_tmp.siren.isnull(), \"siren\"].str.split().apply(set).str.join(\";\")\n",
    "\n",
    "    if any(entities_tmp.siren.str.contains(';', na=False)):\n",
    "        print(\"ATTENTION faire code pour traiter deux siren différents -> ce qui serait bizarre qu'il y ait 2 siren\")\n",
    "    else:\n",
    "        print(f\"taille de entities_tmp avant groupe:{len(entities_tmp)}\")\n",
    "        entities_tmp=entities_tmp.merge(groupe, how='left', on='siren')\n",
    "\n",
    "        # entities_tmp.loc[~entities_tmp.groupe_id.isnull(), 'entities_name_source']= entities_tmp.entities_name\n",
    "        # entities_tmp.loc[~entities_tmp.groupe_id.isnull(), 'entities_acronym_source']= entities_tmp.entities_acronym\n",
    "    print(f\"taille de entities_tmp après groupe {len(entities_tmp)}\")\n",
    "    entities_tmp = entities_tmp.merge(get_source_ID(entities_tmp, 'entities_id'), how='left', on='entities_id')\n",
    "\n",
    "    # traitement catégorie\n",
    "    entities_tmp = category_woven(entities_tmp, sirene)\n",
    "    entities_tmp = category_agreg(entities_tmp)\n",
    "    entities_tmp = mires(entities_tmp)\n",
    "\n",
    "    print(f\"size part1 avant: {len(part1)}\")\n",
    "    part_tmp = part1.merge(genPic_to_new, how='left', on=['generalPic', 'country_code_mapping'])\n",
    "    part_tmp = part_tmp.rename(columns={'generalPic':'pic_old', 'pic_new':'generalPic'})\n",
    "    part_tmp.loc[part_tmp.generalPic.isnull(), 'generalPic'] = part_tmp.loc[part_tmp.generalPic.isnull(), 'pic_old']\n",
    "    part_tmp = part_tmp.merge(entities_tmp.drop(columns='id'), how='left', on=['generalPic', 'country_code_mapping'])\n",
    "    print(f\"size part1 -> part_tmp: {len(part_tmp)}\")\n",
    "\n",
    "    print(len(part_tmp[(part_tmp.entities_name.isnull())]))\n",
    "    part2=part_tmp.loc[(part_tmp.entities_name.isnull()), ['generalPic','entities_id', 'country_code_mapping', 'source_id']]\n",
    "    part2.loc[part2.entities_id.str.contains('-', na=False), 'pic_d'] = part2.loc[part2.entities_id.str.contains('-', na=False)].entities_id.str.split('-').str[0]\n",
    "    part2.loc[part2.pic_d.isnull(), 'pic_d'] = part2.loc[part2.pic_d.isnull()].generalPic\n",
    "\n",
    "    part2 = part2.drop_duplicates()\n",
    "    print(f\"size part2: {len(part2)}, nb unique pic_d: {part2.pic_d.nunique()}\")\n",
    "    part2 = (part2.merge(entities, how='inner', left_on='pic_d', right_on='generalPic')[\n",
    "                ['pic_d','entities_id','legalName', 'businessName', 'legalEntityTypeCode', 'generalState']]\n",
    "            .rename(columns={'businessName':'shortName'})\n",
    "            .drop_duplicates()\n",
    "            )\n",
    "\n",
    "    gen_state=['VALIDATED', 'DECLARED', 'DEPRECATED', 'SLEEPING', 'SUSPENDED', 'BLOCKED']\n",
    "    part2=part2.groupby(['pic_d']).apply(lambda x: x.sort_values('generalState', key=lambda col: pd.Categorical(col, categories=gen_state, ordered=True)), include_groups=True).reset_index(drop=True)\n",
    "    part2=part2.groupby(['pic_d']).head(1).drop(columns='generalState')\n",
    "    print(f\"size part2: {len(part2)}, nb unique pic_d: {part2.pic_d.nunique()}\")\n",
    "\n",
    "    part3=(part_tmp.loc[(~part_tmp.generalPic.isin(part2.pic_d.unique()))&(part_tmp.entities_name.isnull())]\n",
    "        .sort_values(['generalPic','legalName', 'shortName'], ascending=False))\n",
    "    print(part3.generalPic.nunique())\n",
    "\n",
    "    part3=(part3.groupby(['generalPic', 'country_code_mapping'])\n",
    "        .first().reset_index()[['generalPic', 'country_code_mapping', 'legalName', 'shortName', 'legalEntityTypeCode']]\n",
    "        .reset_index(drop=True)\n",
    "        .drop_duplicates()\n",
    "        )\n",
    "    print(part3.generalPic.nunique())\n",
    "\n",
    "    part_tmp = part_tmp.merge(part2, how='left', left_on='generalPic', right_on='pic_d', suffixes=['', '_x'])\n",
    "    part_tmp.loc[~part_tmp.legalName_x.isnull(), 'legalName'] = part_tmp.legalName_x\n",
    "    part_tmp.loc[~part_tmp.shortName_x.isnull(), 'shortName'] = part_tmp.shortName_x\n",
    "    part_tmp.loc[~part_tmp.legalEntityTypeCode_x.isnull(), 'legalEntityTypeCode'] = part_tmp.legalEntityTypeCode_x\n",
    "    print(f\"size part_tmp after merge part2: {len(part_tmp)}\")\n",
    "\n",
    "    part_tmp = part_tmp.merge(part3, how='left', on=['generalPic', 'country_code_mapping'], suffixes=['', '_y'])\n",
    "    part_tmp.loc[~part_tmp.legalName_y.isnull(), 'legalName'] = part_tmp.legalName_y\n",
    "    part_tmp.loc[~part_tmp.shortName_y.isnull(), 'shortName'] = part_tmp.shortName_y\n",
    "    part_tmp.loc[~part_tmp.legalEntityTypeCode_y.isnull(), 'legalEntityTypeCode'] = part_tmp.legalEntityTypeCode_y\n",
    "    part_tmp.drop(part_tmp.columns[part_tmp.columns.str.endswith(('_x','_y'))], axis=1, inplace=True)\n",
    "    print(f\"size part_tmp after merge part2: {len(part_tmp)}\")\n",
    "\n",
    "    liste=['legalName', 'shortName']\n",
    "    for i in liste:\n",
    "        part_tmp[i] = part_tmp[i].apply(lambda x: x.capitalize().strip() if isinstance(x, str) else x)\n",
    "\n",
    "    part_tmp.loc[part_tmp.entities_name.isnull(), 'entities_name'] = part_tmp.legalName\n",
    "    part_tmp.loc[part_tmp.entities_acronym.isnull(), 'entities_acronym'] = part_tmp.shortName\n",
    "    part_tmp.loc[part_tmp.entities_id.isnull(), 'entities_id'] = \"pic\"+part_tmp.generalPic.map(str)\n",
    "\n",
    "    part_tmp.rename(columns={'legalName':'entities_name_source',\n",
    "                            'shortName':'entities_acronym_source'}, inplace=True)\n",
    "\n",
    "    for i in ['entities_acronym', 'entities_name','entities_acronym_source', 'entities_name_source']:\n",
    "        part_tmp[i] = part_tmp[i].str.replace('\\\\n|\\\\t|\\\\r|\\\\s+', ' ', regex=True).str.strip()\n",
    "    print(f\"size part_tmp after clean string: {len(part_tmp)}\")\n",
    "\n",
    "    ##########################################################\n",
    "\n",
    "    # create calculated_fund and coordination_number\n",
    "    part_tmp = (part_tmp\n",
    "                .assign(calculated_fund=np.where(part_tmp.stage=='successful', part_tmp['subv_net'], part_tmp['requestedGrant']), \n",
    "                        coordination_number=np.where(part_tmp.role=='coordinator', 1, 0)))\n",
    "\n",
    "\n",
    "    #############################################################\n",
    "    ### ERC\n",
    "    \n",
    "    proj_erc=proj.loc[proj.action_id=='ERC', ['project_id', 'destination_code']].drop_duplicates()\n",
    "    part_tmp=part_tmp.merge(proj_erc, how='left', on='project_id', indicator=True)\n",
    "    part_tmp.loc[part_tmp._merge=='both', 'fund_ent_erc'] = part_tmp.loc[part_tmp._merge=='both'].calculated_fund\n",
    "    \n",
    "    # traitement erc ROLE\n",
    "    part_tmp['erc_role'] = 'other'\n",
    "    part_tmp.loc[(part_tmp.stage=='evaluated')&(part_tmp.destination_code=='SyG')&((part_tmp.participates_as=='host')|(part_tmp.role=='coordinator')), 'erc_role'] = 'PI'\n",
    "    part_tmp.loc[(part_tmp.stage=='successful')&(part_tmp.destination_code=='SyG')&(part_tmp.participates_as=='beneficiary')&(pd.to_numeric(part_tmp.orderNumber, errors='coerce')<5.), 'erc_role'] = 'PI'\n",
    "    part_tmp.loc[(part_tmp.role=='coordinator')&(part_tmp.destination_code!='SyG'), 'erc_role'] = 'PI'\n",
    "    part_tmp.loc[(part_tmp.destination_code=='SyG')&(part_tmp.role=='coordinator'), 'role'] = 'CO-PI'\n",
    "    part_tmp.loc[(part_tmp.erc_role=='PI')&(part_tmp.role!='CO-PI'), 'role'] = 'PI'\n",
    "    \n",
    "    # traitement subv pour ERC\n",
    "        #calcul budget ERC\n",
    "    pt = part_tmp.loc[(part_tmp._merge=='both')&(part_tmp.destination_code!='SyG')]\n",
    "    pt['calculated_fund'] = np.where(pt.stage=='successful', pt['subv'], pt['requestedGrant'])\n",
    "    spt = pt.loc[pt.stage=='evaluated', ['project_id', 'requestedGrant']].groupby(['project_id'])['requestedGrant'].sum().reset_index()\n",
    "    pt = pt.merge(spt, how='left', on='project_id', suffixes=('', '_y'))\n",
    "    pt.loc[pt.stage=='evaluated', 'calculated_fund'] = pt.loc[pt.stage=='evaluated'].requestedGrant_y\n",
    "    pt.loc[pt.erc_role!='PI', 'calculated_fund'] = 0\n",
    "\n",
    "    from functions_shared import work_csv\n",
    "    work_csv(pt, 'pt_20')\n",
    "    ############################################\n",
    "\n",
    "    part_tmp = pd.concat([part_tmp[~part_tmp.project_id.isin(pt.project_id.unique())], pt], ignore_index=True)\n",
    "    print(f\"size part_tmp after concat with erc: {len(part_tmp)}\")\n",
    "\n",
    "    part_tmp.drop(columns=['destination_code','requestedGrant_y', '_merge'], inplace=True)\n",
    "\n",
    "    part_tmp = part_tmp.assign(number_involved=1)\n",
    "    part_tmp['nb'] = part_tmp.id.str.split(';').str.len()\n",
    "    for i in ['subv', 'subv_net', 'requestedGrant', 'calculated_fund', 'fund_ent_erc']:\n",
    "        part_tmp[i] = np.where(part_tmp['nb']>1, part_tmp[i]/part_tmp['nb'], part_tmp[i])\n",
    "    print(f\"involved successful:{'{:,.1f}'.format(part_tmp.loc[(part_tmp.stage=='successful'), 'number_involved'].sum())}\\nsubv_net_laureat:{'{:,.1f}'.format(part_tmp.loc[(part_tmp.stage=='successful'), 'subv_net'].sum())}\\nsubv_laureat:{'{:,.1f}'.format(part_tmp.loc[(part_tmp.stage=='successful'), 'subv'].sum())}\\nsubv_prop:{'{:,.1f}'.format(part_tmp.loc[(part_tmp.stage=='evaluated'), 'requestedGrant'].sum())}\")\n",
    "\n",
    "\n",
    "    proj_no_coord = proj[(proj.thema_code.isin(['ACCELERATOR','COST']))|(proj.destination_code.isin(['SNLS','PF']))|(proj.action_code3.str.contains('SNLS', na=False))|(proj.thema_code=='ERC')].project_id.to_list()\n",
    "\n",
    "    part_tmp.loc[part_tmp.project_id.isin(proj_no_coord), 'coordination_number'] = 0\n",
    "    part_tmp = part_tmp.assign(with_coord=True)\n",
    "    part_tmp.loc[part_tmp.project_id.isin(proj_no_coord), 'with_coord'] = False\n",
    "\n",
    "    part_tmp.rename(columns={'ZONAGE':'extra_joint_organization'}, inplace=True)\n",
    "    part_tmp = part_tmp.map(lambda x: x.strip() if isinstance(x, str) else x)\n",
    "\n",
    "    part_tmp = (part_tmp\n",
    "            .assign(is_ejo=np.where(part_tmp.extra_joint_organization.isnull(), 'Sans', 'Avec')))\n",
    "\n",
    "    # merge cordis type\n",
    "    part_tmp.loc[part_tmp.legalEntityTypeCode.isnull(), 'legalEntityTypeCode'] = np.nan\n",
    "    part_tmp = cordis_type(part_tmp)\n",
    "    print(f\"size part_tmp after clean codis legal type: {len(part_tmp)}\")\n",
    "\n",
    "    # merge countries \n",
    "    if any(part_tmp.country_code_mapping.isnull()):\n",
    "        print(f\"ATTENTION ! country_code_mapping null: {part_tmp[part_tmp.country_code_mapping.isnull()].countryCode.unique()}\")\n",
    "    else:\n",
    "        part_tmp = (part_tmp\n",
    "                    .merge(countries[['countryCode_iso3', 'country_name_en']]\n",
    "                           .rename(columns={'countryCode_iso3':'country_code_mapping', 'country_name_en': 'country_name_mapping'}), \n",
    "                           how='left', on='country_code_mapping')\n",
    "                    .drop_duplicates())\n",
    "        print(f\"size part_tmp avant: {len(part_tmp)}\")\n",
    "\n",
    "    if any(part_tmp.country_code.isnull()):\n",
    "        print(f\"ATTENTION ! country_code null: {part_tmp[part_tmp.country_code.isnull()].country_code_mapping.unique()}\")\n",
    "    else:\n",
    "        cc=(countries[['countryCode_iso3', 'country_name_en',\n",
    "        'country_association_code_2020', 'country_association_name_2020_en', 'country_group_association_code_2020',\n",
    "        'country_group_association_name_2020_en', 'country_group_association_name_2020_fr', 'country_name_fr', 'article1',\n",
    "        'article2']]\n",
    "        .drop_duplicates()\n",
    "        .rename(columns={'countryCode_iso3': 'country_code',\n",
    "                            'country_association_code_2020':'country_association_code',\n",
    "                            'country_association_name_2020_en':'country_association_name_en', \n",
    "                            'country_group_association_code_2020':'country_group_association_code',\n",
    "                            'country_group_association_name_2020_en':'country_group_association_name_en',\n",
    "                            'country_group_association_name_2020_fr':'country_group_association_name_fr'}))\n",
    "        part_tmp = part_tmp.merge(cc, how='left', on='country_code')\n",
    "        \n",
    "    print(f\"size part_tmp after merge countries: {len(part_tmp)}\")\n",
    "\n",
    "    # agregation des participants\n",
    "    participation=part_tmp[\n",
    "        ['project_id',  'stage', 'participates_as', 'role', 'erc_role', 'calculated_fund', 'fund_ent_erc', 'subv', 'subv_net', 'cordis_is_sme', \n",
    "        'requestedGrant', 'number_involved', 'coordination_number', 'with_coord', 'is_ejo',\n",
    "        'cordis_type_entity_code','cordis_type_entity_name_fr', 'cordis_type_entity_acro',\n",
    "        'cordis_type_entity_name_en', 'participation_nuts', 'region_1_name', 'region_2_name', 'regional_unit_name',\n",
    "        'country_code_mapping', 'country_name_mapping', 'country_code', 'country_name_en', 'extra_joint_organization',\n",
    "        'country_association_code','country_association_name_en', 'country_group_association_code',\n",
    "        'country_group_association_name_en', 'country_group_association_name_fr', 'country_name_fr', 'article1',\n",
    "        'article2', 'entities_name', 'entities_acronym', 'entities_id', 'generalPic',\n",
    "        'entities_name_source', 'entities_acronym_source','paysage_category_priority',\n",
    "        'ror_category', 'paysage_category', 'paysage_category_id', 'category_agregation',\n",
    "        'insee_cat_code', 'insee_cat_name', 'groupe_sector', 'source_id', 'flag_entreprise',\n",
    "        'category_woven', 'operateur_lib', 'operateur_name', 'operateur_num',\n",
    "        'groupe_name','groupe_acronym', 'groupe_id']]\n",
    "\n",
    "    participation = participation.groupby(list(participation.columns.difference(['subv', 'subv_net', 'requestedGrant', 'number_involved', 'calculated_fund', 'fund_ent_erc'])), dropna=False, as_index=False).sum()\n",
    "    print(f\"involved successful:{'{:,.1f}'.format(participation.loc[(participation.stage=='successful'), 'number_involved'].sum())}\\nsubv_laureat:{'{:,.1f}'.format(participation.loc[(participation.stage=='successful'), 'subv_net'].sum())}\\nsubv_prop:{'{:,.1f}'.format(participation.loc[(participation.stage=='evaluated'), 'requestedGrant'].sum())}\")\n",
    "    participation.drop(columns=['requestedGrant', 'subv_net'], inplace=True)\n",
    "\n",
    "    # proj pour synthese\n",
    "    proj_s=proj.loc[~((proj.stage=='successful')&(proj.status_code=='REJECTED')),\n",
    "        ['framework','project_id', 'call_id', 'panel_code', 'status_code', 'topic_code', 'stage', 'call_year', 'abstract',\n",
    "        'pilier_name_en', 'pilier_name_fr','programme_name_en', 'thema_name_en', 'thema_code', 'programme_code',\n",
    "        'panel_name', 'panel_regroupement_code', 'panel_regroupement_name', 'call_deadline', 'free_keywords',\n",
    "        'destination_code','destination_name_en','destination_detail_code','destination_detail_name_en',\n",
    "        'action_code', 'action_code2', 'action_code3', 'action_name', 'action_name2', 'action_name3', 'ecorda_date']]\n",
    "\n",
    "    temp = proj_s.merge(participation, how='inner', on=['project_id', 'stage'])\n",
    "    temp = temp.reindex(sorted(temp.columns), axis=1)\n",
    "    print(f\"involved successful:{'{:,.1f}'.format(temp.loc[(temp.stage=='successful'), 'number_involved'].sum())}\\nsubv_laureat:{'{:,.1f}'.format(temp.loc[(temp.stage=='successful'), 'calculated_fund'].sum())}\\nsubv_prop:{'{:,.1f}'.format(temp.loc[(temp.stage=='evaluated'), 'calculated_fund'].sum())}\")\n",
    "    print(len(temp))\n",
    "\n",
    "    file_name = f\"{PATH_CLEAN}H2020_data.pkl\"\n",
    "    with open(file_name, 'wb') as file:\n",
    "        pd.to_pickle(temp, file)\n",
    "\n",
    "\n",
    "    # sans cordis type\n",
    "    cordis_type_null=[]\n",
    "    for i in ['evaluated', 'successful']:\n",
    "        nb_involved = temp.loc[temp.stage==i].number_involved.sum()\n",
    "        nb_type_null = temp.loc[(temp.cordis_type_entity_code.isnull())&(temp.stage==i)].number_involved.sum()\n",
    "        fund_type = temp.loc[temp.stage==i].calculated_fund.sum()\n",
    "        part_involved_null = nb_type_null/nb_involved*100\n",
    "        fund_type_null = temp.loc[(temp.cordis_type_entity_code.isnull())&(temp.stage==i)].calculated_fund.sum()\n",
    "        part_fund_null = fund_type_null/fund_type*100\n",
    "        d = {'framework': 'H2020', 'stage': i, 'nb_involved': nb_involved, 'nb_type_null':nb_type_null, 'fund_type':fund_type, 'fund_type_null':fund_type_null, 'part_fund_null':part_fund_null}\n",
    "        cordis_type_null.append(d)\n",
    "        print(f\"{i} -> nb_involved {nb_involved}, nb_type_null {nb_type_null}, 'part_involved_null' {part_involved_null} fund_type {fund_type}, fund_type_null {fund_type_null}, {part_fund_null}\")\n",
    "    pd.DataFrame(cordis_type_null).to_csv(f\"{PATH_CONNECT}cordis_type_null.csv\", sep=';')\n",
    "\n",
    "    def h20_proj_success(proj, participation):\n",
    "        from config_path import PATH_CLEAN\n",
    "        country=(participation\n",
    "                .loc[participation.stage=='successful',['project_id','country_code','country_name_fr','country_code_mapping','country_name_mapping', 'participation_nuts', 'region_1_name', 'region_2_name', 'regional_unit_name']]\n",
    "                .drop_duplicates()\n",
    "                .groupby(['project_id'], as_index = False).agg(lambda x: ';'.join(map(str, filter(None, x))))\n",
    "                .drop_duplicates())\n",
    "\n",
    "        prop = (proj.loc[proj.stage=='evaluated', ['project_id', 'proposal_budget', 'proposal_requestedgrant', 'number_involved']]\n",
    "            .rename(columns={'number_involved':'proposal_numberofapplicants'})\n",
    "            .drop_duplicates())\n",
    "\n",
    "        p = participation.loc[participation.stage=='successful', ['project_id', 'calculated_fund']].groupby('project_id', as_index=False).aggregate('sum').rename(columns={'calculated_fund':'project_eucontribution'})\n",
    "\n",
    "\n",
    "        project = (proj.loc[(proj.stage=='successful')&(proj.status_code!='REJECTED'), ['project_id', 'acronym', 'title', 'abstract', 'call_id',\n",
    "            'call_deadline', 'action_code', 'panel_code', 'duration', 'submission_date', 'topic_code', 'topic_name', 'status_code',\n",
    "            'free_keywords', 'eic_panels', 'call_year', 'pilier_name_en', 'programme_name_en', 'thema_name_en', 'programme_code',\n",
    "            'thema_code', 'panel_name', 'panel_regroupement_code', 'panel_regroupement_name', 'panel_description', \n",
    "            'destination_code','destination_name_en','destination_detail_code','destination_detail_name_en',\n",
    "            'action_name', 'action_code2', 'action_name2', 'start_date','end_date', 'signature_date', 'project_webpage', \n",
    "            'number_involved','project_totalcost',  'proposal_expected_number', 'call_budget', 'framework', 'ecorda_date',\n",
    "            'fp_specific_pilier', 'fp_specific_programme', 'fp_specific_instrument']]\n",
    "            .rename(columns={\n",
    "                            'number_involved':'project_numberofparticipants',\n",
    "                            'action_code2':'action_detail_code',\n",
    "                            'action_name2':'action_detail_name'})\n",
    "                .drop_duplicates())\n",
    "\n",
    "        project = project.merge(p, how='left', on='project_id').merge(country, how='inner', on='project_id').merge(prop, how='left' , on='project_id')\n",
    "\n",
    "        print(f\"- size project lauréats: {len(project)}, {len(p)}, fund: {'{:,.1f}'.format(p['project_eucontribution'].sum())}\")\n",
    "        file_name = f\"{PATH_CLEAN}H2020_successful_projects.pkl\"\n",
    "        with open(file_name, 'wb') as file:\n",
    "            pd.to_pickle(project, file)\n",
    "    h20_proj_success(proj, participation)\n",
    "H2020_process()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# size _proj: 342190\n",
    "# - size part: 1311188\n",
    "# involved successful:195,503.0\n",
    "# subv_net_laureat:71,730,094,005.4\n",
    "# subv_laureat:68,609,787,841.5\n",
    "# subv_prop:687,415,105,572.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'{:,.1f}'.format(temp.loc[(temp.stage=='successful'), 'calculated_fund'].sum())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
